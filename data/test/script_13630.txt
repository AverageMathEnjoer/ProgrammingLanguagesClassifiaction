b'{-# LANGUAGE BangPatterns, ScopedTypeVariables, CPP #-}\n{-# OPTIONS_GHC -fwarn-unused-imports #-}\n\n-- The goal of this benchmark is to demonstrate kernels that have private\n-- state which has to be read from memory on each invocation.  Further,\n-- the state forms the majority of the working set -- it\'s larger than\n-- the streaming input.\n\n-- In theory this means that keeping a kernel on one core and\n-- exploiting pipeline parallelism is better than following the data\n-- through the stream graph in a depth first traversal.\n\n\n--module Main(main) where\nmodule Main where\n\nimport Control.Monad as C\nimport Control.DeepSeq\nimport Control.Exception\nimport qualified Control.Parallel.Strategies as Strat\n-- import Data.Array.Unboxed as U\nimport Data.Int\nimport Prelude as P\nimport System.Environment\nimport System.Exit\nimport GHC.Conc as Conc\n-- import GHC.IO (unsafePerformIO, unsafeDupablePerformIO, unsafeInterleaveIO)\n\nimport Debug.Trace\nimport Control.Monad.Par.Logging\n\nimport qualified Data.Vector.Unboxed as UV\nimport           Data.Vector.Unboxed hiding ((++))\n\n-- TEMP: Currently [2011.10.20] the Stream module hasn\'t been\n-- generalized over the ParIVar type class and thus this example\n-- currently works only with the Trace scheduler:\nimport Control.Monad.Par.Stream as S\nimport Control.Monad.Par.Scheds.Trace\n\n-- #ifdef PARSCHED \n-- import PARSCHED\n-- #else\n-- import Control.Monad.Par\n-- #endif\n\n\n\n-- Performs some (presently meaningless) computation on a state & a\n-- window (stream element) to compute a new state and new window.\n--\n-- Assumes statesize is a multiple of bufsize:\nstatefulKern :: Vector Double -> Vector Double -> (Vector Double, Vector Double)\nstatefulKern state buf = (newstate, newelem)\n where \n  -- We could probably test the memory behavior we\'re interested in\n  -- better with inplace update here... but for now, this:\n  newstate = UV.map (\\d -> d/sum + 2) state\n  newelem  = UV.map (+sum) buf\n\n  sum = P.sum partialSums\n  partialSums = [ sumslice (cutslice n) | n <- [0..factor-1] ]   \n\n  cutslice n    = UV.slice (n*blen) blen state\n  sumslice slce = UV.sum (UV.zipWith (+) buf slce)\n\n  factor       = slen `quot` blen\n  slen = UV.length state\n  blen = UV.length buf\n\n--------------------------------------------------------------------------------\n\nmonadpar_version (_,numfilters, bufsize, statecoef, numwins) = do \n  putStrLn$ "Running monad-par version."\n\n  let statesize = bufsize * statecoef\n  results <- evaluate $ runPar$ do \n       strm1 :: Stream (UV.Vector Double) <- S.generate numwins (\\n -> UV.replicate bufsize 0)\n       -- Make a pipeline of numfilters stages:\n       let initstate = UV.generate statesize fromIntegral\n       pipe_end <- C.foldM (\\s _ -> streamScan statefulKern initstate s) strm1 [1..numfilters]\n\n       sums  <- streamMap UV.sum pipe_end\n#if 0\n       return sums\n\n  -- This is tricky, but two different consumers shouldn\'t prevent\n  -- garbage collection.\n  ls <- toListSpin results\n--  Just (Cons h _) <- pollIVar results\n  putStrLn$ "Sum of first window: " ++ show (P.head ls)\n  forkIO$ measureRateList ls\n  putStrLn$ "Final sum = "++ show (P.sum ls)\n#else\n\n       streamFold (+) 0 sums\n\n  putStrLn$ "Final sum = "++ show results\n#endif\n\n\n--------------------------------------------------------------------------------\n\nsparks_version (_,numfilters, bufsize, statecoef, numwins) = do \n  putStrLn$ "Running sparks version."\n\n  -- Here we represent the stream as a plain list.\n  let \n      statesize = bufsize * statecoef\n      strm1 :: [UV.Vector Double] = P.replicate numwins $ UV.replicate bufsize 0\n      initstate = UV.generate statesize fromIntegral\n      applyKern = scan statefulKern initstate\n\n      -- This one has the problem that it fully evaluates the stream for the\n      -- first kernel before moving on to the second:\n      --      strm_last = (parRepeatFun numfilters applyKern) strm1\n\n      pipe_end = applyNKernels statefulKern numfilters initstate strm1\n\n      sums = P.map UV.sum pipe_end\n-- #define SERIAL\n#ifndef SERIAL\n\t     `Strat.using` (Strat.parBuffer numCapabilities Strat.rseq) \n#endif\n\n  putStrLn$ "Sum of first window: "++ show (P.head sums)\n  measureRateList (sums)\n--  measureRateList (strm_last)\n--  measureRateList (forceList strm_last)\n  putStrLn$ "Final Sum = " ++ show (P.sum sums)\n\n-- Make sure the cars of a list are evaluated before following each cdr:\nforceList [] = []\nforceList (h:t) = rnf h `seq` forceList t\n\n-- A slightly different version of Data.List.scanl\nscan :: (a -> b -> (a,c)) -> a -> [b] -> [c]\nscan f q [] = []\nscan f q (h:t) = h\' : scan f q t\n where \n  (q\',h\') = f q h\n\ntype StatefulKernel s a b = s -> a -> (s,b)\n\n-- applyNKernels _ _ _ [] = []\n-- applyNKernels :: NFData a => StatefulKernel s a a -> Int -> s -> [a] -> [a]\napplyNKernels :: (NFData a, NFData s) => StatefulKernel s a a -> Int -> s -> [a] -> [a]\napplyNKernels _    0 _    ls = ls\napplyNKernels kern n init ls =   \n  applyNKernels kern (n-1) init (loop init ls)\n where \n  tasklog = unsafeNewTaskSeries (nameFromValue (n,kern))\n\n  loop st [] = []\n  loop st (h:t) = \n    let (st\', x) = \n#if 0\n\t           timePure tasklog$ \n#endif\n\t\t   kern st h in\n#ifndef SERIAL\n    rnf x `par` \n#endif\n     x : loop st\' t\n   \n-- Compose two stateful kernels in parallel.\ncomposeStatefulKernels :: (NFData b, NFData s1) => \n\t\t\t  StatefulKernel s1 a b -> StatefulKernel s2 b c \n\t\t       -> StatefulKernel (s1,s2) a c\n-- composeStatefulKernels (f1,f2) (s1,s2) x = \ncomposeStatefulKernels f1 f2 (s1,s2) x = \n    rnf pr1 `par` (newstate, snd pr2)\n where \n  pr1 = f1 s1 x\n  pr2 = f2 s2 (snd pr1)\n  newstate = (fst pr1, fst pr2)\n\n\nparRepeatFun n f = \n--  P.foldr (.) id (P.replicate n f)\n  P.foldr (Strat..|| Strat.rdeepseq) id (P.replicate n f)\n\n\n--------------------------------------------------------------------------------\n-- Main script\n\ndefault_version = "monad"\ndefault_numfilters = 4\ndefault_bufsize    = 64\ndefault_statecoef  = 8   -- in MULTIPLES of bufsize\ndefault_numwins    = 500\n-- 4 256 10 10000\n\nmain = do\n  args <- getArgs\n  arg_tup@(version,_,_,_,_) <- \n       case args of \n\t []          -> return (default_version, default_numfilters, default_bufsize, default_statecoef, default_numwins)\n\t [ver]       -> return (            ver, default_numfilters, default_bufsize, default_statecoef, default_numwins)\n\t [a,b,c,d,e] -> return (a, read b, read c, read d, read e)\n\t _         -> do \n\t               putStrLn$ "ERROR: Invalid arguments, must take 0,1, or 5 args."\n\t\t       putStrLn$ "  Expected args: (version=\'monad\'|\'sparks\' #filters, bufsize, stateSizeMultiplier, #bufsToProcess)"\n\t\t       putStrLn$ "  Received args: "++ show args\n\t\t       exitFailure \n\n  putStrLn$ "numCapabilities: "++ show numCapabilities\n  putStrLn$ "  Frequency in measurable ticks:  "++ commaint oneSecond ++ "\\n"\n\n  case version of \n    "monad"  -> monadpar_version arg_tup\n    "sparks" -> sparks_version  arg_tup\n    _        -> error$ "unknown version: "++version\n\n-- This is very verbose:\n--  putStrLn$ "Finally, dumping all logs:"\n--  printAllLogs\n\n\n\n-- It is not necessary to evaluate every element in the case of an unboxed vector.\ninstance NFData a => NFData (UV.Vector a) where\n rnf !vec = ()\n\n\nprint_ msg = trace msg $ return ()\n\n-- work pop 1 peek N push 1 \n-- float->float filter \n-- firFilter n coefs = \n-- {\n\n--     float sum = 0;\n--     for (int i = 0; i < N; i++)\n--       sum += peek(i) * COEFF[N-1-i];\n--     pop();\n--     push(sum);\n--   }\n-- }\n\n\n{- \n\nHere\'s what cachegrind says (on 4 core nehalem):\n\n  $ valgrind --tool=cachegrind ./stream/disjoint_working_sets_pipeline monad 4 768 10 1000 +RTS -N4\n   .....\n      [measureRate] current rate: 58  Total elems&time 916  181,988,055,721\n      [measureRate] Hit end of stream after 1000 elements.\n     Final sum = 1.560518243231086e22\n     ==21202== \n     ==21202== I   refs:      7,111,462,273\n     ==21202== I1  misses:          374,190\n     ==21202== L2i misses:          298,364\n     ==21202== I1  miss rate:          0.00%\n     ==21202== L2i miss rate:          0.00%\n     ==21202== \n     ==21202== D   refs:      3,882,935,974  (3,542,949,529 rd   + 339,986,445 wr)\n     ==21202== D1  misses:       14,606,684  (    9,824,455 rd   +   4,782,229 wr)\n     ==21202== L2d misses:        6,774,479  (    2,088,565 rd   +   4,685,914 wr)\n     ==21202== D1  miss rate:           0.3% (          0.2%     +         1.4%  )\n     ==21202== L2d miss rate:           0.1% (          0.0%     +         1.3%  )\n     ==21202== \n     ==21202== L2 refs:          14,980,874  (   10,198,645 rd   +   4,782,229 wr)\n     ==21202== L2 misses:         7,072,843  (    2,386,929 rd   +   4,685,914 wr)\n     ==21202== L2 miss rate:            0.0% (          0.0%     +         1.3%  )\n\n\nSparks version:\n     Final Sum = 1.560518243231086e22\n     ==21226== \n     ==21226== I   refs:      5,898,314,238\n     ==21226== I1  misses:          291,271\n     ==21226== L2i misses:          246,518\n     ==21226== I1  miss rate:          0.00%\n     ==21226== L2i miss rate:          0.00%\n     ==21226== \n     ==21226== D   refs:      3,264,359,909  (3,206,394,437 rd   + 57,965,472 wr)\n     ==21226== D1  misses:       16,003,068  (   10,905,138 rd   +  5,097,930 wr)\n     ==21226== L2d misses:        9,177,043  (    4,207,106 rd   +  4,969,937 wr)\n     ==21226== D1  miss rate:           0.4% (          0.3%     +        8.7%  )\n     ==21226== L2d miss rate:           0.2% (          0.1%     +        8.5%  )\n     ==21226== \n     ==21226== L2 refs:          16,294,339  (   11,196,409 rd   +  5,097,930 wr)\n     ==21226== L2 misses:         9,423,561  (    4,453,624 rd   +  4,969,937 wr)\n     ==21226== L2 miss rate:            0.1% (          0.0%     +        8.5%  )\n\n -}\n'