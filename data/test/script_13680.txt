b'#\' Unmix samples using loss in a variance stabilized space\n#\'\n#\' Unmixes samples in \\code{x} according to \\code{pure} components,\n#\' using numerical optimization. The components in \\code{pure}\n#\' are added on the scale of gene expression (either normalized counts, or TPMs).\n#\' The loss function when comparing fitted expression to the\n#\' samples in \\code{x} occurs in a variance stabilized space.\n#\' This task is sometimes referred to as "deconvolution",\n#\' and can be used, for example, to identify contributions from\n#\' various tissues.\n#\' Note: some groups have found that the mixing contributions\n#\' may be more accurate if very lowly expressed genes across \\code{x}\n#\' and \\code{pure} are first removed. We have not explored this fully.\n#\' Note: if the \\code{pbapply} package is installed a progress bar\n#\' will be displayed while mixing components are fit.\n#\'\n#\' @param x normalized counts or TPMs of the samples to be unmixed\n#\' @param pure normalized counts or TPMs of the "pure" samples\n#\' @param alpha for normalized counts, the dispersion of the data\n#\' when a negative binomial model is fit. this can be found by examining\n#\' the asymptotic value of \\code{dispersionFunction(dds)}, when using\n#\' \\code{fitType="parametric"} or the mean value when using\n#\' \\code{fitType="mean"}.\n#\' @param shift for TPMs, the shift which approximately stabilizes the variance\n#\' of log shifted TPMs. Can be assessed with \\code{vsn::meanSdPlot}.\n#\' @param power either 1 (for L1) or 2 (for squared) loss function.\n#\' Default is 1.\n#\' @param format \\code{"matrix"} or \\code{"list"}, default is \\code{"matrix"}.\n#\' whether to output just the matrix of mixture components, or a list (see Value).\n#\' \n#\' @param quiet suppress progress bar. default is FALSE, show progress bar\n#\' if pbapply is installed.\n#\'\n#\' @return a matrix, the mixture components for each sample in \\code{x} (rows).\n#\' The "pure" samples make up the columns, and so each row sums to 1.\n#\' If colnames existed on the input matrices they will be propagated to the output matrix.\n#\' If \\code{format="list"}, then a list, containing as elements:\n#\' (1) the matrix of mixture components,\n#\' (2) the correlations in the variance stabilized space of the fitted samples\n#\' to the samples in \\code{x}, and\n#\' (3) the fitted samples as a matrix with the same dimension as \\code{x}.\n#\'\n#\' @examples\n#\'\n#\' # some artificial data\n#\' cts <- matrix(c(80,50,1,100,\n#\'                 1,1,60,100,\n#\'                 0,50,60,100), ncol=4, byrow=TRUE)\n#\' # make a DESeqDataSet\n#\' dds <- DESeqDataSetFromMatrix(cts,\n#\'   data.frame(row.names=seq_len(ncol(cts))), ~1)\n#\' colnames(dds) <- paste0("sample",1:4)\n#\'\n#\' # note! here you would instead use\n#\' # estimateSizeFactors() to do actual normalization\n#\' sizeFactors(dds) <- rep(1, ncol(dds))\n#\'\n#\' norm.cts <- counts(dds, normalized=TRUE)\n#\'\n#\' # \'pure\' should also have normalized counts...\n#\' pure <- matrix(c(10,0,0,\n#\'                  0,0,10,\n#\'                  0,10,0), ncol=3, byrow=TRUE)\n#\' colnames(pure) <- letters[1:3]\n#\' \n#\' # for real data, you need to find alpha after fitting estimateDispersions()\n#\' mix <- unmix(norm.cts, pure, alpha=0.01)\n#\' \n#\' @export\nunmix <- function(x, pure, alpha, shift, power=1, format="matrix", quiet=FALSE) {\n\n  format <- match.arg(format, c("matrix","list"))\n  if (missing(alpha)) stopifnot(!missing(shift))\n  if (missing(shift)) stopifnot(!missing(alpha))\n  stopifnot(missing(shift) | missing(alpha))\n  stopifnot(power %in% 1:2)\n  stopifnot(nrow(x) == nrow(pure))\n  stopifnot(ncol(pure) > 1)\n  \n  if (requireNamespace("pbapply", quietly=TRUE) & !quiet) {\n    lapply <- pbapply::pblapply\n  }\n\n  cor.msg <- "some columns of \'pure\' are highly correlated (>.99 after VST),\n  may result in instabilty of unmix(). visually inspect correlations of \'pure\'"\n  \n  if (missing(shift)) {\n    stopifnot(alpha > 0)\n    # variance stabilizing transformation for NB w/ fixed dispersion alpha\n    vst <- function(q, alpha) ( 2 * asinh(sqrt(alpha * q)) - log(alpha) - log(4) ) / log(2)\n    pure.cor <- cor(vst(pure, alpha)); diag(pure.cor) <- 0\n    if (any(pure.cor > .99)) warning(cor.msg)\n    sumLossVST <- function(p, i, vst, alpha, power) {\n      sum(abs(vst(x[,i], alpha) - vst(pure %*% p, alpha))^power)\n    }\n    res <- lapply(seq_len(ncol(x)), function(i) {\n      optim(par=rep(1, ncol(pure)), fn=sumLossVST, gr=NULL, i, vst, alpha, power,\n            method="L-BFGS-B", lower=1e-6, upper=100)$par\n    })\n  } else {\n    stopifnot(shift > 0)\n    # VST of shifted log\n    vstSL <- function(q, shift) log(q + shift)\n    pure.cor <- cor(vstSL(pure, shift)); diag(pure.cor) <- 0\n    if (any(pure.cor > .99)) warning(cor.msg)\n    sumLossSL <- function(p, i, vst, shift, power) {\n      sum(abs(vstSL(x[,i], shift) - vstSL(pure %*% p, shift))^power)\n    }\n    res <- lapply(seq_len(ncol(x)), function(i) {\n      optim(par=rep(1, ncol(pure)), fn=sumLossSL, gr=NULL, i, vstSL, shift, power,\n            method="L-BFGS-B", lower=0, upper=100)$par\n    })\n  }\n\n  mix <- do.call(rbind, res)\n  mix <- mix / rowSums(mix)\n  colnames(mix) <- colnames(pure)\n  rownames(mix) <- colnames(x)\n\n  if (format == "matrix") {\n    return(mix)\n  } else {\n    fitted <- pure %*% t(mix)\n    cor <- if (missing(shift)) {\n      cor(vst(x, alpha), vst(fitted, alpha))\n    } else {\n      cor(vstSL(x, shift), vstSL(fitted, shift))\n    }\n    return(list(mix=mix, cor=diag(cor), fitted=fitted))\n  }\n  \n}\n\n#\' Collapse technical replicates in a RangedSummarizedExperiment or DESeqDataSet\n#\'\n#\' Collapses the columns in \\code{object} by summing within levels\n#\' of a grouping factor \\code{groupby}. The purpose of this function\n#\' is to sum up read counts from technical replicates to create an object\n#\' with a single column of read counts for each sample.\n#\' Note: by "technical replicates", we mean multiple sequencing runs of the same\n#\' library, in constrast to "biological replicates" in which multiple\n#\' libraries are prepared from separate biological units.\n#\' Optionally renames the columns of returned object with the levels of the\n#\' grouping factor.\n#\' Note: this function is written very simply and\n#\' can be easily altered to produce other behavior by examining the source code.\n#\'\n#\' @param object A \\code{RangedSummarizedExperiment} or \\code{DESeqDataSet}\n#\' @param groupby a grouping factor, as long as the columns of object\n#\' @param run optional, the names of each unique column in object. if provided,\n#\' a new column \\code{runsCollapsed} will be added to the \\code{colData}\n#\' which pastes together the names of \\code{run}\n#\' @param renameCols whether to rename the columns of the returned object\n#\' using the levels of the grouping factor\n#\'\n#\' @return the \\code{object} with as many columns as levels in \\code{groupby}.\n#\' This object has assay/count data which is summed from the various\n#\' columns which are grouped together, and the \\code{colData} is subset using\n#\' the first column for each group in \\code{groupby}.\n#\'\n#\' @examples\n#\'\n#\' dds <- makeExampleDESeqDataSet(m=12)\n#\' \n#\' # make data with two technical replicates for three samples\n#\' dds$sample <- factor(sample(paste0("sample",rep(1:9, c(2,1,1,2,1,1,2,1,1)))))\n#\' dds$run <- paste0("run",1:12)\n#\'\n#\' ddsColl <- collapseReplicates(dds, dds$sample, dds$run)\n#\'\n#\' # examine the colData and column names of the collapsed data\n#\' colData(ddsColl)\n#\' colnames(ddsColl)\n#\'\n#\' # check that the sum of the counts for "sample1" is the same\n#\' # as the counts in the "sample1" column in ddsColl\n#\' matchFirstLevel <- dds$sample == levels(dds$sample)[1]\n#\' stopifnot(all(rowSums(counts(dds[,matchFirstLevel])) == counts(ddsColl[,1])))\n#\' \n#\' @export\ncollapseReplicates <- function(object, groupby, run, renameCols=TRUE) {\n  if (!is.factor(groupby)) groupby <- factor(groupby)\n  groupby <- droplevels(groupby)\n  stopifnot(length(groupby) == ncol(object))\n  sp <- split(seq(along=groupby), groupby)\n  countdata <- sapply(sp, function(i) rowSums(assay(object)[,i,drop=FALSE]))\n  mode(countdata) <- "integer"\n  colsToKeep <- sapply(sp, `[`, 1)\n  collapsed <- object[,colsToKeep]\n  dimnames(countdata) <- dimnames(collapsed)\n  assay(collapsed) <- countdata\n  if (!missing(run)) {\n    stopifnot(length(groupby) == length(run))\n    colData(collapsed)$runsCollapsed <- sapply(sp, function(i) paste(run[i],collapse=","))\n  }\n  if (renameCols) {\n    colnames(collapsed) <- levels(groupby)\n  }\n  stopifnot(sum(as.numeric(assay(object))) == sum(as.numeric(assay(collapsed))))\n  collapsed\n}\n\n#\' FPKM: fragments per kilobase per million mapped fragments\n#\'\n#\' The following function returns fragment counts normalized\n#\' per kilobase of feature length per million mapped fragments\n#\' (by default using a robust estimate of the library size,\n#\' as in \\code{\\link{estimateSizeFactors}}).\n#\'\n#\' The length of the features (e.g. genes) is calculated one of two ways:\n#\' (1) If there is a matrix named "avgTxLength" in \\code{assays(dds)},\n#\' this will take precedence in the length normalization.\n#\' This occurs when using the tximport-DESeq2 pipeline.\n#\' (2) Otherwise, feature length is calculated \n#\' from the \\code{rowRanges} of the dds object,\n#\' if a column \\code{basepairs} is not present in \\code{mcols(dds)}.\n#\' The calculated length is the number of basepairs in the union of all \\code{GRanges}\n#\' assigned to a given row of \\code{object}, e.g., \n#\' the union of all basepairs of exons of a given gene.\n#\' Note that the second approach over-estimates the gene length\n#\' (average transcript length, weighted by abundance is a more appropriate\n#\' normalization for gene counts), and so the FPKM will be an underestimate of the true value.\n#\' \n#\' Note that, when the read/fragment counting has inter-feature dependencies, a strict\n#\' normalization would not incorporate the basepairs of a feature which\n#\' overlap another feature. This inter-feature dependence is not taken into\n#\' consideration in the internal union basepair calculation.\n#\'\n#\' @param object a \\code{DESeqDataSet}\n#\' @param robust whether to use size factors to normalize\n#\' rather than taking the column sums of the raw counts,\n#\' using the \\code{\\link{fpm}} function.\n#\'\n#\' @return a matrix which is normalized per kilobase of the\n#\' union of basepairs in the \\code{GRangesList} or \\code{GRanges}\n#\' of the mcols(object), and per million of mapped fragments,\n#\' either using the robust median ratio method (robust=TRUE, default)\n#\' or using raw counts (robust=FALSE).\n#\' Defining a column \\code{mcols(object)$basepairs} takes\n#\' precedence over internal calculation of the kilobases for each row.\n#\'\n#\' @examples\n#\'\n#\' # create a matrix with 1 million counts for the\n#\' # 2nd and 3rd column, the 1st and 4th have\n#\' # half and double the counts, respectively.\n#\' m <- matrix(1e6 * rep(c(.125, .25, .25, .5), each=4),\n#\'             ncol=4, dimnames=list(1:4,1:4))\n#\' mode(m) <- "integer"\n#\' se <- SummarizedExperiment(list(counts=m), colData=DataFrame(sample=1:4))\n#\' dds <- DESeqDataSet(se, ~ 1)\n#\' \n#\' # create 4 GRanges with lengths: 1, 1, 2, 2.5 Kb\n#\' gr1 <- GRanges("chr1",IRanges(1,1000)) # 1kb\n#\' gr2 <- GRanges("chr1",IRanges(c(1,1001),c( 500,1500))) # 1kb\n#\' gr3 <- GRanges("chr1",IRanges(c(1,1001),c(1000,2000))) # 2kb\n#\' gr4 <- GRanges("chr1",IRanges(c(1,1001),c(200,1300))) # 500bp\n#\' rowRanges(dds) <- GRangesList(gr1,gr2,gr3,gr4)\n#\' \n#\' # the raw counts\n#\' counts(dds)\n#\'\n#\' # the FPM values\n#\' fpm(dds)\n#\' \n#\' # the FPKM values\n#\' fpkm(dds)\n#\' \n#\' @seealso \\code{\\link{fpm}}\n#\'\n#\' @docType methods\n#\' @name fpkm\n#\' @rdname fpkm\n#\' \n#\' @export\nfpkm <- function(object, robust=TRUE) {\n  fpm <- fpm(object, robust=robust)\n  if ("avgTxLength" %in% assayNames(object)) {\n    exprs <- 1e3 * fpm / assays(object)[["avgTxLength"]]\n    if (robust) {\n      sf <- estimateSizeFactorsForMatrix(exprs)\n      exprs <- t(t(exprs)/sf)\n      return(exprs)\n    } else {\n      return(exprs)\n    }\n  }\n  if (is.null(mcols(object)$basepairs)) {\n    if (is(rowRanges(object), "GRangesList")) {\n      ubp <- DataFrame(basepairs = sum(width(reduce(rowRanges(object)))))\n    } else if (is(rowRanges(object), "GRanges")) {\n      ubp <- DataFrame(basepairs = width(rowRanges(object)))\n    }\n    if (all(ubp$basepairs == 0)) {\n      stop("rowRanges(object) has all ranges of zero width.\nthe user should instead supply a column, mcols(object)$basepairs,\nwhich will be used to produce FPKM values")\n    }\n    if (is.null(mcols(mcols(object)))) {\n      mcols(object) <- ubp\n    } else {\n      mcols(ubp) <- DataFrame(type="intermediate",\n                              description="count of basepairs in the union of all ranges")\n      mcols(object) <- cbind(mcols(object), ubp)\n    }\n  }\n  1e3 * sweep(fpm, 1, mcols(object)$basepairs, "/")\n}\n\n#\' FPM: fragments per million mapped fragments\n#\'\n#\' Calculates either a robust version (default)\n#\' or the traditional matrix of fragments/counts per million mapped\n#\' fragments (FPM/CPM).\n#\' Note: this function is written very simply and\n#\' can be easily altered to produce other behavior by examining the source code.\n#\' \n#\' @param object a \\code{DESeqDataSet}\n#\' @param robust whether to use size factors to normalize\n#\' rather than taking the column sums of the raw counts.\n#\' If TRUE, the size factors and the geometric mean of\n#\' column sums are multiplied to create a robust library size estimate.\n#\' Robust normalization is not used if average transcript lengths are present.\n#\' \n#\' @return a matrix which is normalized per million of mapped fragments,\n#\' either using the robust median ratio method (robust=TRUE, default)\n#\' or using raw counts (robust=FALSE).\n#\'\n#\' @examples\n#\'\n#\' # generate a dataset with size factors: .5, 1, 1, 2\n#\' dds <- makeExampleDESeqDataSet(m = 4, n = 1000,\n#\'                                interceptMean=log2(1e3),\n#\'                                interceptSD=0,\n#\'                                sizeFactors=c(.5,1,1,2),\n#\'                                dispMeanRel=function(x) .01)\n#\'\n#\' # add a few rows with very high count\n#\' counts(dds)[4:10,] <- 2e5L\n#\'\n#\' # in this robust version, the counts are comparable across samples\n#\' round(head(fpm(dds), 3))\n#\'\n#\' # in this column sum version, the counts are still skewed:\n#\' # sample1 < sample2 & 3 < sample 4\n#\' round(head(fpm(dds, robust=FALSE), 3))\n#\'\n#\' # the column sums of the robust version\n#\' # are not equal to 1e6, but the\n#\' # column sums of the non-robust version\n#\' # are equal to 1e6 by definition\n#\' \n#\' colSums(fpm(dds))/1e6\n#\' colSums(fpm(dds, robust=FALSE))/1e6\n#\'\n#\' @seealso \\code{\\link{fpkm}}\n#\'\n#\' @docType methods\n#\' @name fpm\n#\' @rdname fpm\n#\' \n#\' @export\nfpm <- function(object, robust=TRUE) {\n  # we do something different if average tx lengths are present\n  noAvgTxLen <- !("avgTxLength" %in% assayNames(object))\n  if (robust & is.null(sizeFactors(object)) & noAvgTxLen) {\n    object <- estimateSizeFactors(object)\n  }\n  k <- counts(object)\n  library.sizes <- if (robust & noAvgTxLen) {\n    sizeFactors(object) * exp(mean(log(colSums(k))))\n  } else {\n    colSums(k)\n  }\n  1e6 * sweep(k, 2, library.sizes, "/")  \n}\n\n\n\n#\' Normalize for gene length\n#\'\n#\' Normalize for gene length using the output of transcript abundance estimators\n#\'\n#\' This function is deprecated and moved to a new general purpose package,\n#\' tximport, which will be added to Bioconductor.\n#\'\n#\' @param ... ...\n#\' \n#\' @export\nnormalizeGeneLength <- function(...) {\n  .Deprecated("tximport, a separate package on Bioconductor")\n}\n\n#\' Normalized counts transformation\n#\'\n#\' A simple function for creating a \\code{\\link{DESeqTransform}}\n#\' object after applying: \\code{f(count(dds,normalized=TRUE) + pc)}.\n#\' \n#\' @param object a DESeqDataSet object\n#\' @param f a function to apply to normalized counts\n#\' @param pc a pseudocount to add to normalized counts\n#\' \n#\' @seealso \\code{\\link{varianceStabilizingTransformation}}, \\code{\\link{rlog}}\n#\' \n#\' @export\nnormTransform <- function(object, f=log2, pc=1) {\n  if (is.null(colnames(object))) {\n    colnames(object) <- seq_len(ncol(object))\n  }\n  if (is.null(sizeFactors(object)) & is.null(normalizationFactors(object))) {\n    object <- estimateSizeFactors(object)\n  }\n  nt <- f(counts(object, normalized=TRUE) + pc)\n  se <- SummarizedExperiment(\n    assays = nt,\n    colData = colData(object),\n    rowRanges = rowRanges(object),\n    metadata = metadata(object))\n  DESeqTransform(se)\n}\n\n#\' Integrate bulk DE results with Bioconductor single-cell RNA-seq datasets\n#\'\n#\' A function that assists with integration of bulk DE results tables\n#\' with pre-processed scRNA-seq datasets available on Bioconductor,\n#\' for downstream visualization tasks. The user is prompted to pick\n#\' a scRNA-seq dataset from a menu. The output of the function is\n#\' a list with the original results table, bulk gene counts,\n#\' and the SingleCellExperiment object selected by the user.\n#\'\n#\' This function assists the user in choosing a datset from a menu of options\n#\' that are selected based on the organism of the current dataset.\n#\' Currently only human and mouse bulk and single-cell RNA-seq datasets\n#\' are supported, and it is assumed that the bulk DE dataset has GENCODE\n#\' or Ensembl gene identifiers. Following the selection of the scRNA-seq\n#\' dataset, visualization can be performed with a package \\code{vizWithSCE},\n#\' which can be installed with \\code{install_github("KwameForbes/vizWithSCE")}.\n#\' \n#\' @param res a results table, as produced via \\code{\\link{results}}\n#\' @param dds a DESeqDataSet with the bulk gene expression data\n#\' (should contain gene-level counts)\n#\' @param ... additional arguments passed to the dataset-accessing function\n#\'\n#\' @return list containing: res, dds, and a SingleCellExperiment as selected\n#\' by the user\n#\'\n#\' @examples\n#\'\n#\' \\dontrun{\n#\'   # involves interactive menu selection...\n#\'   dds <- makeExampleDESeqDataSet()\n#\'   rownames(dds) <- paste0("ENSG",1:nrow(dds))\n#\'   dds <- DESeq(dds)\n#\'   res <- results(dds)\n#\'   dat <- integrateWithSingleCell(res, dds)\n#\' }\n#\' \n#\' @author Kwame Forbes\n#\' \n#\' @export\nintegrateWithSingleCell <- function(res, dds, ...) {\n\n  # function written by Kwame Forbes, to assist with integration of\n  # bulk DE results with Bioconductor single-cell RNA-seq datasets.\n  # provides a menu of dataset options from pre-processed scRNA-seq\n  # datasets. Downstream visualization is performed using the\n  # vizWithSCE package:\n  \n  # https://github.com/KwameForbes/vizWithSCE\n\n  stopifnot(is(res, "DESeqResults"))\n  stopifnot(is(dds, "DESeqDataSet"))\n  \n  # figure out organism from \'dds\' or \'res\', either using tximeta metadata\n  # or guesswork on the gene names in the results table\n  tximetaOrg <- metadata(dds)$txomeInfo$organism\n  if (!is.null(tximetaOrg)) {\n    org <- if (tximetaOrg == "Homo sapiens") {\n             "human"\n           } else if (tximetaOrg == "Mus musculus") {\n             "mouse"\n           } else {\n             stop("Only human and mouse are currently supported")\n           }\n  } else {\n    test.gene <- rownames(res)[1]\n    org <- if (substr(test.gene, 1, 4) == "ENSG") {\n             "human"\n           } else if (substr(test.gene, 1, 7) == "ENSMUSG") {\n             "mouse"\n           } else {\n             stop("Only human and mouse are currently supported")\n           }\n  }\n  \n  message(paste("Your dataset appears to be", org, "\\n"))\n\n  # read in table of options from vizWithSCE CSV file\n  csv.file <- system.file("extdata/singleCellTab.csv", package="DESeq2")\n  tab <- read.csv(csv.file)\n  \n  message(paste("Choose a",org,"single-cell dataset to integrate with (0 to cancel):\\n"))\n  \n  tab <- tab[tab$org == org,]\n  tab2 <- tab[,c("pkg","func","data", "pub","nCells","desc")]\n  tab2$data <- ifelse(is.na(tab2$data), "", tab2$data)\n  rownames(tab2) <- seq_len(nrow(tab2))\n\n  # print the table in two parts\n  print(tab2[,1:3])\n  print(tab2[,4:6])\n\n  cat("\\n")\n  \n  # repeat the message below the fold\n  message(paste("Choose a",org,"single-cell dataset to integrate with (0 to cancel):"))\n  \n  menuOpts <- ifelse(is.na(tab$data), tab$func, paste(tab$func, tab$data, sep="-"))\n  ans <- menu(menuOpts)\n\n  if (ans == 0) stop("No scRNA-seq dataset selected")\n  \n  pkg <- tab$pkg[ans]\n  if (!requireNamespace(package=pkg, quietly=TRUE)) {\n    message(paste0("Package: \'",pkg, "\' is not installed"))\n    ask <- askYesNo("Would you like to install the necessary data package?")\n    if (ask) {\n      if (!requireNamespace(package="BiocManager", quietly=TRUE)) {\n        stop("\'BiocManager\' required to install packages, install from CRAN")\n      }\n      BiocManager::install(pkg)\n    } else {\n      stop("Package would need to be installed")\n    }\n    if (requireNamespace(package=pkg, quietly=TRUE)) {\n      message("Data package was installed successfully")\n    } else {\n      stop("Data package still needs to be installed for integrateWithSingleCell to work")\n    }\n  }\n\n  # load package\n  require(pkg, character.only=TRUE)\n\n  # if the dataset is in the scRNAseq package...\n  if (pkg == "scRNAseq") {\n    # if only one dataset within the function...\n    if (is.na(tab$data[ans])) {\n      sce <- do.call(tab$func[ans], list(ensembl=TRUE, ...))\n    } else {\n      sce <- do.call(tab$func[ans], list(which=tab$data[ans], ensembl=TRUE, ...))\n    }\n  } else {\n    # if only one dataset within the function...\n    if (is.na(tab$data[ans])) {\n      sce <- do.call(tab$func[ans], list(...))\n    } else {\n      sce <- do.call(tab$func[ans], list(dataset=tab$data[ans], ...))\n    }\n  }\n\n  # return the original two objects and the SingleCellExperiment\n  return(list(res=res, dds=dds, sce=sce))\n  \n}\n'