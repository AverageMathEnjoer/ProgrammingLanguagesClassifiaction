b'#\' @export\npredict.FSR <- function(object, newdata, model_to_use=NULL, \n                        standardize=NULL, noisy=TRUE,...)\n{\n  \n\n  if(!is.null(standardize) && object$standardize){\n    for(var_name in names(object[["train_scales"]])){\n      if(var_name %in% colnames(newdata)){\n        newdata[[var_name]] <- (newdata[[var_name]] - object[["train_scales"]][[var_name]][["mean"]])/object[["train_scales"]][[var_name]][["sd"]]\n      }\n    }\n  }\n\n  m <- if(is.null(model_to_use)) which(object$best_formula == object$models$formula) else model_to_use\n\n  mf <- if(is.null(m)) object$best_formula else object$models$formula[m]\n\n  mf <- strsplit(mf, "~")[[1]][2]\n  mf <- formula(paste("~", mf))\n  X_test <- model_matrix(modelFormula = mf, dataFrame = newdata, noisy = noisy, intercept = TRUE)\n\n  y_hat <- X_test %*% object[[mod(m)]][["coeffs"]]\n\n  if(object$outcome == "continuous"){\n\n    return(y_hat) # should this be coeffs?\n\n  }else{\n\n    if(object$outcome == "binary"){\n\n      return(list(probs = y_hat,\n                  classified = classify(y_hat,\n                                        labels = object$training_labels,\n                                        cutoff = object$y_train_mean)))\n\n    }else{  # multinomial case\n\n      p_K <- 1/(1 + rowSums(exp(y_hat))) # prob of being in reference category\n      pred <- list(probs = cbind(p_K, exp(y_hat)*p_K))\n      colnames(pred$probs)[1] <- object$reference_category\n      pred[["classified"]] <- classify(pred$probs)\n\n    }\n\n    return(pred)\n  }\n\n}\n\n#\' @export\nprint.polyFit <- function(x, ...){\n  print(x$fit)\n}\n\n#\' @export\nprint.FSR <- function(x, ...){\n  summary(x, estimation_overview=FALSE, results_overview=TRUE)\n}\n\n#\' @export\nsummary.FSR <- function(object, estimation_overview=TRUE, results_overview=TRUE, model_number = NULL, ...){\n\n  if(estimation_overview){\n\n    message("The dependent variable is \'",  object$y_name,\n        "\' which will be treated as ",  object$outcome, ". ",\n        " The data contains ", object$N, " observations (N_train == ",\n        object$N_train, " and N_test == ", object$N_test,\n        "), which were split using seed ", object$seed, ". The data contains ",\n        object$P_continuous,\n        " continuous features and ", object$P_factor,\n        " dummy variables. Between ", object$min_models, " and ",\n        min(nrow(object$models), object$N_train - 1),\n        " models will be considered. Each model will add a feature, ",\n        "which will be included in subsequent models if it explains at least an additional ",\n        object$threshold_include,\n        " of variance out-of-sample (after adjusting for the additional term on [0, 1]).",\n        " Note: x3*x7, for example, will not be included if x3 and and x7 were not previously included.\\n\\n",\n        sep="")\n\n  }\n\n  if(results_overview){\n\n    if(sum(object$models$estimated) == 0){\n\n      message("\\nNo models could be estimated, likely due to (near) singularity; returning NULL. Check for highly correlated features or factors with rarely observed levels. (Increasing pTraining may help.)\\n")\n\n    }else{\n      message("\\nEstimated ", sum(object$models$estimated), " models. \\n")\n\n      if(object$outcome == "continuous"){\n        message("\\nThe best model has Predicted Adjusted R^2: ", round(object$best_adjR2, 4))\n        message("\\nThe best model has Mean Absolute Predicted Error: ", round(object$best_MAPE, 4))\n      }else{\n        if(object$outcome == "binary"){\n          message("\\nThe best model has pseudo R^2 (adjusted for P and N): ",\n              object$best_test_adjR2)\n          message("\\nThe best model has out-of-sample accuracy: ",\n              object$models$test_accuracy[which(object$best_formula == object$models$formula)])\n        }else{\n          message("\\nThe best model has out-of-sample accuracy (adjusted for P and N):",\n              object$best_test_adj_accuracy)\n        }\n      }\n      message("\\n\\nThe output has a data.frame out$models that contains measures of fit and information about each model, such as the formula call. The output is also a nested list such that if the output is called \'out\', out$model1, out$model2, and so on, contain further metadata. The predict method will automatically use the model with the best validated fit but individual models can also be selected like so:\\n\\npredict(z, newdata = Xnew, model_to_use = 3) \\n\\n")\n    }\n  }\n\n  if(!is.null(model_number)){\n\n    m <- model_number\n    message("\\n\\n\\nThe added coefficient, corresponding to ", object$models$features[m],\n        ifelse(object$models$accepted[m], ", WAS", ", WAS NOT"),\n        " accepted into model ", m, ".\\n\\n", sep="")\n\n    if(object$outcome == "continuous"){\n\n      message("Adjusted R^2: ", round(object$models$test_adjR2[m], 5), "\\n")\n      message("Mean Absolute Predicted Error (MAPE): ", round(object$models$MAPE[m], 5), "\\n")\n\n      if(sum(object$models$accepted[1:m]) > 1){\n\n        message("Adjusted R^2 improvement over best model so far: ",\n            round(object$models$test_adjR2[m] - max(object$models$test_adjR2[1:(m - 1)], na.rm=TRUE), 5),\n            "\\n")\n        message("MAPE improvement over best model so far: ",\n            round(object$models$MAPE[m] - max(object$models$MAPE[1:(m - 1)], na.rm=TRUE), 5),\n            "\\n\\n\\n")\n\n      }\n    }else{\n\n      if(object$outcome == "binary")\n        message("Pseudo R^2 (adjusted for P and N): ", round(object$models$test_adjR2[m], 5), "\\n")\n\n      if(!object$linear_estimation){\n        message("(training) AIC: ",  round(object$models$AIC[m], 5), "\\n")\n        message("(training) BIC: ",  round(object$models$BIC[m], 5), "\\n")\n      }\n      message("(test) classification accuracy:", round(object$models$test_accuracy[m], 5), "\\n")\n\n      if(sum(object$models$accepted) > 1){\n        message("Classification accuracy improvement on the test data: ",\n            round(object$models$test_accuracy[m] - max(object$models$test_accuracy[1:(m - 1)], na.rm=TRUE), 5), \n            "\\n")\n        if(object$outcome == "binary"){\n          message("pseudo-R^2 (adjusted based on P and N) improvement over best model so far:",\n              round(object$models$test_adjR2[m] - max(object$models$test_adjR2[1:(m - 1)], na.rm=TRUE), 5),\n              "\\n")\n        }\n      }\n    }\n  }\n}\n\n##################################################################\n# predict.polyFit: predict the fitted models on newdata\n##################################################################\n\npredict.polyFit <- function(object, newdata, ...)\n{\n  use <- object$use\n\n  if (is.vector(newdata)) {\n     if (object$nOrigFeatures == 1) {\n        newdata <- matrix(newdata,ncol=1)\n        colnames(newdata) <- object$namesOrigFeatures\n     }\n  }\n  \n  # the next couple dozen lines are devoted to forming plm.newdata, which\n  # will ultimately be fed into predict.lm(), predict.glm() or whatever;\n  # to do this, newdata, the argument above, must be expanded to\n  # polynomial form\n  \n    plm.newdata <- getPoly(newdata, object$degree, \n                           object$maxInteractDeg, \n                           modelFormula = object$XtestFormula,\n                           retainedNames = object$retainedNames,\n                           ...)$xdata\n    \n  \n  if (object$use == "lm") {\n    pred <- predict(object$fit, plm.newdata)\n    return(pred)\n  }\n  \n  if (object$use == "mvrlm") {\n    pre <- predict(object$fit, plm.newdata)\n    pred <- apply(pre,1,which.max)\n    return(pred)\n  }\n  \n  # glm case\n  if (is.null(object$glmMethod)) { # only two classes\n    pre <- predict(object$fit, plm.newdata, type = \'response\')\n    pred <- ifelse(pre > 0.5, object$classes[1], object$classes[2])\n    attr(pred, "prob") <- pre\n\n  } else { # more than two classes\n    len <- length(object$classes)\n    if (object$glmMethod == "multlog") { # multinomial logistics\n      pr <- predict(object$fit, plm.newdata, type="probs")\n      idx <- apply(pr,1, which.max)\n      col.name <- colnames(pr)\n      lc <- length(col.name)\n      tempM <- matrix(rep(col.name, length(idx)), ncol=lc, byrow = TRUE)\n      pred <- NULL\n      for (r in 1:nrow(tempM)) {\n        pred[r] <- tempM[r,idx[r]]\n      }\n      attr(pred, "prob") <- pr\n      return(pred)\n    } # end multinomial logistics\n    else if (object$glmMethod == "all") { # all-vs-all method\n      votes <- matrix(0, nrow = nrow(plm.newdata), ncol = len)\n      for (i in 1:len) {\n        for (j in 1:len) {\n          if (i == j)\n            next\n          pre <- predict(object$fit[[i]][[j]], plm.newdata, type="response")\n          votes[,i] <- votes[,i] + ifelse(pre > 0.5, 1, 0)\n        } # for i\n      } # for j\n      winner <- apply(votes, 1, which.max)\n    } else if (object$glmMethod == "one") { # one-vs-all method\n      prob <- matrix(0, nrow=nrow(plm.newdata), ncol=len)\n      for (i in 1:len) {\n        # prob[,i] <- parSapplyLB(object$fit[[i]],\n        prob[,i] <- predict(object$fit[[i]],\n                            plm.newdata, type = "response")\n      }\n      winner <- apply(prob, 1, which.max)\n    } # one-vs-all method\n    # calculate pred for all-vs-all & one-vs-all\n    pred <- NULL\n    for (k in 1:nrow(plm.newdata)) {\n      pred[k] <- object$classes[winner[k]]\n    }\n    attr(pred, "prob") <- prob\n  } # end more than two classes\n  return(pred)\n  # end glm case\n  \n}\n\n\n'