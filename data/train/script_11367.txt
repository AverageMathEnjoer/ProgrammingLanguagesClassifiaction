b'#################################\n# FSR: Forward Stepwise Regression ###\n#################################\n# FSR() by default, uses 90% of data for training, 20% of which is set aside for validation.\n# FSR() is primarily intended for dense matrices.\n# If you have a data frame that contains factors with many levels,\n# you may wish to use another function found in the polyreg library\n# which takes advantage of sparse data.\n# continues making the model more complicated (adding interactions, polynomials, etc.)\n# until either max_poly_degree and max_interaction_degree are reached or\n# improvements do not add at least threshold (default 0.01) to explained variance (of validation data).\n#\' FSR\n#\' @export\nFSR <- function(Xy,\n                max_poly_degree = 3, max_interaction_degree = 2,\n                outcome = NULL, linear_estimation = FALSE,\n                threshold_include = 0.01, threshold_estimate = 0.001,\n                min_models = NULL, max_fails = 2,\n                standardize = FALSE,\n                pTraining = 0.8,\n                file_name = NULL,\n                store_fit = "none",\n                max_block = 250,\n                noisy = TRUE, seed = NULL){\n\n  if(!is.matrix(Xy) && !is.data.frame(Xy))\n    stop("Xy must be a matrix or data.frame. Either way, y must be the final column.")\n  Xy <- complete(Xy)\n  if(pTraining <= 0 || pTraining > 1)\n    stop("pTraining should all be between 0 and 1.")\n  pValidation <- 1 - pTraining\n  stopifnot(is.numeric(threshold_estimate))\n  stopifnot(is.numeric(threshold_include))\n  stopifnot(is.logical(linear_estimation))\n\n  store_fit <- match_arg(store_fit, c("none", "accepted", "all"))\n  outcome <- match_arg(outcome, c("continuous", "binary", "multinomial"))\n\n  out <- list()\n  class(out) <- "FSR" # nested list, has S3 method\n  out[["standardize"]] <- standardize\n  out[["N"]] <- n <- nrow(Xy)\n\n  out[["seed"]] <- if(is.null(seed)) sample(10^9, 1) else seed\n  set.seed(out$seed)\n  if(noisy) message("set seed to ", out$seed, ".\\n")\n  out[["split"]] <- sample(c("train", "test"), n, replace=TRUE,\n                           prob = c(pTraining, pValidation))\n\n\n  Xy <- as.data.frame(Xy, stringsAsFactors=TRUE)\n  factor_features <- c() # stores individual levels, omitting one\n\n  Xy_distincts <- unlist(lapply(Xy, N_distinct))\n  Xy_ch <- unlist(lapply(Xy, is.character))\n  for(i in 1:ncol(Xy)){\n    if(Xy_distincts[i] == 2 || Xy_ch[i]){\n      Xy[,i] <- as.factor(Xy[,i])\n    }\n  }\n  x_factors <- unlist(lapply(Xy[,-ncol(Xy)], is.factor))\n  for(i in which(x_factors)){\n\n    if(Xy_distincts[i] > 2){\n      tmp <- paste(colnames(Xy)[i], "==", paste0("\\\'", levels(Xy[,i])[-1], "\\\'"))\n      tmp <- paste0("(", tmp, ")")\n      factor_features <- c(factor_features, tmp)\n    }else{\n      factor_features <- c(factor_features, colnames(Xy)[i])\n    }\n  }\n\n  # below code can probably\n  continuous_features <- cf <- colnames(Xy)[-ncol(Xy)][!x_factors]\n  P_continuous <- length(continuous_features)\n\n  out[["continuous_features"]] <- continuous_features\n  out[["factors"]] <- colnames(Xy)[-ncol(Xy)][unlist(lapply(Xy[-ncol(Xy)], is.factor))]\n  out[["y"]] <- colnames(Xy)[ncol(Xy)]\n  out[["P_continuous"]] <- P_continuous\n  out[["P_factor"]] <- P_factor <- sum(x_factors)\n  P <- P_continuous + P_factor # P does not reflect intercept, interactions, or poly\n\n  if(is.null(outcome)){\n    out[["outcome"]] <- if(is.factor(Xy[,ncol(Xy)])) if(N_distinct(Xy[,ncol(Xy)]) > 2) "multinomial" else "binary" else "continuous"\n  }\n\n  out[["linear_estimation"]] <- if(out$outcome == "continuous") TRUE else linear_estimation\n  if(out$linear_estimation)\n    out[["XtX_inv_accepted"]] <- NULL\n\n  out[["y_scale"]] <- if(standardize && out$outcome == "continuous") sd(Xy[out$split == "train", ncol(Xy)]) else 1\n\n  out[["train_scales"]] <- list()\n  if(standardize){\n    tmp <- which(unlist(lapply(Xy, is_continuous)))\n    for(i in tmp){\n      out[["train_scales"]][[colnames(Xy)[i]]][["mean"]] <- mean(Xy[,i], na.rm=TRUE)\n      out[["train_scales"]][[colnames(Xy)[i]]][["sd"]] <- sd(Xy[,i], na.rm=TRUE)\n    }\n    Xy[,tmp] <- scale(Xy[,tmp])\n  }\n\n  for(i in 2:max_poly_degree)\n    continuous_features <- c(continuous_features, paste0("I(", cf, "^", i, ")"))\n#    continuous_features <- c(continuous_features, paste("pow(", cf, ",", i, ")"))\n\n  features <- f <- c(continuous_features, factor_features)\n  if(max_interaction_degree > 1){\n    for(i in 2:max_interaction_degree){\n      features <- c(f, apply(combn(f, i), 2, paste, collapse = " * "))\n    }\n  }\n\n\n  models <- data.frame(features = features,\n                       test_adjR2 = NA,\n                       stringsAsFactors = FALSE)\n  models[["estimated"]]<- FALSE # will be updated based on whether successful ...\n\n  if(out$outcome == "multinomial"){\n    out[["best_test_adj_accuracy"]] <- 0\n    models[["test_adj_accuracy"]] <- 0\n  }else{\n    out[["best_test_adjR2"]] <- 0\n    # computes pseudo, most important for logit. for ols, multiple definitions of R^2 equivalent including squared correlation.\n    # applies the adjustment found in adj R^2 even though out of sample ...\n  }\n\n  out[["improvement"]] <- 0  # 0 not meaningful; just initializing ...\n\n  if(out$outcome == "continuous"){\n    models[["MAPE"]] <- NA\n  }else{\n    models[["test_accuracy"]] <- NA\n    if(!out$linear_estimation){\n      models[["AIC"]] <- NA\n      models[["BIC"]] <- NA\n\n    }\n  }\n\n  models[["formula"]] <- NA\n  models[["accepted"]] <- FALSE\n  models[["P"]] <- NA\n  out[["models"]] <- models\n\n  out[["y_name"]] <- as.character(colnames(Xy)[ncol(Xy)])\n  out[["N_train"]] <- N_train <- sum(out$split == "train")\n  out[["N_test"]] <- N_test <- sum(out$split == "test")\n  out[["unable_to_estimate"]] <- 0\n\n  out[["best_formula"]] <- ""\n  if(is.null(min_models))\n    min_models <- min(P, N_train - 1) # \'attempt all x variables additively\'\n\n  out[["max_poly_degree"]] <- max_poly_degree\n  out[["max_interaction_degree"]] <- max_interaction_degree\n  out[["threshold_include"]] <- threshold_include\n  out[["threshold_estimate"]] <- 0.001\n  out[["max_fails"]] <- max_fails\n  out[["min_models"]] <- min_models\n  out[["file_name"]] <- file_name\n  out[["store_fit"]] <- store_fit\n  out[["max_block"]] <- 250\n  out[["noisy"]] <- noisy\n\n  if(noisy) summary(out, results_overview=FALSE)\n\n  m <- 1            # counts which model\n\n  if(out$outcome == "continuous"){\n\n    y_train <- Xy[out$split == "train", ncol(Xy)]\n\n  }else{ # classification setup\n\n    out[["training_labels"]] <- as.character(unique(Xy[out$split == "train", ncol(Xy)]))\n    out[["labels"]] <- Xy[,ncol(Xy)]\n    out[["y_test_labels"]] <- out$labels[out$split == "test"]\n\n    if(out$outcome == "multinomial"){\n\n      tallies <- table(Xy[out$split == "train", ncol(Xy)])\n      if(noisy && min(tallies) < 10){\n        warning("Training the model with rarely observed labels is not recommended.")\n        table(tallies)\n      }\n\n      modal_outcome <- names(tallies)[which.max(tallies)]\n      out[["reference_category"]] <- modal_outcome\n      Xy[ , ncol(Xy)] <- relevel(Xy[,ncol(Xy)], out$reference_category)\n\n      if(out$noisy) message("Multinomial models will be fit with \'",\n                          modal_outcome,\n                          "\' (the sample mode of the training data) as the reference category.\\n\\n")\n\n    }else{\n\n      out[["y_train_mean"]] <- mean(as.numeric(Xy[out$split == "train", ncol(Xy)]) - 1)\n\n    }\n\n    if(out$linear_estimation){\n\n      ln_odds <- log_odds(Xy[,ncol(Xy)], split = (out$split == "train"), noisy = noisy)\n\n      if(out$outcome == "binary"){\n\n        Xy[ , ncol(Xy)] <- ln_odds\n        y_train <- ln_odds[out$split == "train"]\n\n      }else{\n        y_train <- ln_odds[out$split == "train", ]\n      }\n    }else{\n      y_train <- Xy[out$split == "train", ncol(Xy)]\n      y_test <- Xy[out$split == "test", ncol(Xy)]\n    }\n  } # end classification setup\n\n if(noisy) message("beginning Forward Stepwise Regression...")\n\n while((m <= nrow(out$models)) &&\n        ((out$improvement > out$threshold_estimate) || m <= out$min_models) &&\n        out$unable_to_estimate < out$max_fails){\n\n    out[[mod(m)]] <- list()\n\n    if(sum(out$models$accepted)){\n\n      if(grepl("\\\\*", out$models$features[m])){\n\n        tmp <- unlist(strsplit(out$models$features[m], "\\\\*"))\n        if(mean(tmp %in% out$models$features[out$models$accepted]) == 1){\n          out$models$formula[m] <- paste(out[["best_formula"]], "+", out$models$features[m])\n        }else{\n          if(noisy) message("Skipping ", out$models$features[m], "\\n")\n        }\n      }else{\n        out$models$formula[m] <- paste(out[["best_formula"]], "+", out$models$features[m])\n      }\n    }else{\n      out$models$formula[m] <- paste(out$y_name, "~", out$models$features[m])\n    }\n\n    if(!is.na(out$models$formula[m])){\n\n      if(out$outcome == "continuous"){ # fit, etc.\n\n        system.time(out <- ols(out, Xy, m, y = y_train))\n\n      }else{ # begin classification\n\n        if(out$outcome == "multinomial"){\n\n          if(out$linear_estimation){\n\n            system.time(out <- ols(out, Xy, m, y = y_train))\n\n          }else{\n\n            if(noisy) message("\\n")\n            system.time(out[[mod(m)]][["fit"]] <- multinom(as.formula(out$models$formula[m]),\n                                                           Xy[out$split == "train", ], trace = noisy))\n            out[[mod(m)]][["coeffs"]] <- t(as.matrix(coefficients(out[[mod(m)]][["fit"]])))\n            out <- post_estimation(out, Xy, m)\n\n          }\n        }else{ # start binary\n\n          if(out$linear_estimation){\n            system.time(out <- ols(out, Xy, m, y = y_train))\n          }else{\n            system.time(out[[mod(m)]][["fit"]] <- glm(as.formula(out$models$formula[m]),\n                                                      Xy[out$split == "train",],\n                                                      family = binomial(link = "logit")))\n            out[[mod(m)]][["coeffs"]] <- out[[mod(m)]][["fit"]][["coefficients"]]\n            out <- post_estimation(out, Xy, m, y_test)\n          }\n        } # end logit\n      } # end fit, etc.\n\n      out$models$estimated[m] <- complete_vector(out[[mod(m)]][["coeffs"]])\n\n      if(out$noisy)\n        summary(out, estimation_overview = FALSE, results_overview = FALSE, model_number = m)\n\n      # saving or removing ...\n      if(out$store_fit == "none")\n        out[[mod(m)]][["fit"]] <- NULL\n\n      if(out$store_fit == "accepted" && !out$models$accepted[m])\n        out[[mod(m)]][["fit"]] <- NULL\n\n      if(!is.null(out$file_name)){\n        if(out$noisy) message("\\n\\nsaving (updated) results as ", out$file_name, "\\n\\n")\n        save(out, file=out$file_name)\n      }\n\n    }\n    m <- m + 1\n\n  } # end WHILE loop\n\n  out$XtX_inv_accepted <- NULL\n\n  if(out$noisy) summary(out, estimation_overview=FALSE)\n\n  return(out)\n\n}\n\n\n'