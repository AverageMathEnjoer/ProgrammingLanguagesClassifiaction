b'#\' Apply a \'regularized log\' transformation\n#\'\n#\' This function transforms the count data to the log2 scale in a way \n#\' which minimizes differences between samples for rows with small counts,\n#\' and which normalizes with respect to library size.\n#\' The rlog transformation produces a similar variance stabilizing effect as\n#\' \\code{\\link{varianceStabilizingTransformation}},\n#\' though \\code{rlog} is more robust in the\n#\' case when the size factors vary widely.\n#\' The transformation is useful when checking for outliers\n#\' or as input for machine learning techniques\n#\' such as clustering or linear discriminant analysis.\n#\' \\code{rlog} takes as input a \\code{\\link{DESeqDataSet}} and returns a\n#\' \\code{\\link{RangedSummarizedExperiment}} object.\n#\'\n#\' Note that neither rlog transformation nor the VST are used by the\n#\' differential expression estimation in \\code{\\link{DESeq}}, which always\n#\' occurs on the raw count data, through generalized linear modeling which\n#\' incorporates knowledge of the variance-mean dependence. The rlog transformation\n#\' and VST are offered as separate functionality which can be used for visualization,\n#\' clustering or other machine learning tasks. See the transformation section of the\n#\' vignette for more details, including a statement on timing. If \\code{rlog}\n#\' is run on data with number of samples in [30-49] it will print a message\n#\' that it may take a few minutes, if the number of samples is 50 or larger, it\n#\' will print a message that it may take a "long time", and in both cases, it\n#\' will mention that the \\code{\\link{vst}} is a much faster transformation.\n#\'\n#\' The transformation does not require that one has already estimated size factors\n#\' and dispersions.\n#\'\n#\' The regularization is on the log fold changes of the count for each sample\n#\' over an intercept, for each gene. As nearby count values for low counts genes\n#\' are almost as likely as the observed count, the rlog shrinkage is greater for low counts.\n#\' For high counts, the rlog shrinkage has a much weaker effect.\n#\' The fitted dispersions are used rather than the MAP dispersions\n#\' (so similar to the \\code{\\link{varianceStabilizingTransformation}}).\n#\' \n#\' The prior variance for the shrinkag of log fold changes is calculated as follows: \n#\' a matrix is constructed of the logarithm of the counts plus a pseudocount of 0.5,\n#\' the log of the row means is then subtracted, leaving an estimate of\n#\' the log fold changes per sample over the fitted value using only an intercept.\n#\' The prior variance is then calculated by matching the upper quantiles of the observed \n#\' log fold change estimates with an upper quantile of the normal distribution.\n#\' A GLM fit is then calculated using this prior. It is also possible to supply the variance of the prior.\n#\' See the vignette for an example of the use and a comparison with \\code{varianceStabilizingTransformation}.\n#\'\n#\' The transformed values, rlog(K), are equal to\n#\' \\eqn{rlog(K_{ij}) = \\log_2(q_{ij}) = \\beta_{i0} + \\beta_{ij}}{rlog(K_ij) = log2(q_ij) = beta_i0 + beta_ij},\n#\' with formula terms defined in \\code{\\link{DESeq}}.\n#\'\n#\' The parameters of the rlog transformation from a previous dataset\n#\' can be frozen and reapplied to new samples. See the \'Data quality assessment\'\n#\' section of the vignette for strategies to see if new samples are\n#\' sufficiently similar to previous datasets. \n#\' The frozen rlog is accomplished by saving the dispersion function,\n#\' beta prior variance and the intercept from a previous dataset,\n#\' and running \\code{rlog} with \'blind\' set to FALSE\n#\' (see example below).\n#\' \n#\' @aliases rlog rlogTransformation\n#\' @rdname rlog\n#\' @name rlog\n#\' \n#\' @param object a DESeqDataSet, or matrix of counts\n#\' @param blind logical, whether to blind the transformation to the experimental\n#\' design. blind=TRUE should be used for comparing samples in an manner unbiased by\n#\' prior information on samples, for example to perform sample QA (quality assurance).\n#\' blind=FALSE should be used for transforming data for downstream analysis,\n#\' where the full use of the design information should be made.\n#\' blind=FALSE will skip re-estimation of the dispersion trend, if this has already been calculated.\n#\' If many of genes have large differences in counts due to\n#\' the experimental design, it is important to set blind=FALSE for downstream\n#\' analysis.\n#\' @param intercept by default, this is not provided and calculated automatically.\n#\' if provided, this should be a vector as long as the number of rows of object,\n#\' which is log2 of the mean normalized counts from a previous dataset.\n#\' this will enforce the intercept for the GLM, allowing for a "frozen" rlog\n#\' transformation based on a previous dataset.\n#\' You will also need to provide \\code{mcols(object)$dispFit}.\n#\' @param betaPriorVar a single value, the variance of the prior on the sample\n#\' betas, which if missing is estimated from the data\n#\' @param fitType in case dispersions have not yet been estimated for \\code{object},\n#\' this parameter is passed on to \\code{\\link{estimateDispersions}} (options described there).\n#\' \n#\' @return a \\code{\\link{DESeqTransform}} if a \\code{DESeqDataSet} was provided,\n#\' or a matrix if a count matrix was provided as input.\n#\' Note that for \\code{\\link{DESeqTransform}} output, the matrix of\n#\' transformed values is stored in \\code{assay(rld)}.\n#\' To avoid returning matrices with NA values, in the case of a row\n#\' of all zeros, the rlog transformation returns zeros\n#\' (essentially adding a pseudocount of 1 only to these rows).\n#\'\n#\' @references\n#\'\n#\' Reference for regularized logarithm (rlog):\n#\' \n#\' Michael I Love, Wolfgang Huber, Simon Anders: Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome Biology 2014, 15:550. \\url{http://dx.doi.org/10.1186/s13059-014-0550-8}\n#\' \n#\' @seealso \\code{\\link{plotPCA}}, \\code{\\link{varianceStabilizingTransformation}}, \\code{\\link{normTransform}}\n#\' @examples\n#\'\n#\' dds <- makeExampleDESeqDataSet(m=6,betaSD=1)\n#\' rld <- rlog(dds)\n#\' dists <- dist(t(assay(rld)))\n#\' # plot(hclust(dists))\n#\'\n#\' @export\nrlog <- function(object, blind=TRUE, intercept, betaPriorVar, fitType="parametric") {\n  n <- ncol(object)\n  if (n >= 30 & n < 50) {\n    message("rlog() may take a few minutes with 30 or more samples,\nvst() is a much faster transformation")\n  } else if (n >= 50) {\n    message("rlog() may take a long time with 50 or more samples,\nvst() is a much faster transformation")\n  }\n  if (is.null(colnames(object))) {\n    colnames(object) <- seq_len(ncol(object))\n  }\n  if (is.matrix(object)) {\n    matrixIn <- TRUE\n    object <- DESeqDataSetFromMatrix(object, DataFrame(row.names=colnames(object)), ~ 1)\n  } else {\n    matrixIn <- FALSE\n  }\n  if (is.null(sizeFactors(object)) & is.null(normalizationFactors(object))) {\n    object <- estimateSizeFactors(object)\n  }\n  if (blind) {\n    design(object) <- ~ 1\n  }\n  # sparsity test\n  if (missing(intercept)) {\n    sparseTest(counts(object, normalized=TRUE), .9, 100, .1)\n  }\n  if (blind | is.null(mcols(object)$dispFit)) {\n    # estimate the dispersions on all genes\n    if (is.null(mcols(object)$baseMean)) {\n      object <- getBaseMeansAndVariances(object)\n    }\n    object <- estimateDispersionsGeneEst(object, quiet=TRUE)\n    object <- estimateDispersionsFit(object, fitType, quiet=TRUE)\n  }\n  if (!missing(intercept)) {\n    if (length(intercept) != nrow(object)) {\n      stop("intercept should be as long as the number of rows of object")\n    }\n  }\n  rld <- rlogData(object, intercept, betaPriorVar)\n  if (matrixIn) {\n    return(rld)\n  }\n  se <- SummarizedExperiment(\n           assays = rld,\n           colData = colData(object),\n           rowRanges = rowRanges(object),\n           metadata = metadata(object))\n  dt <- DESeqTransform(se)\n  attr(dt,"betaPriorVar") <- attr(rld, "betaPriorVar")\n  if (!is.null(attr(rld,"intercept"))) {\n    mcols(dt)$rlogIntercept <- attr(rld,"intercept")\n  }\n  dt\n}\n\n#\' @rdname rlog\n#\' @export\nrlogTransformation <- rlog\n\n###################### unexported\n\nrlogData <- function(object, intercept, betaPriorVar) {\n  if (is.null(mcols(object)$dispFit)) {\n    stop("first estimate dispersion")\n  }\n  samplesVector <- as.character(seq_len(ncol(object)))\n  if (!missing(intercept)) {\n    if (length(intercept) != nrow(object)) {\n      stop("intercept should be as long as the number of rows of object")\n    }\n  }\n  if (is.null(mcols(object)$allZero) | is.null(mcols(object)$baseMean)) {\n    object <- getBaseMeansAndVariances(object)\n  }\n  \n  # make a design matrix with a term for every sample\n  # this would typically produce unidentifiable solution\n  # for the GLM, but we add priors for all terms except\n  # the intercept\n  samplesVector <- factor(samplesVector,levels=unique(samplesVector))\n  if (missing(intercept)) {\n    samples <- factor(c("null_level",as.character(samplesVector)),\n                      levels=c("null_level",levels(samplesVector)))\n    modelMatrix <- stats::model.matrix.default(~samples)[-1,]\n    modelMatrixNames <- colnames(modelMatrix)\n    modelMatrixNames[modelMatrixNames == "(Intercept)"] <- "Intercept"\n  } else {\n    # or we want to set the intercept using the\n    # provided intercept instead\n    samples <- factor(samplesVector)\n    if (length(samples) > 1) {\n      modelMatrix <- stats::model.matrix.default(~ 0 + samples)\n    } else {\n      modelMatrix <- matrix(1,ncol=1)\n      modelMatrixNames <- "samples1"\n    }\n    modelMatrixNames <- colnames(modelMatrix)\n    if (!is.null(normalizationFactors(object))) { \n      nf <- normalizationFactors(object)\n    } else {\n      sf <- sizeFactors(object)\n      nf <- matrix(rep(sf,each=nrow(object)),ncol=ncol(object))\n    }\n    # if the intercept is not finite, these rows\n    # were all zero. here we put a small value instead\n    intercept <- as.numeric(intercept)\n    infiniteIntercept <- !is.finite(intercept)\n    intercept[infiniteIntercept] <- -10\n    normalizationFactors(object) <- nf * 2^intercept\n    # we set the intercept, so replace the all zero\n    # column with the rows which were all zero\n    # in the previous dataset\n    mcols(object)$allZero <- infiniteIntercept\n  }\n\n  # only continue on the rows with non-zero row sums\n  objectNZ <- object[!mcols(object)$allZero,]\n  stopifnot(all(!is.na(mcols(objectNZ)$dispFit)))\n  \n  # if a prior sigma squared not provided, estimate this\n  # by the matching upper quantiles of the\n  # log2 counts plus a pseudocount\n  if (missing(betaPriorVar)) {\n    logCounts <- log2(counts(objectNZ,normalized=TRUE) + 0.5)\n    logFoldChangeMatrix <- logCounts - log2(mcols(objectNZ)$baseMean + 0.5)\n    logFoldChangeVector <- as.numeric(logFoldChangeMatrix)\n    varlogk <- 1/mcols(objectNZ)$baseMean + mcols(objectNZ)$dispFit\n    weights <- 1/varlogk   \n    betaPriorVar <- matchWeightedUpperQuantileForVariance(logFoldChangeVector, rep(weights,ncol(objectNZ)))\n  }\n  stopifnot(length(betaPriorVar)==1)\n  \n  lambda <- 1/rep(betaPriorVar,ncol(modelMatrix))\n  # except for intercept which we set to wide prior\n  if ("Intercept" %in% modelMatrixNames) {\n    lambda[which(modelMatrixNames == "Intercept")] <- 1e-6\n  }\n  \n  fit <- fitNbinomGLMs(object=objectNZ, modelMatrix=modelMatrix,\n                       lambda=lambda, renameCols=FALSE,\n                       alpha_hat=mcols(objectNZ)$dispFit,\n                       betaTol=1e-4, useOptim=FALSE,\n                       useQR=TRUE)\n  normalizedDataNZ <- t(modelMatrix %*% t(fit$betaMatrix))\n\n  normalizedData <- buildMatrixWithZeroRows(normalizedDataNZ, mcols(object)$allZero)\n\n  # add back in the intercept, if finite\n  if (!missing(intercept)) {\n    normalizedData <- normalizedData + ifelse(infiniteIntercept, 0, intercept)\n  }\n  rownames(normalizedData) <- rownames(object)\n  colnames(normalizedData) <- colnames(object)\n  attr(normalizedData,"betaPriorVar") <- betaPriorVar\n  if ("Intercept" %in% modelMatrixNames) {\n    fittedInterceptNZ <- fit$betaMatrix[,which(modelMatrixNames == "Intercept"),drop=FALSE]\n    fittedIntercept <- buildMatrixWithNARows(fittedInterceptNZ, mcols(object)$allZero)\n    fittedIntercept[is.na(fittedIntercept)] <- -Inf\n    attr(normalizedData,"intercept") <- fittedIntercept\n  }\n  normalizedData\n}\n\nsparseTest <- function(x, p, t1, t2) {\n  rs <- rowSums(x)\n  rmx <- apply(x, 1, max)\n  if (all(rs <= t1)) return(invisible())\n  prop <- (rmx/rs)[rs > t1]\n  total <- mean(prop > p)\n  if (total > t2) warning("the rlog assumes that data is close to a negative binomial distribution, an assumption\nwhich is sometimes not compatible with datasets where many genes have many zero counts\ndespite a few very large counts.\nIn this data, for ",round(total,3)*100,"% of genes with a sum of normalized counts above ",t1,", it was the case \nthat a single sample\'s normalized count made up more than ",p*100,"% of the sum over all samples.\nthe threshold for this warning is ",t2*100,"% of genes. See plotSparsity(dds) for a visualization of this.\nWe recommend instead using the varianceStabilizingTransformation or shifted log (see vignette).")\n}\n'