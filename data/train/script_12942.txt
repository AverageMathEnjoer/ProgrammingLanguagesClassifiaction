b'#\' @useDynLib velocyto.R\n#\' @import MASS\n#\' @import stats\n#\' @import graphics\n#\' @importFrom Matrix colSums rowSums spMatrix Diagonal t rowMeans colMeans rowSums colSums diag\n#\' @importFrom utils read.delim\n#\' @importFrom pcaMethods pca\n#\' @importFrom mgcv gam s\n#\' @importFrom parallel mclapply\n#\' @importFrom cluster pam\n#\' @importFrom Rcpp evalCpp\n#\' @importFrom grDevices adjustcolor colorRampPalette\n#//\' @importFrom Biostrings PWM matchPWM\n#//\' @importFrom GenomicRanges GRanges\n#//\' @importFrom IRanges IRanges\n#//\' @importFrom data.table fread\n#\' @importFrom methods as\nNULL\n\n# optional imports\n# @import igraph\n# @importFrom abind abind\n# @import hdf5r\n# @importFrom edgeR calcNormFactors\n# @import GenomicAlignments\n# @import Rsamtools\n# @importFrom Rtsne Rtsne\n\n\n##\' Estimate RNA velocity using gene-relative slopes\n##\'\n##\' @param emat - spliced (exonic) count matrix\n##\' @param nmat - unspliced (nascent) count matrix\n##\' @param deltaT - amount of time to project the cell forward\n##\' @param smat - optional spanning read matrix (used in offset calculations)\n##\' @param steady.state.cells - optional set of steady-state cells on which the gamma should be estimated (defaults to all cells)\n##\' @param kCells - number of k nearest neighbors (NN) to use in slope calculation smoothing\n##\' @param cellKNN - optional pre-calculated cell KNN matrix\n##\' @param kGenes - number of genes (k) to use in gene kNN pooling\n##\' @param old.fit - optional old result (in this case the slopes and offsets won\'t be recalculated, and the same kNN graphs will be used)\n##\' @param mult - library scaling factor (1e6 in case of FPM)\n##\' @param min.nmat.smat.correlation - minimum required Spearman rank correlation between n and s counts of a gene\n##\' @param min.nmat.emat.correlation - minimum required Spearman rank correlation between n and e counts of a gene\n##\' @param min.nmat.emat.slope - minimum sloope of n~e regression\n##\' @param zero.offset - should offset be set to zero, or determined (through smat regression or using near-0 e cases)\n##\' @param deltaT2 - scaling of the projected difference vector (normally should be set to 1)\n##\' @param fit.quantile perform gamma fit on a top/bottom quantiles of expression magnitudes\n##\' @param diagonal.quantiles whether extreme quantiles should be computed diagonally\n##\' @param show.gene an optional name of a gene for which the velocity estimation details should be shown (instead of estimating all velocities)\n##\' @param do.par whether the graphical device parameters should be reset as part of show.gene (default=TRUE)\n##\' @param cell.dist - cell distance to use in cell kNN pooling calculations\n##\' @param emat.size - pre-calculated cell sizes for the emat (spliced) matrix\n##\' @param nmat.size - pre-calculated cell sizes for the nmat (unspliced) matrix\n##\' @param cell.emb - cell embedding to be used in show.gene function\n##\' @param cell.colors - cell colors to be used in show.gene function\n##\' @param expression.gradient - color palette used to show the expression magnitudes in show.gene function\n##\' @param residual.gradient - color palette used to show the u residuals in show.gene function\n##\' @param n.cores - number of cores to use\n##\' @param verbose - output messages about progress\n##\' @return a list with velocity results, including the current normalized expression state ($current), projected ($projected) over a certain time ($deltaT), unscaled transcriptional change ($deltaE), fit results ($gamma, $ko, $sfit if spanning reads were used), optional cell pooling parameters ($cellKNN, $kCells), kNN-convolved normalized matrices (conv.nmat.norm and conv.emat.norm), library scale ($mult)\n##\' @examples\n##\' \\dontrun{\n##\'  # use min/max quantile gamma fit (recommended option when one can afford to do cell kNN smoothing)\n##\'  # The example below uses k=5 cell kNN pooling, and top/bottom 2% exprssion quantiles\n##\'  # emat and nmat are spliced (exonic) and unspliced (intronic) molecule/read count matirces\n##\' (preferably filtered for informative genes)\n##\'  rvel <- gene.relative.velocity.estimates(emat,nmat,deltaT=1,kCells = 5,fit.quantile = 0.02)\n##\'\n##\'  # alternativly, the function can be used to visualize gamma fit and regression for a\n##\' particular gene. here we pass embedding (a matrix/data frame with rows named with cell names,\n##\' and columns corresponding to the x/y coordinates)\n##\' \n##\'  # and cell colors. old.fit is used to save calculation time.\n##\'  gene.relative.velocity.estimates(emat,nmat,deltaT=1,kCells = 5,fit.quantile = 0.02,\n##\'     old.fit=rvel,show.gene=\'Chga\',cell.emb=emb,cell.colors=cell.colors)\n##\' }\n##\' @export\ngene.relative.velocity.estimates <- function(emat,nmat,deltaT=1,smat=NULL,steady.state.cells=colnames(emat),kCells=10,cellKNN=NULL,kGenes=1,old.fit=NULL,mult=1e3,min.nmat.smat.correlation=0.05,min.nmat.emat.correlation=0.05, min.nmat.emat.slope=0.05, zero.offset=FALSE,deltaT2=1, fit.quantile=NULL, diagonal.quantiles=FALSE, show.gene=NULL, do.par=TRUE, cell.dist=NULL, emat.size=NULL, nmat.size=NULL, cell.emb=NULL, cell.colors=NULL, expression.gradient=NULL,residual.gradient=NULL, n.cores=defaultNCores(), verbose=TRUE) {\n  if(!all(colnames(emat)==colnames(nmat))) stop("emat and nmat must have the same columns (cells)");\n  if(!is.null(smat)) { if(!all(colnames(emat)==colnames(smat))) stop("smat must have the same columns (cells) as emat") }\n  resl <- list();\n  # bring matrices to the same gene set (just in case)\n  vg <- intersect(rownames(emat),rownames(nmat));\n  if(is.null(smat)) {\n    emat <- emat[vg,]; nmat <- nmat[vg,]\n  } else {\n    vg <- intersect(vg,rownames(smat))\n    emat <- emat[vg,]; nmat <- nmat[vg,]; smat <- smat[vg,]\n  }\n  if(!is.null(show.gene)) {\n    if(!show.gene %in% rownames(emat)) { stop(paste("gene",show.gene,"is not present in the filtered expression matrices")) }\n  }\n  # TODO: add gene filtering options\n  pcount <- 1;\n\n  if(!is.null(cell.dist)) {\n    if(class(cell.dist)!=\'dist\') { stop("cell.dist must be of a class dist") }\n    if(!all(labels(cell.dist)==colnames(emat))) {\n      cat("matching cells between cell.dist and emat/nmat ... ")\n      cell.dist <- as.matrix(cell.dist)\n      cn <- intersect(colnames(emat),colnames(cell.dist))\n      cell.dist <- as.dist(cell.dist[cn,cn]);\n      emat <- emat[,cn]; nmat <- nmat[,cn];\n      if(!is.null(smat)) { smat <- smat[,cn] }\n      cat("done\\n")\n    }\n  }\n  \n  # size estimates\n  if(is.null(emat.size)) { emat.size <- Matrix::colSums(emat); }\n  if(is.null(nmat.size)) { nmat.size <- Matrix::colSums(nmat); }\n  emat.cs <- emat.size[colnames(emat)]/mult;\n  nmat.cs <- nmat.size[colnames(nmat)]/mult;\n  \n  \n  emat.log.norm <- log(as.matrix(t(t(emat)/emat.cs))+pcount);\n  if(!is.null(old.fit)) { cellKNN <- old.fit[[\'cellKNN\']]}\n  knn.maxl <- 1e2\n  if(kCells>1) {\n    if(is.null(cellKNN)) {\n      cat("calculating cell knn ... ")\n      if(is.null(cell.dist)) {\n        cellKNN <- balancedKNN(emat.log.norm,kCells,kCells*knn.maxl,n.threads=n.cores);\n      } else {\n        cellKNN <- balancedKNN(emat.log.norm,kCells,kCells*knn.maxl,n.threads=n.cores,dist=cell.dist);\n      }\n      diag(cellKNN) <- 1;\n      resl$cellKNN <- cellKNN;\n      cat("done\\n")\n    }\n    rm(emat.log.norm);\n    # smoothed matrices\n    cat("calculating convolved matrices ... ")\n    conv.emat <- emat %*% cellKNN[colnames(emat),colnames(emat)]\n    conv.nmat <- nmat %*% cellKNN[colnames(nmat),colnames(nmat)]\n    conv.emat.cs <- (emat.cs %*% cellKNN[colnames(emat),colnames(emat)])[1,]\n    conv.nmat.cs <- (nmat.cs %*% cellKNN[colnames(nmat),colnames(nmat)])[1,]\n    cat("done\\n")\n  } else {\n    conv.emat <- emat; conv.nmat <- nmat; cellKNN <- NULL;\n    conv.emat.cs <- emat.cs; conv.nmat.cs <- nmat.cs;\n  }\n  \n  #browser()\n  \n  # size-normalized counts\n  conv.emat.norm <- t(t(conv.emat)/conv.emat.cs)\n  conv.nmat.norm <- t(t(conv.nmat)/conv.nmat.cs)\n\n  # size-normalized counts\n  emat.norm <- t(t(emat)/emat.cs)\n  nmat.norm <- t(t(nmat)/nmat.cs)\n\n  if(kGenes>1) {\n    if(!is.null(old.fit) && !is.null(old.fit$geneKNN)) {\n      geneKNN <- old.fit$geneKNN; \n    } else {\n      cat("gene kNN ... ")\n      geneKNN <- balancedKNN(t(log(as.matrix(conv.emat.norm)+pcount)),kGenes,kGenes*1.2e3,n.threads=n.cores); diag(geneKNN) <- 1;\n    }\n    resl$geneKNN <- geneKNN;\n\n\n    # normalize contribution of different neighbor genes to match the median totals (to avoid distortions due to high-yielding genes)\n    cat("scaling gene weights ... ")\n    gt <- rowSums(conv.emat.norm)\n    scaledGeneKNN <- t(apply(geneKNN,2,function(ii) pmin(1,median(gt[which(ii>0)])/gt) * ii))\n    cat("convolving matrices ... ")\n    conv.emat.norm <- scaledGeneKNN %*% conv.emat.norm;\n    conv.nmat.norm <- scaledGeneKNN %*% conv.nmat.norm;\n    \n    cat("done\\n")\n  }\n  \n  if(!is.null(smat)) {\n    \n    if(kCells>1) {\n      conv.smat <- smat %*% cellKNN[colnames(smat),colnames(smat)]\n    } else {\n      conv.smat <- smat\n    }\n    conv.smat.cs <- Matrix::colSums(conv.smat)/mult;\n    conv.smat.norm <- t(t(conv.smat)/conv.smat.cs)\n\n    if(kGenes>1) {\n      conv.smat.norm <- scaledGeneKNN %*% conv.smat.norm;\n    } \n    \n    # use spanning reads to fit offset for the intronic reads, test correlation\n    if(is.null(old.fit)) {\n      cat("fitting smat-based offsets ... ")\n      sfit <- data.frame(do.call(rbind,parallel::mclapply(sn(rownames(conv.emat.norm)),function(gn) {\n        df <- data.frame(n=(conv.nmat.norm[gn,steady.state.cells]),e=(conv.emat.norm[gn,steady.state.cells]),s=conv.smat.norm[gn,steady.state.cells])\n        sd <- lm(n~s,data=df)\n        r <- with(df[df$s>0,],cor(n,s,method=\'spearman\'),3)\n        return(c(o=pmax(0,as.numeric(sd$coef[1])),s=as.numeric(sd$coef[2]),r=r))\n      },mc.cores=n.cores,mc.preschedule=T)))\n      cat("done\\n")\n      \n    } else {\n      sfit <- old.fit$sfit;\n    }\n  }\n\n  resl$conv.nmat.norm <- conv.nmat.norm;\n  resl$conv.emat.norm <- conv.emat.norm;\n\n  # fit gamma, using the offset above\n  if(!is.null(show.gene)) {\n    gn <- show.gene;\n    if(!is.null(cell.emb)) {\n      # show embedding heatmaps\n      cc <- intersect(rownames(cell.emb),colnames(conv.emat.norm));\n      if(do.par) { par(mfrow=c(1,4), mar = c(2.5,2.5,2.5,0.5), mgp = c(1.5,0.65,0), cex = 0.85); }\n      plot(cell.emb[cc,],pch=21,col=ac(1,alpha=0.2),bg=val2col(conv.emat.norm[gn,cc],gradientPalette=expression.gradient),cex=0.8,xlab=\'\',ylab=\'\',main=paste(gn,\'s\'),axes=F); box();\n      plot(cell.emb[cc,],pch=21,col=ac(1,alpha=0.2),bg=val2col(conv.nmat.norm[gn,cc],gradientPalette=expression.gradient),cex=0.8,xlab=\'\',ylab=\'\',main=paste(gn,\'u\'),axes=F); box();\n    }\n    do <- NULL;\n    if(!is.null(smat)) { # use smat-based offsets\n      df <- data.frame(n=(conv.nmat.norm[gn,steady.state.cells]),e=(conv.emat.norm[gn,steady.state.cells]),o=sfit[gn,\'o\'])\n      if(zero.offset) df$o <- 0;\n    }  else { # calculate offset based on the nascent counts obsered for near-0 exonic levels\n      df <- data.frame(n=(conv.nmat.norm[gn,steady.state.cells]),e=(conv.emat.norm[gn,steady.state.cells]))\n      o <- 0;\n      df$o <- o;\n      #zi <- emat[gn,steady.state.cells]==0;\n      if(!zero.offset) { zi <- df$e<1/conv.emat.cs[steady.state.cells]; if(any(zi)) { o <- sum(df$n[zi])/(sum(zi)+1)} }\n      df$o <- o;\n      \n      #table(zi)\n      # if(any(zi)) {\n      #   do <- lm(n~e,data=df[zi,])\n      #   #summary(do)\n      #   df$o <- max(0,do$coefficients[1])\n      # }\n\n      \n    }\n    \n    \n    #browser()\n    d <- lm(n~e+offset(o)+0,data=df,weights=df$e^4+df$n^4);\n    cell.col <- ac(rep(1,nrow(df)),alpha=0.1); names(cell.col) <- rownames(df)\n    if(!is.null(cell.colors)) { \n      cc <- intersect(names(cell.colors),rownames(df)); \n      cell.col[cc] <- cell.colors[cc]\n    }\n    plot(df$e,df$n,pch=21,bg=ac(cell.col,alpha=0.3),col=ac(1,alpha=0.1),cex=0.8,xlab=\'s\',ylab=\'u\',main=paste(gn,\'fit\'))\n    if(!is.null(do)) {\n      abline(do,lty=2,col=8)\n    }\n    \n    # min/max fit\n    if(!is.null(fit.quantile)) {\n      if(diagonal.quantiles) {\n        # determine maximum ranges \n        emax <- quantile(df$e,p=0.99)\n        nmax <- quantile(df$n,p=0.99)\n        if(emax==0) emax <- max(max(df$e),1e-3)\n        if(nmax==0) nmax <- max(max(df$n),1e-3)\n        x <- df$e/emax + df$n/nmax;\n        eq <- quantile(x,p=c(fit.quantile,1-fit.quantile))\n        if(!is.null(smat)) { # will use smat offset, so disregard lower quantile\n          pw <- as.numeric(x>=eq[2])\n        } else {\n          pw <- as.numeric(x>=eq[2] | x<=eq[1])\n        }\n      } else {\n        eq <- quantile(df$e,p=c(fit.quantile,1-fit.quantile))\n        if(!is.null(smat) || zero.offset) { # will use smat offset, so disregard lower quantile\n          pw <- as.numeric(df$e>=eq[2])\n        } else {\n          pw <- as.numeric(df$e>=eq[2] | df$e<=eq[1])\n        }\n      }\n\n      if(!is.null(smat) || zero.offset) { # use smat offset\n        d <- lm(n~e+offset(o)+0,data=df,weights=pw);\n      } else {\n        d <- lm(n~e,data=df,weights=pw)\n      }\n\n      ## eq <- quantile(df$e,p=c(fit.quantile,1-fit.quantile))\n      ## pw <- as.numeric(df$e>=eq[2] | df$e<=eq[1])\n      ## if(!is.null(smat)) { # use smat offset\n      ##   d <- lm(n~e+offset(o)+0,data=df,weights=pw);\n      ## } else {\n      ##   d <- lm(n~e,data=df,weights=pw)\n      ## }\n    } \n    \n    \n    df <- df[order(df$e,decreasing=T),]; \n    lines(df$e,predict(d,newdata=df),lty=2,col=2)\n\n\n    if(!is.null(cell.emb)) {\n      plot(cell.emb[cc,],pch=21,col=ac(1,alpha=0.2),bg=val2col(resid(d)[cc],gradientPalette=residual.gradient),cex=0.8,xlab=\'\',ylab=\'\',main=paste(gn,\'resid\'),axes=F); box();\n    }\n    if(kGenes>1) { return(invisible(geneKNN)) } else { return(1) }\n  }\n  \n  cat("fitting gamma coefficients ... ")\n  if(is.null(old.fit)) {\n    ko <- data.frame(do.call(rbind,parallel::mclapply(sn(rownames(conv.emat.norm)),function(gn) {\n      if(!is.null(smat)) { # use smat-based offsets\n        df <- data.frame(n=(conv.nmat.norm[gn,steady.state.cells]),e=(conv.emat.norm[gn,steady.state.cells]),o=sfit[gn,\'o\'])\n        if(zero.offset) df$o <- 0;\n      }  else { # calculate offset based on the nascent counts obsered for near-0 exonic levels\n        df <- data.frame(n=(conv.nmat.norm[gn,steady.state.cells]),e=(conv.emat.norm[gn,steady.state.cells]))\n        o <- 0;\n        if(!zero.offset) { zi <- df$e<1/conv.emat.cs[steady.state.cells]; if(any(zi)) { o <- sum(df$n[zi])/(sum(zi)+1)} }\n        df$o <- o;\n      }\n      if(is.null(fit.quantile)) {\n        #d <- lm(n~e+offset(o)+0,data=df,weights=df$e^4+df$n^4);\n        d <- lm(n~e+offset(o)+0,data=df,weights=df$e^4+df$n^4);\n        return(c(o=df$o[1],g=as.numeric(coef(d)[1]),r=cor(df$e,df$n,method=\'spearman\')))\n      } else {\n        if(diagonal.quantiles) {\n          # determine maximum ranges \n          emax <- quantile(df$e,p=0.99)\n          nmax <- quantile(df$n,p=0.99)\n          if(emax==0) emax <- max(max(df$e),1e-3)\n          if(nmax==0) nmax <- max(max(df$n),1e-3)\n          x <- df$e/emax + df$n/nmax;\n          eq <- quantile(x,p=c(fit.quantile,1-fit.quantile))\n          if(!is.null(smat)) { # will use smat offset, so disregard lower quantile\n            pw <- as.numeric(x>=eq[2])\n          } else {\n            pw <- as.numeric(x>=eq[2] | x<=eq[1])\n          }\n        } else {\n          eq <- quantile(df$e,p=c(fit.quantile,1-fit.quantile))\n          if(!is.null(smat) || zero.offset) { # will use smat offset, so disregard lower quantile\n            pw <- as.numeric(df$e>=eq[2])\n          } else {\n            pw <- as.numeric(df$e>=eq[2] | df$e<=eq[1])\n          }\n        }\n        if(!is.null(smat) || zero.offset) { # use smat offset\n          d <- lm(n~e+offset(o)+0,data=df,weights=pw);\n          return(c(o=df$o[1],g=as.numeric(coef(d)[1]),r=cor(df$e,df$n,method=\'spearman\')))\n        } else {\n          d <- lm(n~e,data=df,weights=pw)\n          # note: re-estimating offset here\n          return(c(o=as.numeric(coef(d)[1]),g=as.numeric(coef(d)[2]),r=cor(df$e,df$n,method=\'spearman\')))\n        }\n      }\n        \n    },mc.cores=n.cores,mc.preschedule=T)))\n    ko <- na.omit(ko)\n    cat("done. succesfful fit for",nrow(ko),"genes\\n")\n  } else { full.ko <- ko <- na.omit(old.fit$ko); }\n\n  if(!is.null(smat)) {\n    sfit <- na.omit(sfit)\n    ko <- ko[rownames(ko) %in% rownames(sfit),]; # omit genes for which sfit didn\'t work\n    vi <- sfit$r > min.nmat.smat.correlation\n    ko <- ko[vi,]\n    if(!all(vi)) cat("filtered out",sum(!vi),"out of",length(vi),"genes due to low nmat-smat correlation\\n")\n  }\n  \n  full.ko <- ko;\n  vi <- ko$r>min.nmat.emat.correlation\n  if(!all(vi)) cat("filtered out",sum(!vi),"out of",length(vi),"genes due to low nmat-emat correlation\\n")\n  ko <- ko[vi,]\n  \n  vi <- ko$g>min.nmat.emat.slope\n  if(!all(vi)) cat("filtered out",sum(!vi),"out of",length(vi),"genes due to low nmat-emat slope\\n")\n  ko <- ko[vi,]\n\n  gamma <- ko$g; offset <- ko$o; names(gamma) <- names(offset) <- rownames(ko);\n  cat("calculating RNA velocity shift ... ")\n  if(kGenes>1) { # gene-convolved estimation\n    # estimate M value\n    npred <- gamma*conv.emat.norm[names(gamma),] + ko$o;\n    npred[npred<0] <- 0;\n    mval <- log2(conv.nmat.norm[names(gamma),]+pcount) - log2(npred+pcount);\n    resl$mval <- mval;\n    \n    #resl$conv.deltaE <- t.get.projected.delta(conv.emat.norm,conv.nmat.norm,gamma,offset=offset,delta=deltaT)\n    #resl$conv.projected <- t.get.projected.cell2(conv.emat.norm,emat.size,as.matrix(resl$conv.deltaE),mult = mult,delta=deltaT2);\n    #resl$conv.projected[resl$conv.projected<0] <- 0;\n    \n    # switch back to non-gene-kNN conv.* matrices\n    conv.emat.norm <- t(t(conv.emat)/conv.emat.cs)\n    conv.nmat.norm <- t(t(conv.nmat)/conv.nmat.cs)\n    # estimate gamma\n    cat("re-estimating gamma of individual genes ... ")\n    \n    am <- conv.nmat.norm[rownames(mval),]-offset; am[am<0] <- 0;\n    fm <- log2(am) - mval - log2(conv.emat.norm[rownames(mval),])\n    wm <- is.finite(fm)\n    fm[!is.finite(fm)] <- 0;\n    gammaA <- 2^(rowSums(fm * wm)/rowSums(wm))\n    gammaA <- gammaA[is.finite(gammaA)];\n \n    \n    gamma <- gammaA;\n    cat("done\\n")\n\n    \n    # can estimate deltaE from the mval\n    cat("calculating RNA velocity shift ... ")  \n    # estimate delta from M value\n    deltaE <- t.get.projected.delta.from.log2ratio(em=conv.emat.norm,gamma=gamma,r=mval,delta=deltaT)\n    #deltaE <- t.get.projected.delta2(conv.emat.norm,conv.nmat,conv.nmat.cs,gamma,offset=offset,delta=deltaT)\n  } else { # regular estimation\n    #deltaE <- t.get.projected.delta2(conv.emat.norm,conv.nmat,conv.nmat.cs,gamma,offset=offset,delta=deltaT)\n    deltaE <- t.get.projected.delta(conv.emat.norm,conv.nmat.norm,gamma,offset=offset,delta=deltaT)\n  }\n\n  resl$gamma <- gamma;\n    \n  cat("done\\n")\n  cat("calculating extrapolated cell state ... ")\n\n \n  # reduced cell normalization (only genes for which momentum was estimated)\n  emat.norm <- emat[rownames(emat) %in% rownames(deltaE),]\n  #emat.sz <- Matrix::colSums(emat.norm)/mult;\n  #browser()\n  emat.sz <- emat.cs;\n  emat.norm <- t(t(emat.norm)/(emat.sz));\n  \n  emn <- t.get.projected.cell2(emat.norm,emat.sz,as.matrix(deltaE),mult = mult,delta=deltaT2);\n  #emn <- t.get.projected.cell(emat.norm,as.matrix(deltaE),target.mult = mult,model.mult=mult,delta=deltaT2,size.normalize=FALSE);\n  \n  #table(emat.norm[,cn]==0)\n  #table(emn[,cn]==0)\n  \n  cat("done\\n")\n  full.ko$valid <- rownames(full.ko) %in% rownames(ko)\n  resl <- c(resl,list(projected=emn,current=emat.norm,deltaE=deltaE,deltaT=deltaT,ko=full.ko,mult=mult,kCells=kCells));\n  if(!is.null(smat)) { resl$sfit <- sfit }\n  return(resl)\n}\n\n##\' Structure-based gene velocity estimation\n##\'\n##\'\n##\' @param emat - spliced (exonic) count matrix\n##\' @param nmat - unspliced (nascent) count matrix\n##\' @param vel - initial gene-relative velocity estimates (output of the gene.relative.velocity.estimates function) \n##\' @param base.df gene structure information data frame ($gene.df in output of read.gene.mapping.info()), containing the following columns ($il - total intronic length in log10(length+1) scale; $el - total exonic length; $nex - number of expressed (above some low threshold) exons; as well as optional $nipconc/$nipdisc giving number of concordant and discordant internal priming sites)\n##\' @param deltaT - amount of time to project the cell forward\n##\' @param smat - optional spanning read matrix (used in offset calculations)\n##\' @param kGenes - number of genes to use in evaluating trimmed mean of M values\n##\' @param kGenes.trim - number of genes to trim (from both ends)\n##\' @param smooth.kGenes - gene kNN pooling k value (used in the initial gene-relative fit)\n##\' @param kCells - number of k nearest neighbors (NN) to use in slope calculation smoothing\n##\' @param deltaT2 - scaling of the projected difference vector (normally should be set to 1)\n##\' @param min.gene.conuts - minimum number of spliced reads/molecules that a gene should have\n##\' @param min.gene.cells - minimum number of cells in which a gene should be expressed\n##\' @param min.intron.length - minimum exon length\n##\' @param min.exon.length - minimum exon length\n##\' @param top.global.pearson.deviance - maximum deviance threshold to filter out genes with very high unsplied counts (likely due to other processes)\n##\' @param cellKNN - optional pre-calculated cell KNN matrix\n##\' @param cell.dist - cell distance to use in cell kNN pooling calculations\n##\' @param fit.quantile perform gamma fit on a top/bottom quantiles of expression magnitudes\n##\' @param zero.offset force gene offsets to be zero (default if smat is not supplied), otherwise estimated from the lower quantile or quantile fit\n##\' @param diagonal.quantiles whether diagonal quantile determination should be used (if fit.quantile is specified)\n##\' @param m.pcount - pseudocount to be used in M value calculations (defaults to 5)\n##\' @param plot.model.fit plot gamma values predicted by the structure-bsaed model as a function of gene-relative gamma estimates.\n##\' @param n.cores - number of cores to use\n##\' @return a list with velocity results, including the current normalized expression state ($current), projected ($projected), unscaled transcriptional change ($deltaE), fit results ($ko, $sfit), optional cell pooling parameters ($cellKNN, $kCells), kNN-convolved normalized matrices (conv.nmat.norm and conv.emat.norm)\n##\' @examples\n##\' \\dontrun{\n##\'  # emat / nmat are the spliced/unpsliced matrices respectively\n##\'  # rvel is a gene-relative velocity estimate\n##\'  # base.df (here dat$base.df) is a gene information table.\n##\'  #   For SMART-seq2, it is part of the \\code{\\link{read.smartseq2.bams}} output.\n##\'  #   For droplet data, this info can be obtained \\code{\\link{}}\n##\'  gvel <- global.velcoity.estimates(emat, nmat, rvel, dat$base.df, deltaT=1, kCells=5,\n##\'        kGenes = 15, kGenes.trim = 5, min.gene.cells = 0, min.gene.conuts = 500)\n##\'  \n##\' }\n##\' @export\nglobal.velcoity.estimates <- function(emat,nmat,vel,base.df,deltaT=1,smat=NULL,kGenes=15,kGenes.trim=5,smooth.kGenes=0,kCells=10,deltaT2=1,min.gene.conuts=100,min.gene.cells=20,min.intron.length=10^3.5,min.exon.length=10^2.7,top.global.pearson.deviance=3,cellKNN=NULL,cell.dist=NULL,fit.quantile=NULL, zero.offset=NULL, diagonal.quantiles=FALSE, m.pcount=5,plot.model.fit=FALSE, n.cores=defaultNCores()) {\n\n  if(is.null(zero.offset)) zero.offset <- is.null(smat); # set zero offset to true unless we have smat data\n  \n  mult <- vel$mult; # use the same library scale as in the supplied relative velocity estimates\n  # reconsile gene lists\n  gi <- intersect(intersect(rownames(base.df),rownames(emat)),rownames(nmat))\n  emat <- emat[gi,]; nmat <- nmat[gi,]; \n  base.df <- base.df[gi,]\n\n  # do some gene filtering\n  vi <- rowSums(emat[rownames(base.df),])>min.gene.conuts & rowSums(emat[rownames(base.df),]>0)>min.gene.cells\n  if(!all(vi)) cat("filtered out",sum(!vi),"out of",length(vi),"genes due to low emat levels\\n")\n  base.df <- base.df[vi,]\n  \n  vi <- base.df$il>log10(min.intron.length) & base.df$el>log10(min.exon.length) #requiring some minimum intronic and exonic lengths\n  if(!all(vi)) cat("filtered out",sum(!vi),"out of",length(vi),"genes due to insufficient exonic or intronic lengths\\n")\n  base.df <- base.df[vi,]\n\n  mult <- vel$mult;\n  \n  # do a quick global model of total nascent reads as a function of total exonic reads and gene structural parameters to filter \n  # out the genes with very high nascent/exonic ratio - those are likely driven by other transcripts\n  df <- data.frame(e=rowSums(emat[rownames(base.df),]), n=rowSums(nmat[rownames(base.df),]), base.df)\n  #df$ir <- expr.lstat[rownames(df),\'t\']/expr.lstat[rownames(df),\'i\']\n  df$eir <- base.df$il/base.df$el\n  df$e <- log(df$e)\n  gm <- MASS::glm.nb(n~.,data=df,link=log,init.theta=2)\n  vi <- resid(gm,type=\'pearson\') <= top.global.pearson.deviance;\n  if(!all(vi)) cat("filtered out",sum(!vi),"out of",length(vi),"genes due to excessive nascent counts\\n")\n  base.df <- base.df[vi,] \n  \n  # reconcile gene lists and matrices\n  gi <- intersect(rownames(base.df),rownames(emat))\n  emat <- emat[gi,]; nmat <- nmat[gi,]; \n  base.df <- base.df[gi,]\n  \n  # start with gene-relative slopes\n  gamma <- vel$gamma;\n  gamma <- gamma[intersect(rownames(base.df),names(gamma))]\n  #gamma <- ko$g; offset <- ko$o; names(gamma) <- names(offset) <- rownames(ko);\n  cat("using relative slopes for",length(gamma),"genes to fit structure-based model ... ")\n  \n  df <- data.frame(k=log(gamma),base.df[names(gamma),]); # note we\'re working with log gamma to get good resolution of low values\n  df$eir <- log(((10^df$il)-1)/((10^df$el)-1)) # intronic/exonic ratio\n  df$e <- log(rowSums(emat[rownames(df),])) # total gene expression\n  # genome-wide model fit\n  # fit genome-wide model for the slope, based on the gene structural parameters and the total expression (exonic) magnitude\n  if(\'nipconc\' %in% colnames(base.df)) {\n    cat("with internal priming info ... ")\n    df <- cbind(df,data.frame(log10(base.df[names(gamma),c("nipconc","nipdisc")]+1))) \n    km <- mgcv::gam(k~s(il,e)+s(eir)+s(nex)+s(nipconc)+s(nipdisc),data=df,weights=sqrt(rowSums(emat[rownames(df),])))\n  } else {\n    km <- mgcv::gam(k~s(il,e)+s(eir)+s(nex),data=df,weights=sqrt(rowSums(emat[rownames(df),])))\n  }\n  cat(paste0(round((1-km$deviance/km$null.deviance)*100,1),"% deviance explained.\\n"))\n\n  if(plot.model.fit) {\n    plot(df$k,predict(km),xlab=expression(paste(\'log[ gene-relative \',gamma,\']\')),ylab=expression(paste(\'log[ structure-based \',gamma,\']\')),pch=19,cex=0.7,col=ac(1,alpha=0.2));\n    abline(a=0,b=1,col=2,lty=2)\n    legend(x=\'bottomright\',bty=\'n\',legend=c(paste0(round((1-km$deviance/km$null.deviance)*100,1),"% deviance explained")))\n  }\n  \n  \n  # generate predictions for all genes\n  df <- base.df; # note we\'re working with log gamma to get good resolution of low values\n  df$eir <- log(((10^df$il)-1)/((10^df$el)-1)) # intronic/exonic ratio\n  df$e <- log(rowSums(emat[rownames(df),])) # total gene expression\n  cat("predicting gamma ... ")\n  sqGammaPred <- predict(km,newdata=df,type=\'response\')\n  cat("done\\n")\n  \n  # re-estimate offsets (and cellKNN) using relative fit\n  cat("refitting offsets ... ")\n  vel2 <- gene.relative.velocity.estimates(emat,nmat,smat=smat,kCells=kCells,kGenes=1,cell.dist=cell.dist,fit.quantile=fit.quantile,zero.offset=zero.offset,diagonal.quantiles=diagonal.quantiles)\n  ko <- vel2$ko;\n  cat("re-estimated offsets for",nrow(ko),"out of",nrow(emat),"genes\\n")\n  emat <- emat[rownames(ko),]; nmat <- nmat[rownames(ko),]\n  sqGammaPred <- sqGammaPred[rownames(ko)]\n  if(kCells>1) {\n    cellKNN <- vel2$cellKNN;\n    cat("calculating convolved matrices ... ")\n    conv.emat <- emat %*% cellKNN[colnames(emat),colnames(emat)]\n    conv.nmat <- nmat %*% cellKNN[colnames(nmat),colnames(nmat)]\n    cat("done\\n")    \n  } else {\n    conv.emat <- emat; conv.nmat <- nmat;\n  }\n  \n  # size estimates\n\n  conv.emat.cs <- Matrix::colSums(conv.emat)/mult;\n  conv.nmat.cs <- Matrix::colSums(conv.nmat)/mult;\n  # size-normalized counts\n  conv.emat.norm <- t(t(conv.emat)/conv.emat.cs)\n  conv.nmat.norm <- t(t(conv.nmat)/conv.nmat.cs)\n  \n\n  \n  # TODO: we want to restrict the set of genes that can be neighbors\n  cat("calculating gene knn ... ")\n  emm <- log10(as.matrix(conv.emat.norm)+1)\n  gknn <- balancedKNN(t(emm),kGenes,nrow(emm),n.threads=n.cores); diag(gknn) <- 1;\n  cat("done\\n")\n  cat("estimating M values ... ")\n  # amount of nascent transcription predicted by the model-based k esimates\n  npred <- t(t(conv.emat.norm[names(sqGammaPred),]*exp(as.numeric(sqGammaPred)))*conv.nmat.cs)\n  # adjust for offset\n  npred <- npred+ko$o\n  \n  mval <- log2((conv.nmat[rownames(npred),]+m.pcount)/(npred+m.pcount))\n  cat("adjusting mval offsets ... ")\n  # estimate median M value (log2 observed nascent / expected nascent ratio) across kNN genes\n  mmval <- do.call(rbind,parallel::mclapply(sn(colnames(gknn)),function(gn) {\n    gin <- names(which(gknn[,gn]>0)) # neighbor gene names\n    #x <- apply(mval[gin,],2,median) # works well too\n    x <- apply(mval[gin,],2,mean,trim=kGenes.trim)\n  },mc.cores=n.cores,mc.preschedule=TRUE))\n  \n  if(kGenes>1) { # gene-convolved estimation\n    # switch back to non-gene-kNN conv.* matrices\n    conv.emat.norm <- t(t(conv.emat)/conv.emat.cs)\n    conv.nmat.norm <- t(t(conv.nmat)/conv.nmat.cs)\n  }\n\n  # adjust gamma predictions\n  cat("re-estimating gamma ... ")\n\n  offset <- ko[,\'o\']\n\n  ### alternative gamma fit procedure\n  ## gammaA <- unlist(mclapply(sn(rownames(mval)),function(gn) {\n  ##   # here we try to optimize k to match the expression change based on mmval\n  ##   df <- data.frame(n=conv.nmat.norm[gn,],e=conv.emat.norm[gn,],m=2^mval[gn,],o=ko[gn,\'o\'])\n  ##   df$na <- df$n-df$o; df$na[df$na<0] <- 0; # apply offset\n  ##   pc <- mean(df$e)/5; # min count\n  ##   # fitness function, capturing discrepancy in deltaE\n  ##   #f <- function(k) { ep <- (df$e+pc)*df$m - df$e; np <- df$n/k-df$e; sum(sqrt(abs(ep-np))) }\n  ##   f <- function(k) { egt <- exp(-k); ep <- (df$e+pc)*(egt*(1-df$m)+df$m) - df$e; np <- df$e*egt + (df$na)/k*(1-egt)-df$e; sum(sqrt(abs(ep-np))) }\n  ##   # poor man\'s optimization here, to guide interval methods\n  ##   iv <- 10^seq(-4,2,by=0.1)\n  ##   ivv <- unlist(lapply(iv,f))\n  ##   mi <- which.min(ivv)\n  ##   ov <- optim(iv[mi],f,lower=iv[max(1,mi-2)],upper=iv[min(length(iv),mi+2)],method=\'L-BFGS-B\')\n  ##   if(ov$value<ivv[mi]) { return((ov$par))} else { return((iv[mi]))}\n  ## },mc.cores=n.cores,mc.preschedule=T))\n\n\n  am <- conv.nmat.norm[rownames(mmval),]-offset; am[am<0] <- 0;\n  fm <- log2(am) - mmval - log2(conv.emat.norm[rownames(mmval),])\n  wm <- is.finite(fm)\n  fm[!is.finite(fm)] <- 0;\n  gammaA <- 2^(rowSums(fm * wm)/rowSums(wm))\n  gammaA <- gammaA[is.finite(gammaA)];\n\n  #plot(log(old.gammaA),log(gammaA)); abline(a=0,b=1,lty=2,col=2);\n  \n  cat("done\\n")\n  \n  # can estimate deltaE from the mval\n  cat("calculating RNA velocity shift ... ")  \n  deltaE <- t.get.projected.delta.from.log2ratio(em=conv.emat.norm,gamma=gammaA,r=mmval,delta=deltaT)\n  cat("done\\n")\n  cat("calculating extrapolated cell state ... ")\n  emat.size <- Matrix::colSums(emat)/mult;\n  \n  em <- as.matrix(t(t(emat)/emat.size))\n  em <- em[rownames(em) %in% rownames(deltaE),]\n  emn <- t.get.projected.cell2(em,emat.size,as.matrix(deltaE),mult = mult,delta=deltaT2);\n  cat("done\\n")\n  resl <- list(projected=emn,current=em,deltaE=deltaE,deltaT=deltaT,mval=mval,mult=mult,vel2=vel2,gammaA=gammaA);\n  return(resl)\n  \n}\n\n##\' Filter genes by requirining minimum average expression within at least one of the provided cell clusters\n##\'\n##\' @param emat spliced (exonic) count matrix\n##\' @param clusters named cell factor defining clusters\n##\' @param min.max.cluster.average required minimum average expression count (no normalization is perfomed)\n##\' @return filtered emat matrix\n##\' @export\nfilter.genes.by.cluster.expression <- function(emat,clusters,min.max.cluster.average=0.1) {\n  if(!any(colnames(emat) %in% names(clusters))) stop("provided clusters do not cover any of the emat cells!")\n  vc <- intersect(colnames(emat),names(clusters))\n  cl.emax <- apply(do.call(cbind,tapply(vc,as.factor(clusters[vc]),function(ii) Matrix::rowMeans(emat[,ii]))),1,max)\n  vi <- cl.emax>min.max.cluster.average;\n  emat[vi,]\n}\n\n##\' PCA-based visualization of the velocities\n##\'\n##\' @param vel velocity estimation (gene-relative or global)\n##\' @param nPcs number of successive PCs to visualize\n##\' @param cell.colors a named vector of cell colors for visualization\n##\' @param scale scale to use for expression state transform (default: \'log\', other possible values are \'sqrt\',\'linear\')\n##\' @param plot.cols number of columns into which to arrange the plots\n##\' @param norm.nPcs optional total number of PCs to use for velocity magnitude normalization\n##\' @param do.par whether to set up graphical parameters of a plot\n##\' @param pc.multipliers an optional vector multipliers for the cell PC scores (useful for reorienting the PCs)\n##\' @param show.grid.flow whether a grid flow should be shown\n##\' @param grid.n number of grid points (on each axis)\n##\' @param grid.sd standard deviation of the grid\n##\' @param arrow.scale scale multiplier for the velocity estimates\n##\' @param min.grid.cell.mass minimum cellular mass\n##\' @param min.arrow.size minimum size of an arrow to show\n##\' @param pcount pseudocount\n##\' @param arrow.lwd thickness of arrows to plot\n##\' @param size.norm whether to rescale current and projected states by cell size (default=FALSE)\n##\' @param return.details whether to return detailed output\n##\' @param plot.grid.points whether to show dots at every grid point\n##\' @param fixed.arrow.length whether to use fixed-size arrow\n##\' @param max.grid.arrow.length limit to the size of the arrows that could be shown (when fixed.arrow.length=FALSE)\n##\' @param n.cores number of cores to use in the calculations\n##\' @param ... extra parameters are passed to plot() function\n##\' @return If return.details=F, returns invisible list containing PCA info (epc) and projection of velocities onto the PCs (delta.pcs). If return.details=T, returns an extended list that can be passed into p1 app for velocity visualization.\n##\' @export\npca.velocity.plot <- function(vel,nPcs=4,cell.colors=NULL,scale=\'log\',plot.cols=min(3,nPcs-1),norm.nPcs=NA,do.par=T, pc.multipliers=NULL, show.grid.flow=FALSE, grid.n=20, grid.sd=NULL, arrow.scale=1, min.grid.cell.mass=1, min.arrow.size=NULL, pcount=1, arrow.lwd=1, size.norm=FALSE, return.details=FALSE, plot.grid.points=FALSE, fixed.arrow.length=FALSE,max.grid.arrow.length=NULL, n.cores=defaultNCores(), ...) {\n  x0 <- vel$current;\n  x1 <- vel$projected;\n  if(is.null(cell.colors)) { cell.colors <- ac(rep(1,ncol(x0)),alpha=0.3); names(cell.colors) <- colnames(x0) }\n  # rescale to the same size\n  if(size.norm) {\n    cat("rescaling ... ")\n    sz <- Matrix::colSums(x0);\n    x0 <- t(t(x0)/sz)*mean(sz)\n    x1 <- t(t(x1)/Matrix::colSums(x1))*mean(sz);\n  }\n  # transform\n  if(scale==\'log\') { \n    cat("log ... ")\n    x0.log <- log2(x0+pcount)\n    x1.log <- log2(x1+pcount)\n  } else if(scale==\'sqrt\') {\n    cat("sqrt ... ")\n    x0.log <- sqrt(x0)\n    x1.log <- sqrt(x1)\n  } else { # linear\n    cat("linear ... ")\n    x0.log <- x0\n    x1.log <- x1\n  }\n  \n  cat("pca ... ")\n  cent <- rowMeans(x0.log);\n  epc <- pcaMethods::pca(t(x0.log-cent),center=F,nPcs=ifelse(is.na(norm.nPcs),nPcs,norm.nPcs))\n  \n  if(!is.null(pc.multipliers)) { # apply multipliers (used for flipping the direction of PCs in the plots)\n    if(length(pc.multipliers)!=nPcs) stop("pc.multipliers must be a vector equal in length to the number of PCs")\n    cat("pc multipliers ... ")\n    epc@loadings <- t(t(epc@loadings)*pc.multipliers)\n    epc@scores <- scale(epc@completeObs,scale=F,center=T) %*% epc@loadings;\n  }\n  \n  x1.scores <- t(x1.log - cent) %*% epc@loadings\n  \n  # normalize velocities ...?\n  cat("delta norm ... ")\n  delta.pcs <- as.matrix(x1.scores-epc@scores)\n  if(!is.na(norm.nPcs)) {\n    delta.pcs <- delta.pcs/mean(sqrt(rowSums(delta.pcs^2))) # suggested by Gioele, unsure about this ....\n  }\n  \n  # browser()\n  # z <- as.matrix(t(x1.log-x0.log)) %*% epc@loadings\n  # \n  # hist(apply(vel$deltaE,2,mean))\n  # summary(apply(vel$deltaE,2,mean))\n  # hist(apply(as.matrix(x1.log-x0.log),2,mean))\n  # summary(apply(as.matrix(x1.log-x0.log),2,mean))\n  # \n  # z <- t(as.matrix(vel$deltaE)[rownames(epc@loadings),]) %*%  epc@loadings\n  # str(z)\n  # str(delta.pcs)\n  # cn <- \'L6\'\n  # cn <- \'O15\'\n  # z <- (x1.log-x0.log)[,cn] * epc@loadings[,3]\n  # z2 <- vel$deltaE[names(z),cn] * epc@loadings[,3]\n  # sort(z,d=T)[1:20]\n  # sum(z)\n  # delta.pcs[cn,3]\n  # summary(delta.pcs[,3])\n  # str(epc@loadings[,3])\n  # sort(delta.pcs[,3],d=T)[1:10]\n  # z <- rowMeans(x1.log-x0.log) * epc@loadings[,3]\n  #summary(z)\n  #sort(z,d=T)[1:10]\n\n  delta.pcs <- delta.pcs *arrow.scale;\n  cat("done\\n")\n  if(do.par) par(mfrow=c(ceiling((nPcs-1)/plot.cols),plot.cols), mar = c(3.5,3.5,2.5,1.5), mgp = c(2,0.65,0), cex = 0.85);\n  vinfo <- lapply(1:(nPcs-1),function(i) {\n    pos <- epc@scores[,c((i-1)+1,(i-1)+2)];\n    #ppos <- x1.scores[,c((i-1)+1,(i-1)+2)];\n    ppos <- pos+delta.pcs[,c((i-1)+1,(i-1)+2)];\n    plot(pos,bg=cell.colors[rownames(pos)],pch=21,col=ac(1,alpha=0.3),lwd=0.5,xlab=paste("PC",(i-1)+1),ylab=paste("PC",(i-1)+2),axes=T,main=paste(\'PC\',(i-1)+1,\' vs. PC\',(i-1)+2,sep=\'\'),  ...); box();\n    \n    if(show.grid.flow) { # show grid summary of the arrows\n      # arrow estimates for each cell\n      ars <- data.frame(pos[,1],pos[,2],ppos[,1],ppos[,2])\n      colnames(ars) <- c(\'x0\',\'y0\',\'x1\',\'y1\')\n      arsd <- data.frame(xd=ars$x1-ars$x0,yd=ars$y1-ars$y0)\n      rownames(ars) <- rownames(arsd) <- rownames(pos);\n      \n      # set up a grid\n      rx <- range(c(range(ars$x0),range(ars$x1)))\n      ry <- range(c(range(ars$y0),range(ars$y1)))\n      gx <- seq(rx[1],rx[2],length.out=grid.n)\n      gy <- seq(ry[1],ry[2],length.out=grid.n)\n      \n      # for each grid point calculate Gaussian-weighted delta average\n      if(is.null(grid.sd)) {\n        grid.sd <- sqrt((gx[2]-gx[1])^2 + (gy[2]-gy[1])^2)/2\n        cat("grid.sd=",grid.sd," ")\n      }\n\n      if(is.null(min.arrow.size)) {\n        min.arrow.size <- sqrt((gx[2]-gx[1])^2 + (gy[2]-gy[1])^2)*1e-2;\n        cat("min.arrow.size=",min.arrow.size," ")\n      }\n\n      if(is.null(max.grid.arrow.length)) {\n        max.grid.arrow.length <- sqrt(sum((par(\'pin\')/c(length(gx),length(gy)))^2))*0.25\n        cat("max.grid.arrow.length=",max.grid.arrow.length," ")\n      }\n\n\n      garrows <- do.call(rbind,lapply(gx,function(x) {\n        # cell distances (rows:cells, columns: grid points)\n        cd <- sqrt(outer(pos[,2],-gy,\'+\')^2 + (x-pos[,1])^2)\n        cw <- dnorm(cd,sd=grid.sd)\n        # calculate x and y delta expectations\n        gw <- Matrix::colSums(cw)\n        cws <- pmax(1,Matrix::colSums(cw));\n        gxd <- Matrix::colSums(cw*arsd$xd)/cws\n        gyd <- Matrix::colSums(cw*arsd$yd)/cws\n        \n        al <- sqrt(gxd^2+gyd^2);\n        vg <- gw>=min.grid.cell.mass & al>=min.arrow.size\n        \n        cbind(rep(x,sum(vg)),gy[vg],x+gxd[vg],gy[vg]+gyd[vg])\n      }))\n      colnames(garrows) <- c(\'x0\',\'y0\',\'x1\',\'y1\')\n      \n      # plot\n      if(fixed.arrow.length) {\n        suppressWarnings(arrows(garrows[,1],garrows[,2],garrows[,3],garrows[,4],length=0.05,lwd=arrow.lwd))\n      } else {\n        alen <- pmin(max.grid.arrow.length,sqrt( ((garrows[,3]-garrows[,1]) * par(\'pin\')[1] / diff(par(\'usr\')[c(1,2)]) )^2 + ((garrows[,4]-garrows[,2])*par(\'pin\')[2] / diff(par(\'usr\')[c(3,4)]) )^2))\n        # can\'t specify different arrow lengths in one shot :(\n        suppressWarnings(lapply(1:nrow(garrows),function(i) arrows(garrows[i,1],garrows[i,2],garrows[i,3],garrows[i,4],length=alen[i],lwd=arrow.lwd)))\n      }\n      if(plot.grid.points) points(rep(gx,each=length(gy)),rep(gy,length(gx)),pch=\'.\',cex=1e-1,col=ac(1,alpha=0.4))\n    \n      if(return.details) { # for the p1 app\n        # calculate expression shift\n        cat("expression shifts .")\n        # for individual cells\n        es <- as.matrix(epc@loadings[,c((i-1)+1,(i-1)+2)] %*% t(delta.pcs[,c((i-1)+1,(i-1)+2)]))\n        \n        cat(".");\n        gs <- epc@loadings[,c((i-1)+1,(i-1)+2)] %*% rbind(garrows[,3]-garrows[,1],garrows[,4]-garrows[,2])\n\n        # note: here we\'re using deltaE vector, which may be normalized a bit differently from the $current/$projectted that was used above\n        nd <- as.matrix(vel$deltaE)\n        if(scale==\'log\') {\n          nd <- (log10(abs(nd)+1)*sign(nd))\n        } else if(scale==\'sqrt\') {\n          nd <- (sqrt(abs(nd))*sign(nd))\n        }\n        cat(".");\n        # velocity for the grid (weight-averaged velocity vectors)\n        \n        \n        gv <- do.call(cbind,parallel::mclapply(gx,function(x) {\n          # cell distances (rows:cells, columns: grid points)\n          cd <- sqrt(outer(pos[,2],-gy,\'+\')^2 + (x-pos[,1])^2)\n          cw <- dnorm(cd,sd=grid.sd)\n          # calculate x and y delta expectations\n          gw <- Matrix::colSums(cw)\n          cws <- pmax(1,Matrix::colSums(cw));\n          cw <- t(t(cw)/cws)\n          gxd <- Matrix::colSums(cw*arsd$xd)\n          gyd <- Matrix::colSums(cw*arsd$yd)\n          al <- sqrt(gxd^2+gyd^2);\n          vg <- gw>=min.grid.cell.mass & al>=min.arrow.size\n          if(any(vg)) {\n            z <- nd %*% cw[,vg]\n          } else { NULL }\n        },mc.cores=n.cores,mc.preschedule=T))\n        cat(". done\\n")\n        \n        return(invisible(list(garrows=garrows,arrows=as.matrix(ars),vel=nd,eshifts=es,gvel=gv,geshifts=gs,scale=scale,emb=pos,epc=epc)))\n      }\n      \n    } else {\n      # draw individual arrows\n      grid();\n      suppressWarnings(arrows(pos[,1],pos[,2],ppos[,1],ppos[,2],length=0.05,lwd=arrow.lwd))\n    }\n  })\n  cat("done\\n")\n  if(return.details) { return(vinfo) }\n  return(invisible(list(epc=epc,delta.pcs=delta.pcs)))\n  \n}\n##\' Joint t-SNE visualization of the velocities by joint t-SNE embedding of both current and extraploated cell positions\n##\'\n##\' @param vel velocity result\n##\' @param cell.colors named color vector for the cells\n##\' @param scale whether to rescale current/projected\n##\' @param do.par whether to reset par (default=T)\n##\' @param delta.norm whether to renormalize velocities following PCA projection\n##\' @param nPcs number of PCs onto which project the velocities\n##\' @param norm.nPcs number of PCs to use for velocity normalization\n##\' @param perplexity perplexity parameter to use in joint t-SNE calculation\n##\' @param show.grid.flow whether grid flow pattern should be drawn\n##\' @param grid.n number of grid points along each axis\n##\' @param grid.sd standard deviation of each grid point (used to determine the averaging radius for each grid point)\n##\' @param min.grid.cell.mass minimal number of cells around a grid point required for the grid point to show up\n##\' @param pcount pseudocount\n##\' @param verbose whether to show messages\n##\' @param min.arrow.median.ratio minimal ratio of arrow length (to the median arrow length) below which the arrows are not drawn (default=1/10)\n##\' @param max.arrow.quantile max arrow quantile that\'s used for arrow size calculation (default=0.9)\n##\' @param arrow.scale scaling factor for the arrows\n##\' @param arrow.lwd arrow line width\n##\' @param xlab x axis label\n##\' @param ylab y axis label\n##\' @param n.cores number of cores to use\n##\' @param size.norm whether to re-normalize current and projected cell sizes\n##\' @param ... extra parameters are passed to plot() routine.\n##\' @return invisible list containing embedding positions of current state (current.emb) and extrapolated states (projected.emb)\n##\' @export\ntSNE.velocity.plot <- function(vel,cell.colors=NULL,scale=\'log\',do.par=T, delta.norm=TRUE, nPcs=15, norm.nPcs=nPcs*10, perplexity=ncol(vel$current)/3, show.grid.flow=FALSE, grid.n=20, grid.sd=NULL, min.grid.cell.mass=1, pcount=0.1, verbose=TRUE, min.arrow.median.ratio=1/10, max.arrow.quantile=0.9, arrow.scale=1, arrow.lwd=1, xlab="", ylab="", n.cores=defaultNCores(), size.norm=TRUE, ...) {\n  x0 <- vel$current;\n  x1 <- vel$projected;\n  if(is.null(cell.colors)) { cell.colors <- ac(rep(1,ncol(x0)),alpha=0.3); names(cell.colors) <- colnames(x0) }\n  \n  if(size.norm) {\n    # rescale to the same size\n    cat("rescaling ... ")\n    sz <- Matrix::colSums(x0);\n    x0 <- t(t(x0)/sz)*mean(sz)\n    x1 <- t(t(x1)/Matrix::colSums(x1))*mean(sz);\n  }\n  # transform\n  if(scale==\'log\') { \n    cat("log ... ")\n    x0.log <- log2(x0+pcount)\n    x1.log <- log2(x1+pcount)\n  } else if(scale==\'sqrt\') {\n    cat("sqrt ... ")\n    x0.log <- sqrt(x0)\n    x1.log <- sqrt(x1)\n  } else { # linear\n    cat("linear ... ")\n    x0.log <- x0\n    x1.log <- x1\n  }\n  if(!is.null(nPcs)) { # reduce using PCA first\n    cat("pca ... ")\n    cent <- rowMeans(x0.log);\n    epc <- pcaMethods::pca(t(x0.log-cent),center=F,nPcs=ifelse(is.na(norm.nPcs),nPcs,norm.nPcs))\n    x0.log <- epc@scores;\n    x1.log <- t(x1.log - cent) %*% epc@loadings\n    if(delta.norm) {\n      # normalize velocities ...?\n      cat("delta norm ... ")\n      delta.pcs <- x1.log-x0.log;\n      if(!is.na(norm.nPcs)){ \n        delta.pcs <- delta.pcs/mean(sqrt(rowSums(delta.pcs^2))) # ? unsure about this (cell-wise L2 norm)\n      }\n      x1.log <- x0.log+delta.pcs;\n    }\n    # drop extra Pcs \n    x0.log <- t(x0.log[,1:nPcs])\n    x1.log <- t(x1.log[,1:nPcs])\n  }\n  cat("tSNE ...")\n  emb <- Rtsne::Rtsne(t(cbind(as.matrix(x0.log),as.matrix(x1.log))), num_threads=n.cores, perplexity=perplexity, verbose=verbose)$Y;\n  x0.emb <- emb[1:ncol(x0.log),]\n  x1.emb <- emb[-(1:ncol(x0.log)),]\n  rownames(x0.emb) <- rownames(x1.emb) <- colnames(x0.log);\n  \n  cat("delta norm ... ") # again, somewhat unsure about this part\n  delta.emb <- x1.emb - x0.emb;\n  asize <- rowSums(delta.emb^2);\n  no.arrow <- asize<= median(asize)*min.arrow.median.ratio;\n  # restrict size top top 90% quantile\n  delta.emb <- delta.emb/asize * pmin(asize,2*quantile(asize,p=max.arrow.quantile))*arrow.scale;\n  delta.emb[no.arrow,] <- 0;\n  cat("done\\n") \n  if(do.par) par(mfrow=c(1,1), mar = c(3.5,3.5,2.5,1.5), mgp = c(2,0.65,0), cex = 0.85);\n  plot(x0.emb,bg=cell.colors[rownames(x0.emb)],pch=21,col=ac(1,alpha=0.3),xlab=ylab,ylab=xlab, ... ); box();\n  \n  \n  if(show.grid.flow) { # show grid summary of the arrows\n    # arrow estimates for each cell\n    ars <- data.frame(x0.emb[,1],x0.emb[,2],x0.emb[,1]+delta.emb[,1],x0.emb[,2]+delta.emb[,2])\n    colnames(ars) <- c(\'x0\',\'y0\',\'x1\',\'y1\')\n    arsd <- data.frame(xd=ars$x1-ars$x0,yd=ars$y1-ars$y0)\n    \n    # set up a grid\n    cat("grid estimates ... ")\n    rx <- range(c(range(ars$x0),range(ars$x1)))\n    ry <- range(c(range(ars$y0),range(ars$y1)))\n    gx <- seq(rx[1],rx[2],length.out=grid.n)\n    gy <- seq(ry[1],ry[2],length.out=grid.n)\n    \n    # for each grid point calculate Gaussian-weighted delta average\n    if(is.null(grid.sd)) {\n      grid.sd <- sqrt((gx[2]-gx[1])^2 + (gy[2]-gy[1])^2)/2\n    }\n    ginfo <- lapply(gx,function(x) {\n      # cell distances (rows-cells,columsn - grid points)\n      cd <- sqrt(outer(x0.emb[,2],-gy,\'+\')^2 + (x-x0.emb[,1])^2)\n      cw <- dnorm(cd,sd=grid.sd)\n      # calculate x and y delta expectations\n      gw <- Matrix::colSums(cw)\n      cws <- pmax(1,Matrix::colSums(cw));\n      gxd <- Matrix::colSums(cw*arsd$xd)/cws\n      gyd <- Matrix::colSums(cw*arsd$yd)/cws\n      \n      vg <- gw>=min.grid.cell.mass\n      if(any(vg)) {\n        suppressWarnings(arrows(rep(x,sum(vg)),gy[vg],x+gxd[vg],gy[vg]+gyd[vg],length=0.05,lwd=arrow.lwd))\n      }\n      points(rep(x,length(gy)),gy,pch=\'.\')\n    })\n    cat("done\\n")\n  } else {\n    # draw individual arrows\n    suppressWarnings(arrows(x0.emb[,1],x0.emb[,2],x0.emb[,1]+delta.emb[,1],x0.emb[,2]+delta.emb[,2],length=0.05,lwd=arrow.lwd))\n  }\n\n  return(invisible(list(current.emb=x0.emb,projected.emb=x1.emb+delta.emb)))\n}\n\n##\' Visualize RNA velocities on an existing embedding using correlation-based transition probability matrix within the kNN graph\n##\'\n##\' @param emb embedding onto which to project the velocities; The dimensions of coordinates should be on the order of 10x10 for the default values to make sense.\n##\' @param vel velocity estimates (e.g. returned by gene.relative.velocity.estimates() )\n##\' @param n neighborhood size (default=100 cells)\n##\' @param cell.colors name vector of cell colors\n##\' @param corr.sigma sigma parameter used to translate velocity-(expression delta) correlation into a transition probability\n##\' @param show.grid.flow whether to show grid velocity summary\n##\' @param grid.n number of grid points along each axis\n##\' @param grid.sd standard deviation (in embedding coordinate space) used to determine the weighting of individual cells around each grid point\n##\' @param min.grid.cell.mass minimal cell "mass" (weighted number of cells) around each grid point required for it to show up\n##\' @param min.arrow.size minimal arrow size\n##\' @param arrow.scale arrow scale multiplier\n##\' @param max.grid.arrow.length minimal arrow size\n##\' @param fixed.arrow.length whether to use fixed arrow width (default=FALSE)\n##\' @param plot.grid.points whether to mark all grid points with dots (even if they don\'t have valid velocities)\n##\' @param scale velocity scale to use (default: \'log\', other values: \'sqrt\',\'rank\',\'linear\')\n##\' @param nPcs number of PCs to use for velocity regularization (default NA, turns off regularization)\n##\' @param arrow.lwd arrow width (under fixed.arrow.length=T)\n##\' @param xlab x axis label\n##\' @param ylab y axls label\n##\' @param n.cores number of cores to use\n##\' @param do.par whether to reset plotting parameters\n##\' @param show.cell whether to show detailed velocity estimates for a specified cell\n##\' @param cell.border.alpha transparency for the cell border\n##\' @param cc velocity-(exprssion delta) correlation matrix (can be passed back from previous results, as $cc) to save calculation time when replotting the same velocity estimates on the same embedding with different parameters\n##\' @param return.details whether to return detailed output (which can be passed to p1 app for visualization)\n##\' @param expression.scaling whether to scale the velocity length by the projection of velocity onto the expected expression change (based on the transition probability matrix)\n##\' @param ... extra parameters passed to plot() function\n##\' @return if return.details=F, returns invisible list containing transition probability matrix ($tp) and the velocity-(expression delta) correlation matrix ($cc). If return.details=T, returns a more extended list that can be passed as veloinfo to pagoda2::p2.make.pagoda1.app() for visualization\n##\' @export\nshow.velocity.on.embedding.cor <- function(emb,vel,n=100,cell.colors=NULL, corr.sigma=0.05, show.grid.flow=FALSE, grid.n=20, grid.sd=NULL, min.grid.cell.mass=1, min.arrow.size=NULL, arrow.scale=1, max.grid.arrow.length=NULL, fixed.arrow.length=FALSE, plot.grid.points=FALSE, scale=\'log\', nPcs=NA,  arrow.lwd=1, xlab="", ylab="", n.cores=defaultNCores(), do.par=T, show.cell=NULL, cell.border.alpha=0.3,cc=NULL, return.details=FALSE, expression.scaling=FALSE,  ...) {\n  randomize <- FALSE;\n  if(do.par) par(mfrow=c(1,1), mar = c(3.5,3.5,2.5,1.5), mgp = c(2,0.65,0), cex = 0.85);\n  celcol <- \'white\'\n  if(is.null(show.cell)) { celcol <- cell.colors[rownames(emb)] }\n  plot(emb,bg=celcol,pch=21,col=ac(1,alpha=cell.border.alpha), xlab=xlab, ylab=ylab, ...);\n  \n  #plot(emb,bg=cell.colors[rownames(emb)],pch=21,col=ac(1,alpha=0.3), xlab=xlab, ylab=ylab);\n  em <- as.matrix(vel$current); \n  ccells <- intersect(rownames(emb),colnames(em));\n  em <- em[,ccells]; emb <- emb[ccells,]\n  nd <- as.matrix(vel$deltaE[,ccells])\n  cgenes <- intersect(rownames(em),rownames(nd));\n  nd <- nd[cgenes,]; em <- em[cgenes,]\n  #browser()\n  if(randomize) {\n    # randomize cell and sign for each gene\n    nd <- t(apply(nd,1,function(x) (rbinom(length(x),1,0.5)*2-1)*abs(sample(x))))\n  }\n  #vg <- rownames(em) %in% rownames(r)\n\n  \n  if(is.null(cc)) {\n    # cosine projections\n    cat("delta projections ... ")\n\n    if(scale==\'log\') {\n      cat("log ")\n      cc <- colDeltaCorLog10(em,(log10(abs(nd)+1)*sign(nd)),nthreads=n.cores);\n    } else if(scale==\'sqrt\') {\n      cat("sqrt ")\n      cc <- colDeltaCorSqrt(em,(sqrt(abs(nd))*sign(nd)),nthreads=n.cores);\n    } else if(scale==\'rank\') {\n      cat("rank ")\n      cc <- colDeltaCor((apply(em,2,rank)),(apply(nd,2,rank)),nthreads=n.cores);\n    } else { # linear\n      cat("linear ")\n      cc <- colDeltaCor(em,nd,nthreads=n.cores);\n    }\n    colnames(cc) <- rownames(cc) <- colnames(em)\n    diag(cc) <- 0;\n  }\n\n  cat("knn ... ")\n  if(n>nrow(cc)) { n <- nrow(cc) }\n  # TODO: add kNN based on high-dimensional correlation or Euclidean distances\n  # define kNNs based on the embedding (L2 distance)\n  emb.knn <- balancedKNN(t(emb),k=n,maxl=nrow(emb),dist=\'euclidean\',n.threads=n.cores)\n  diag(emb.knn) <- 1\n  # caluclate transition probabilities (from col to row)\n  cat("transition probs ... ")\n  tp <- exp(cc/corr.sigma)*emb.knn\n  #diag(tp) <- 0; #  should we allow the self-corelation for scaling?\n  tp <- t(t(tp)/Matrix::colSums(tp)); # tp shows transition from a given column cell to different row cells\n  tp <- as(tp,\'dgCMatrix\')\n  cat("done\\n")\n  if(!is.null(show.cell)) {\n    i <- match(show.cell,rownames(emb));\n    if(is.na(i)) stop(paste(\'specified cell\',i,\'is not in the embedding\'))\n    # plot transition prob for a given cell\n    points(emb,pch=19,col=ac(val2col(tp[rownames(emb),show.cell],gradient.range.quantile=1),alpha=0.5))\n    points(emb[show.cell,1],emb[show.cell,2],pch=3,cex=1,col=1)\n    di <- t(t(emb)-emb[i,])\n    di <- di/sqrt(Matrix::rowSums(di^2))*arrow.scale; di[i,] <- 0;\n    dir <- Matrix::colSums(di*tp[,i]) \n    dic <- Matrix::colSums(di*(tp[,i]>0)/sum(tp[,i]>0)); # relative to expected kNN center\n    dia <- dir-dic;\n    #browser()\n    suppressWarnings(arrows(emb[colnames(em)[i],1],emb[colnames(em)[i],2],emb[colnames(em)[i],1]+dic[1],emb[colnames(em)[i],2]+dic[2],length=0.05,lwd=1,col=\'blue\'))\n    suppressWarnings(arrows(emb[colnames(em)[i],1],emb[colnames(em)[i],2],emb[colnames(em)[i],1]+dir[1],emb[colnames(em)[i],2]+dir[2],length=0.05,lwd=1,col=\'red\'))\n    suppressWarnings(arrows(emb[colnames(em)[i],1]+dic[1],emb[colnames(em)[i],2]+dic[2],emb[colnames(em)[i],1]+dir[1],emb[colnames(em)[i],2]+dir[2],length=0.05,lwd=1,lty=1,col=\'grey50\'))\n    suppressWarnings(arrows(emb[colnames(em)[i],1],emb[colnames(em)[i],2],emb[colnames(em)[i],1]+dia[1],emb[colnames(em)[i],2]+dia[2],length=0.05,lwd=1,col=\'black\'))\n  } else {\n    # arrow estimates for each cell\n    cat("calculating arrows ... ")\n    arsd <- data.frame(t(embArrows(emb,tp,arrow.scale,n.cores)))\n    rownames(arsd) <- rownames(emb);\n    \n    if(expression.scaling) {\n      tpb <- tp>0; tpb <- t(t(tpb)/colSums(tpb));\n      es <- as.matrix(em %*% tp) -as.matrix(em %*% as.matrix(tpb));\n      # project velocity onto expression shift\n      #pm <- as.matrix(t(vel$deltaE)/sqrt(colSums(vel$deltaE*vel$deltaE)))[colnames(es),] * (t(es)/sqrt(colSums(es*es)))\n      #pl <- pmax(0,apply(pm,1,sum))\n      pl <- pmin(1,pmax(0,apply(as.matrix(vel$deltaE[,colnames(es)]) * es, 2, sum)/sqrt(colSums(es*es))))\n      \n      \n      arsd <- arsd * pl;\n    }\n    \n    \n    ars <- data.frame(cbind(emb,emb+arsd));\n    colnames(ars) <- c(\'x0\',\'y0\',\'x1\',\'y1\')\n    colnames(arsd) <- c(\'xd\',\'yd\')\n    rownames(ars) <- rownames(emb);\n    cat("done\\n")\n    \n    \n    if(show.grid.flow) { # show grid summary of the arrows\n\n    # set up a grid\n    cat("grid estimates ... ")\n    rx <- range(c(range(ars$x0),range(ars$x1)))\n    ry <- range(c(range(ars$y0),range(ars$y1)))\n    gx <- seq(rx[1],rx[2],length.out=grid.n)\n    gy <- seq(ry[1],ry[2],length.out=grid.n)\n    \n    # for each grid point calculate Gaussian-weighted delta average\n    if(is.null(grid.sd)) {\n      grid.sd <- sqrt((gx[2]-gx[1])^2 + (gy[2]-gy[1])^2)/2\n      cat("grid.sd=",grid.sd," ")\n    }\n    if(is.null(min.arrow.size)) {\n      min.arrow.size <- sqrt((gx[2]-gx[1])^2 + (gy[2]-gy[1])^2)*1e-2;\n      cat("min.arrow.size=",min.arrow.size," ")\n    }\n    if(is.null(max.grid.arrow.length)) {\n      max.grid.arrow.length <- sqrt(sum((par(\'pin\')/c(length(gx),length(gy)))^2))*0.25\n      cat("max.grid.arrow.length=",max.grid.arrow.length," ")\n    }\n    \n    garrows <- do.call(rbind,lapply(gx,function(x) {\n      # cell distances (rows:cells, columns: grid points)\n      cd <- sqrt(outer(emb[,2],-gy,\'+\')^2 + (x-emb[,1])^2)\n      cw <- dnorm(cd,sd=grid.sd)\n      # calculate x and y delta expectations\n      gw <- Matrix::colSums(cw)\n      cws <- pmax(1,Matrix::colSums(cw));\n      gxd <- Matrix::colSums(cw*arsd$xd)/cws\n      gyd <- Matrix::colSums(cw*arsd$yd)/cws\n      \n      al <- sqrt(gxd^2+gyd^2);\n      vg <- gw>=min.grid.cell.mass & al>=min.arrow.size\n      \n      cbind(rep(x,sum(vg)),gy[vg],x+gxd[vg],gy[vg]+gyd[vg])\n    }))\n    colnames(garrows) <- c(\'x0\',\'y0\',\'x1\',\'y1\')\n\n    # plot\n    if(fixed.arrow.length) {\n      suppressWarnings(arrows(garrows[,1],garrows[,2],garrows[,3],garrows[,4],length=0.05,lwd=arrow.lwd))\n    } else {\n      alen <- pmin(max.grid.arrow.length,sqrt( ((garrows[,3]-garrows[,1]) * par(\'pin\')[1] / diff(par(\'usr\')[c(1,2)]) )^2 + ((garrows[,4]-garrows[,2])*par(\'pin\')[2] / diff(par(\'usr\')[c(3,4)]) )^2))\n      # can\'t specify different arrow lengths in one shot :(\n      #suppressWarnings(arrows(garrows[,1],garrows[,2],garrows[,3],garrows[,4],length=alen,lwd=arrow.lwd))\n      suppressWarnings(lapply(1:nrow(garrows),function(i) arrows(garrows[i,1],garrows[i,2],garrows[i,3],garrows[i,4],length=alen[i],lwd=arrow.lwd)))\n    }\n    if(plot.grid.points) points(rep(gx,each=length(gy)),rep(gy,length(gx)),pch=\'.\',cex=1e-1,col=ac(1,alpha=0.4))\n    \n    cat("done\\n")\n    \n    if(return.details) { # for the p1 app\n      # calculate expression shift\n      cat("expression shifts .")\n      # for individual cells\n      \n      scale.int <- switch(scale,\'log\'=2,\'sqrt\'=3,1)\n      #es <- expectedExpressionShift(e=as.matrix(em),tp=tp,scale=scale.int,nthreads=n.cores); colnames(es) <- colnames(em); rownames(es) <- rownames(em);\n      if(!expression.scaling) { #otherwise it has already been calculated\n        tpb <- tp>0; tpb <- t(t(tpb)/colSums(tpb));\n        #es <- expectedExpressionShift(e=as.matrix(em %*% as.matrix(tpb)),tp=tp,scale=scale.int,nthreads=n.cores); colnames(es) <- colnames(em); rownames(es) <- rownames(em);\n        es <- as.matrix(em %*% tp) -as.matrix(em %*% as.matrix(tpb));\n      }\n      cat(".");\n      # for the grid\n      gs <- do.call(cbind,parallel::mclapply(gx,function(x) {\n        # cell distances (rows:cells, columns: grid points)\n        cd <- sqrt(outer(emb[,2],-gy,\'+\')^2 + (x-emb[,1])^2)\n        cw <- dnorm(cd,sd=grid.sd)\n        # calculate x and y delta expectations\n        gw <- Matrix::colSums(cw)\n        cws <- pmax(1,Matrix::colSums(cw));\n        cw <- t(t(cw)/cws)\n        gxd <- Matrix::colSums(cw*arsd$xd)\n        gyd <- Matrix::colSums(cw*arsd$yd)\n        al <- sqrt(gxd^2+gyd^2);\n        vg <- gw>=min.grid.cell.mass & al>=min.arrow.size\n        if(any(vg)) {\n          z <- es %*% cw[,vg]\n        } else { NULL }\n      },mc.cores=n.cores,mc.preschedule=T))\n\n      if(scale==\'log\') {\n        nd <- (log10(abs(nd)+1)*sign(nd))\n      } else if(scale==\'sqrt\') {\n        nd <- (sqrt(abs(nd))*sign(nd))\n      }\n      cat(".");\n      # velocity for the grid\n      gv <- do.call(cbind,parallel::mclapply(gx,function(x) {\n        # cell distances (rows:cells, columns: grid points)\n        cd <- sqrt(outer(emb[,2],-gy,\'+\')^2 + (x-emb[,1])^2)\n        cw <- dnorm(cd,sd=grid.sd)\n        # calculate x and y delta expectations\n        gw <- Matrix::colSums(cw)\n        cws <- pmax(1,Matrix::colSums(cw));\n        cw <- t(t(cw)/cws)\n        gxd <- Matrix::colSums(cw*arsd$xd)\n        gyd <- Matrix::colSums(cw*arsd$yd)\n        al <- sqrt(gxd^2+gyd^2);\n        vg <- gw>=min.grid.cell.mass & al>=min.arrow.size\n        if(any(vg)) {\n          z <- nd %*% cw[,vg]\n        } else { NULL }\n      },mc.cores=n.cores,mc.preschedule=T))\n      cat(". done\\n")\n      \n      \n      return(invisible(list(tp=tp,cc=cc,garrows=garrows,arrows=as.matrix(ars),vel=nd,eshifts=es,gvel=gv,geshifts=gs,scale=scale)))\n    }\n    \n    \n    \n    \n    } else { # draw individual arrows\n      # calculate arrows, draw\n      # lapply(1:nrow(emb),function(i) {\n      #   # normalized directions to each point\n      #   di <- t(t(emb)-emb[i,])\n      #   di <- di/sqrt(Matrix::rowSums(di^2))*arrow.scale; di[i,] <- 0;\n      #   di <- Matrix::colSums(di*tp[,i]) - Matrix::colSums(di*(tp[,i]>0)/sum(tp[,i]>0)); # relative to expected kNN center\n      #   \n      #   if(fixed.arrow.length) {\n      #     suppressWarnings(arrows(emb[colnames(em)[i],1],emb[colnames(em)[i],2],emb[colnames(em)[i],1]+di[1],emb[colnames(em)[i],2]+di[2],length=0.05,lwd=arrow.lwd))\n      #   } else {\n      #     ali <- sqrt( (di[1] * par(\'pin\')[1] / diff(par(\'usr\')[c(1,2)]) )^2 + (di[2]*par(\'pin\')[2] / diff(par(\'usr\')[c(3,4)]) )^2)\n      #     suppressWarnings(arrows(emb[colnames(em)[i],1],emb[colnames(em)[i],2],emb[colnames(em)[i],1]+di[1],emb[colnames(em)[i],2]+di[2],length=min(0.05,ali),lwd=arrow.lwd))\n      #   }\n      # })\n      \n      apply(ars,1,function(x) {\n        if(fixed.arrow.length) {\n          suppressWarnings(arrows(x[1],x[2],x[3],x[4],length=0.05,lwd=arrow.lwd))\n        } else {\n          ali <- sqrt( ((x[3]-x[1]) * par(\'pin\')[1] / diff(par(\'usr\')[c(1,2)]) )^2 + ((x[4]-x[2])*par(\'pin\')[2] / diff(par(\'usr\')[c(3,4)]) )^2)\n          suppressWarnings(arrows(x[1],x[2],x[3],x[4],length=min(0.05,ali),lwd=arrow.lwd))\n        }\n      })\n      \n      \n    }\n  }\n  return(invisible(list(tp=tp,cc=cc)))\n}\n\n\n\n##\' Visualize RNA velocities on an existing embedding using Euclidean-based transition probability matrix within the kNN graph.\n##\'\n##\'  based on Euclidean distance of the extrapolated cell to others\n##\' The direction of the arrow is towards n closest neighbors. The magnitude of the arrow is determined by the cosine projection of the velocity on to the chosen direction\n##\' n=1 will only show arrows for cells that end up being projected closer to some other cell than to the original position\n##\' n=k (k>1) will show an average direction\n##\' Given an expression distance between cells d, and ratio of extrapolated to current expression distances between cells f, the transition probability is calculated as exp(- (d*(f^beta))^2/(2*sigma^2) )\n##\' \n##\' @param emb embedding to be used for projection\n##\' @param vel velocity result\n##\' @param n neighborhood size (default=30)\n##\' @param embedding.knn pre-calculated kNN\n##\' @param cell.colors named color vector for cell plotting\n##\' @param sigma sigma to use in calculating transition probability from the eucledian distance (estimated automatically by default)\n##\' @param beta beta parameter used in calculation of transition probability (by default=1)\n##\' @param arrow.scale additional scaling factor for the arrows (default=1)\n##\' @param scale scale to use in calculating distances (default: \'log\', also supported \'sqrt\'\n##\' @param nPcs number of PCs to project the cells onto (to perform distance calculations in lower dimensions), default=NA which turns off PCA dimensional reduction\n##\' @param arrow.lwd arrow line width\n##\' @param xlab x axis label\n##\' @param ylab y axis label\n##\' @param control.for.neighborhood.density compensate for cell density variations in the embedding (default: TRUE)\n##\' @param ntop.trajectories number of top trajectories to trace back for a given cell (when show.trajectories=TRUE)\n##\' @param do.par whether to reset plotting parameters (default=TRUE)\n##\' @param show.cell.arrows show detailed velocity projection for the specified cell\n##\' @param show.cell.trajectories show trajectories for a specified cell\n##\' @param show.trajectories show top median diffusion trajectories\n##\' @param show.all.trajectories show all diffusion paths (messy)\n##\' @param show.cell.diffusion.posterior show diffusion posterior of a given cell\n##\' @param show.grid.flow show velocity projections on a grid\n##\' @param diffusion.steps number of diffusion steps to take forward (default=10)\n##\' @param cell.dist - optional custom distance (must include all of the cells that are intersecting between emb and vel)\n##\' @param trajectory.spline.shape shape parameter for smoothing median cell trajectories (default=1)\n##\' @param cell.color.alpha trasparency parameter to apply when showing cell colors\n##\' @param n.cores number of cores to use in calculations\n##\' @param n.trajectory.clusters number of trajectory clusters to show median paths for (when show.trajectories=TRUE)\n##\' @param ... extra parameters are passed to the plot() function\n##\' @return transition probability matrix\n##\' @export\nshow.velocity.on.embedding.eu <- function(emb,vel,n=30,embedding.knn=TRUE,cell.colors=NULL, sigma=NA, beta=1, arrow.scale=1, scale=\'log\', nPcs=NA, arrow.lwd=1, xlab="", ylab="", control.for.neighborhood.density=TRUE, ntop.trajectories=1, do.par=T, show.cell.arrows=NULL, show.cell.trajectories=NULL, show.trajectories=FALSE, show.all.trajectories=FALSE, show.cell.diffusion.posterior=NULL, show.grid.flow=FALSE, diffusion.steps=10, cell.dist=NULL, trajectory.spline.shape=1, cell.color.alpha=0.5, n.cores=defaultNCores(), n.trajectory.clusters=10, ...) {\n  em <- vel$current; emn <- vel$projected;\n  if(is.null(cell.colors)) { cell.colors <- ac(rep(1,ncol(em)),alpha=0.3); names(cell.colors) <- colnames(em) }\n  if(do.par) par(mar = c(3.5,3.5,2.5,1.5), mgp = c(2,0.65,0), cex = 0.85);\n  cc <- \'white\'\n  if(is.null(show.cell.arrows) && is.null(show.cell.diffusion.posterior)) { cc <- cell.colors[rownames(emb)] }\n  plot(emb,bg=cc,pch=21,col=ac(1,alpha=cell.color.alpha), xlab=xlab, ylab=ylab, ...);\n  \n  ccells <- intersect(rownames(emb),colnames(em));\n  emn <- emn[,ccells]; em <- em[,ccells]; emb <- emb[ccells,]\n  \n  if(scale==\'log\') {\n    cat("log scale ... ")\n    em <- log10(em+1); emn <- log10(emn+1); \n  } else if(scale==\'sqrt\') {\n    cat("sqrt scale ... ")\n    em <- sqrt(em); emn <- sqrt(emn);\n  }\n  \n  if(!is.na(nPcs)) { # run PCA reduction on the em\n    cat("reducing to",nPcs,"PCs ... ")\n    epc.center <- rowMeans(em);\n    epc <- pcaMethods::pca(t(em-epc.center),center=F,nPcs=nPcs);\n    em <- t(epc@scores)\n    emn <- t(t(emn - epc.center) %*% epc@loadings)\n  }\n  \n  cat("distance ... ")\n  cc <- colEuclid(as.matrix(em),as.matrix(emn))\n  cc0 <- colEuclid(as.matrix(em),as.matrix(em))\n  cd <- (cc0-cc); # reduction in the Euclidean distance\n\n  if(n>nrow(cc)) { n <- nrow(cc) }\n  \n  # pick reasonable sigma and beta if they weren\'t provided\n  if(is.na(sigma) | is.na(beta)) { mcd <- mean(abs(cd)/cc) }\n  # TODO: adaptive methods for signal \n  if(is.na(sigma)) { sigma <- mcd/10 }\n  if(is.na(beta)) { beta <- mcd/20 }\n  cat("sigma=",round(sigma,3)," beta=",round(beta,3)," transition probs ... ")\n\n  # exp(- (d*(f^beta))^2/(2*sigma^2) )\n  f <- (cc/cc0)^beta; diag(f) <- 1;\n  tp <- exp(- ((cc0*f)^2) / (2*sigma^2))\n  np <- exp(- ((cc0)^2) / (2*sigma^2))\n\n  \n  \n\n  if(n<nrow(emb)) {\n    if(!is.null(cell.dist)) {\n      cat("kNN on provided distance ... ")\n      if(!all(labels(cell.dist)==colnames(em))) {\n        cat("matching cells between cell.dist and emat/nmat ... ")\n        cell.dist <- as.matrix(cell.dist)\n        cn <- colnames(em)\n        cell.dist <- as.dist(cell.dist[cn,cn]);\n      }\n      cell.knn <- balancedKNN(t(emb),k=n,maxl=nrow(emb),n.threads=n.cores,dist=cell.dist)\n      diag(cell.knn) <- 1;\n    } else {\n      if(embedding.knn) {\n        cat("embedding kNN ... ")\n        # define kNNs based on the embedding (L2 distance)\n        cell.knn <- balancedKNN(t(emb),k=n,maxl=nrow(emb),dist=\'euclidean\',n.threads=n.cores)\n        #diag(cell.knn) <- 0; # disallow self-transitions?\n        diag(cell.knn) <- 1;\n      } else {\n        cat("expression kNN ... ")\n        # define kNN based on the correlation distance in high-d\n        cell.knn <- balancedKNN(em,k=n,maxl=ncol(em),dist=\'cor\',n.threads=n.cores)\n        diag(cell.knn) <- 1;\n      }\n    }\n    tp <- tp*cell.knn;\n    np <- np*cell.knn;\n  }\n  \n  # estimate density of the neighborhood\n  tp <- t(t(tp)/Matrix::colSums(tp))\n  np <- t(t(np)/Matrix::colSums(np))\n  #diag(tp) <- diag(np) <- 0;\n  #tp.nd <- colSums(tp); np.nd <- colSums(np);\n  #tp <- tp * np.nd;\n\n  if(control.for.neighborhood.density) { \n    np.f <- Matrix::diag(np);\n    tp <- tp*(np.f)\n    np <- np*(np.f)\n  }\n\n  #diag(tp) <- diag(np) <- 0;  \n  # normalize\n  tp <- t(t(tp)/Matrix::colSums(tp))\n  np <- t(t(np)/Matrix::colSums(np))\n\n  \n\n  ## if(diffusion.step.size>1) {\n  ##   # bring transition probability to the specified power\n  ##   require(expm)\n  ##   tp <- t(as.matrix(t(tp)) %^% diffusion.step.size);\n  ##   np <- t(as.matrix(t(np)) %^% diffusion.step.size);\n  ##   tp <- t(t(tp)/Matrix::colSums(tp))\n  ##   np <- t(t(np)/Matrix::colSums(np))\n  ## }\n  \n  # normalize transition probabilities\n  rownames(tp) <- colnames(tp) <- rownames(np) <- colnames(np) <- colnames(em);\n  cat("done\\n")\n  \n  \n  if(!is.null(show.cell.diffusion.posterior)) {\n    i <- match(show.cell.diffusion.posterior,rownames(emb));\n    if(is.na(i)) stop(paste(\'specified cell\',i,\'is not in the embedding\'))\n    # run diffusion\n    cp <- Matrix::Diagonal(ncol(tp)); # cell position probabilities\n    rownames(cp) <- colnames(cp) <- rownames(tp);\n    ttp <- t(tp);\n    \n    # run diffusion steps to figure out end positions\n    cat("simulating diffusion ... ")\n    for(i in 1:diffusion.steps) {\n      cp <- cp %*% ttp;\n      #cp[cp<1e-5] <- 0;\n    }\n    cat("done\\n");\n    # plot\n    points(emb,pch=19,col=ac(val2col(cp[show.cell.diffusion.posterior,rownames(emb)],gradient.range.quantile=1),alpha=0.5))\n    points(emb[show.cell.diffusion.posterior,1],emb[show.cell.diffusion.posterior,2],pch=3,cex=1,col=1)\n  } else if(!is.null(show.cell.arrows)) {\n    i <- match(show.cell.arrows,rownames(emb));\n    if(is.na(i)) stop(paste(\'specified cell\',i,\'is not in the embedding\'))\n    # plot transition prob for a given cell\n    points(emb,pch=19,col=ac(val2col(tp[rownames(emb),show.cell.arrows],gradient.range.quantile=1),alpha=0.5))\n    points(emb[i,1],emb[i,2],pch=3,cex=1,col=1)\n    di <- t(t(emb)-emb[i,])\n    di <- di/sqrt(Matrix::rowSums(di^2))*arrow.scale; di[i,] <- 0;\n    dir <- Matrix::colSums(di*tp[,i]) \n    dic <- Matrix::colSums(di*np[,i]); # relative to the neighborhood\n    dia <- dir-dic;\n    suppressWarnings(arrows(emb[colnames(em)[i],1],emb[colnames(em)[i],2],emb[colnames(em)[i],1]+dic[1],emb[colnames(em)[i],2]+dic[2],length=0.05,lwd=1,col=\'blue\'))\n    suppressWarnings(arrows(emb[colnames(em)[i],1],emb[colnames(em)[i],2],emb[colnames(em)[i],1]+dir[1],emb[colnames(em)[i],2]+dir[2],length=0.05,lwd=1,col=\'red\'))\n    suppressWarnings(arrows(emb[colnames(em)[i],1]+dic[1],emb[colnames(em)[i],2]+dic[2],emb[colnames(em)[i],1]+dir[1],emb[colnames(em)[i],2]+dir[2],length=0.05,lwd=1,lty=1,col=\'grey50\'))\n    suppressWarnings(arrows(emb[colnames(em)[i],1],emb[colnames(em)[i],2],emb[colnames(em)[i],1]+dia[1],emb[colnames(em)[i],2]+dia[2],length=0.05,lwd=1,col=\'black\'))\n  } else if(show.trajectories) { # show diffusion paths\n    cp <- Matrix::Diagonal(ncol(tp)); # cell position probabilities\n    rownames(cp) <- colnames(cp) <- rownames(tp);\n  \n    #cpt <- as.array(cp);\n    cpl <- list(); cpl[[1]] <- cp;\n    #cp <- as.matrix(cp); tp <- as.matrix(tp)\n    \n    #ep <- as.array(emb)\n    ttp <- t(tp);\n\n    # run diffusion steps to figure out end positions\n    cat("simulating diffusion ... ")\n    for(i in 1:diffusion.steps) {\n      cp <- cp %*% ttp;\n      #cp[cp<1e-5] <- 0;\n      #cpt <- abind(cpt,cp,along=3)\n      cpl[[i+1]] <- cp;\n      # clean up to zero out all but top n cells\n      #cp <- t(apply(cp,1,function(x) { x[x<sort(x,decreasing=TRUE)[10]] <- 0; x }))\n      #diag(cp) <- 0; #  prohibit the cell from returning to itself\n      #cp <- cp/Matrix::rowSums(cp)\n    }\n\n\n    #cpt <- abind(lapply(cpl,as.matrix),along=3)\n    \n    # calculate probabilistic trajectories to the final ntop points\n    \n    # rank final points by probability\n    cpo <- t(apply(-cp,1,order))\n\n    # graph-based walkback approach\n    \n    # construct a walkback graph\n    trp <- as(ttp,\'dgTMatrix\')\n    cat("constructing path graph ... ")\n    x <- do.call(rbind,lapply(1:(diffusion.steps+1),function(i) {\n      cbind(i=trp@i+1 + (i-1)*nrow(cp), # current time step\n            j=trp@j+1 + (i)*nrow(cp))\n    }))\n    x <- x[x[,2]<=nrow(cp)*(diffusion.steps+1),]\n    x <- spMatrix(nrow(cp)*(diffusion.steps+1),nrow(cp)*(diffusion.steps+1),i=x[,1],j=x[,2],x=rep(1,nrow(x)))\n    g <- igraph::graph.adjacency(x,mode=\'directed\')\n    rm(x); gc();\n    \n    # find topn trajectories for each cell\n    cat("tracing shortest trajectories ... ")\n    sps <- parallel::mclapply(1:nrow(cp),function(celli) {\n      top.desti <- order(cp[celli,],decreasing=TRUE)[1:ntop.trajectories]\n      # calculate cell-specific weights\n      cw <- unlist(lapply(cpl,function(d) as.numeric(trp@x*(d[celli,trp@i+1]))))\n      cw <- cw[1:igraph::ecount(g)] # trim extra edges\n      # convert into penalty scores\n      cw <- -log(cw)\n      sp <- igraph::shortest_paths(g,from=celli,to=nrow(cp)*(diffusion.steps-1)+top.desti,weights=cw,mode=\'out\')\n      # remove time offset on the path nodes\n      sp <- lapply(sp$vpath,function(x) { y <- (as.integer(x) %% nrow(cp)); y[y==0] <- nrow(cp); y});\n      names(sp) <- rownames(cp)[top.desti]\n      sp\n    },mc.cores=n.cores)\n\n\n\n    # cluster paths\n    cat("clustering ... ")\n    all.cells <- 1:nrow(cp)\n    #spuci <- do.call(cbind,lapply(sps,function(x) all.cells %in% x[[1]]))\n    # filter out empty paths\n    sps <- lapply(sps,function(y) y[unlist(lapply(y,function(x) length(unique(x))))>1])\n    spuci <- do.call(cbind,lapply(sps,function(y) do.call(cbind,lapply(y,function(x) all.cells %in% x))))\n    usps <- unlist(sps,recursive=F); # will be used in looking up median trajectories in plotting\n\n    \n    spuci.dist <- as.matrix(dist(t(spuci),method = \'manhattan\'))\n    spuci.pam <- pam(spuci.dist,n.trajectory.clusters)\n    cat("done.\\n")\n    \n    # bezier\n    # determine common start/end points\n    #plot(emb,bg=\'white\',pch=21,col=ac(1,alpha=cell.color.alpha), xlab=xlab, ylab=ylab);\n    lapply(1:length(spuci.pam$id.med),function(cn) {\n      if(length(usps[[spuci.pam$id.med[cn]]])>0){\n        mp <- usps[[spuci.pam$id.med[cn]]]; mp <- mp[!duplicated(mp)]\n        bp <- data.frame(do.call(cbind,xspline(emb[mp,],shape=trajectory.spline.shape,draw=F)))\n        lines(bp$x,bp$y,col=ac(1,alpha=0.6))\n        bp <- bp[abs(diff(bp$x))+abs(diff(bp$y))>1e-5,]\n        ai <- round(length(bp$x)*c(0.2,0.8,0.5))\n        arrows(bp$x[ai],bp$y[ai],bp$x[ai+1],bp$y[ai+1],angle=30,length=0.1,col=ac(1,alpha=0.6))\n      }\n    })\n    return(invisible(list(spuci.dist=spuci.dist,sps=sps,tp=tp,cpl=cpl)))\n  } else if(!is.null(show.cell.trajectories)) {\n    # show optimal path(s) for a particular cell\n    celli <- match(show.cell.trajectories,rownames(emb));\n    if(is.na(celli)) stop(paste(\'specified cell\',show.cell.trajectories,\'is not in the embedding\'))\n\n    cp <- Matrix::Diagonal(ncol(tp)); # cell position probabilities\n    rownames(cp) <- colnames(cp) <- rownames(tp);\n    \n    #cpt <- as.array(cp);\n    cpl <- list(); cpl[[1]] <- cp;\n    #cp <- as.matrix(cp); tp <- as.matrix(tp)\n    \n    #ep <- as.array(emb)\n    ttp <- t(tp);\n    \n    # run diffusion steps to figure out end positions\n    cat("simulating diffusion ... ")\n    for(i in 1:diffusion.steps) {\n      cp <- cp %*% ttp;\n      #cp[cp<1e-5] <- 0;\n      cpl[[i+1]] <- cp;\n    }\n\n    # rank final points by probability\n    cpo <- t(apply(-cp,1,order))\n\n    # graph-based walkback approach\n    \n    # construct a walkback graph\n    trp <- as(ttp,\'dgTMatrix\')\n    \n    cat("constructing path graph ... ")\n    x <- do.call(rbind,lapply(1:(diffusion.steps+1),function(i) {\n      cbind(i=trp@i+1 + (i-1)*nrow(cp), # current time step\n            j=trp@j+1 + (i)*nrow(cp))\n    }))\n    x <- x[x[,2]<=nrow(cp)*(diffusion.steps+1),]\n    x <- spMatrix(nrow(cp)*(diffusion.steps+1),nrow(cp)*(diffusion.steps+1),i=x[,1],j=x[,2],x=rep(1,nrow(x)))\n    g <- igraph::graph.adjacency(x,mode=\'directed\')\n    rm(x); gc();\n    \n    # find topn trajectories for each cell\n    cat("tracing shortest trajectories ... ")\n    top.desti <- order(cp[celli,],decreasing=TRUE)[1:ntop.trajectories]\n    \n    \n    # calculate cell-specific weights\n    cw <- unlist(lapply(cpl,function(d) as.numeric(trp@x*(d[celli,trp@i+1]))))\n    cw <- cw[1:igraph::ecount(g)] # trim extra edges\n    # convert into penalty scores\n    cw <- -log(cw)\n    sp <- igraph::shortest_paths(g,from=celli,to=nrow(cp)*(diffusion.steps)+top.desti,weights=cw,mode=\'out\')\n    # remove time offset on the path nodes\n    sp <- lapply(sp$vpath,function(x) { y <- (as.integer(x) %% nrow(cp)); y[y==0] <- nrow(cp); y});\n    names(sp) <- rownames(cp)[top.desti]\n    cat("done.\\n")\n    lapply(sp,function(mp) {\n      if(!is.null(mp) && length(mp)>0) {\n        mp <- mp[!duplicated(mp)]\n        if(length(mp)>1)  {\n          lines(emb[mp,1],emb[mp,2],col=8,lty=3)\n          bp <- data.frame(do.call(cbind,xspline(emb[mp,],shape=trajectory.spline.shape,draw=F)))\n          lines(bp$x,bp$y,col=ac(1,alpha=0.6))\n          bp <- bp[abs(diff(bp$x))+abs(diff(bp$y))>1e-5,]\n          ai <- round(length(bp$x)*c(0.2,0.8,0.5))\n          arrows(bp$x[ai],bp$y[ai],bp$x[ai+1],bp$y[ai+1],angle=30,length=0.1,col=ac(1,alpha=0.6))\n        }\n      }\n    })\n    return(invisible(sp))\n  } else if(show.all.trajectories) { # show diffusion paths\n    cp <- Matrix::Diagonal(ncol(tp)); # cell position probabilities row-from col-to\n    rownames(cp) <- colnames(cp) <- rownames(tp);\n    ep <- as.array(emb)\n    \n    for(i in 1:diffusion.steps) {\n      cp <- cp %*% t(tp);\n      # expected position\n      cpm <- t(apply(cp,1,function(x) { x[x<sort(x,decreasing=TRUE)[3]] <- 0; x/sum(x) }))\n      epi <- as.matrix(cpm %*% emb);\n      #epi <- as.matrix(cp %*% emb);\n      ep <- abind::abind(ep,epi,along=3)\n    }\n    apply(ep,c(1),function(d) {\n      lines(d[1,],d[2,],lwd=1,col=ac(1,alpha=0.05))\n    })\n  } else {\n    # calculate arrows, draw\n    lapply(1:nrow(emb),function(i) {\n      # normalized directions to each point\n      di <- t(t(emb)-emb[i,])\n      di <- di/sqrt(Matrix::rowSums(di^2))*arrow.scale; di[i,] <- 0;\n      di <- Matrix::colSums(di*tp[,i]) - Matrix::colSums(di*np[,i]); # relative to expected kNN center\n      suppressWarnings(arrows(emb[colnames(em)[i],1],emb[colnames(em)[i],2],emb[colnames(em)[i],1]+di[1],emb[colnames(em)[i],2]+di[2],length=0.05,lwd=arrow.lwd))\n    })\n  }\n  return(invisible(tp))\n  \n}\n\n\n##\' DEPRECATED: Read in cell-specific bam files for SMART-seq2 measurement\n##\' This function is deprecated. Please use velocyto.py to prepare loom file from SMART-seq2 bam files.\n##\' @title read.smartseq2.bams\n##\' @param bam.files list of bam files\n##\' @param annotation.file refFlat genome annotation file (use gtfToGenePred to generate refFlat file from gtf)\n##\' @param min.exon.count minimum number of reads (across all cells) for an exon to be considered expressed in the dataset\n##\' @param n.cores number of cores to use\n##\' @return a list containing: emat - exonic (spliced) read count matrix ; iomat - intronic (unspliced) matrix; smat - spanning read matrix; base.df - data frame containing gene structural information; exons - exon annotation and read counts; genes - gene annotation table with additional structural info; expr.lstat - gene length statistics when considering only expressed exons\n##\' @export\nread.smartseq2.bams <- function(bam.files,annotation.file,min.exon.count=100,n.cores=defaultNCores()) {\n  # read in annotation \n  # TODO: enable direct gtf read\n  cat("reading gene annotation ... ")\n  x <- read.delim(annotation.file,header=F,sep="\\t",stringsAsFactors=F)\n  genes <- data.frame(name=x[,1],chr=x[,3],strand=x[,4],start=x[,5],end=x[,6],stringsAsFactors=F)\n  genes$p5 <- genes$start; genes$p5[genes$strand=="-"] <- genes$end[genes$strand=="-"];\n  genes$p3 <- genes$end; genes$p3[genes$strand=="-"] <- genes$start[genes$strand=="-"];\n  genes$size <- genes$end - genes$start;\n  cat("done (",nrow(genes),"genes)\\n")\n  \n  \n  # parse out exon information\n  cat("parsing exon information ... ")\n  exons <- do.call(rbind,lapply(1:nrow(x),function(i) {\n    df <- do.call(cbind,strsplit(as.character(x[i,c(10,11)]),\',\'))\n    cbind(df,rep(as.character(x[i,1]),nrow(df)))\n  }))\n  exons <- data.frame(gene=as.character(exons[,3]),start=as.numeric(exons[,1]),end=as.numeric(exons[,2]),stringsAsFactors=F)\n  exons$chr <- genes$chr[match(exons$gene,genes$name)]\n  \n  # eliminate duplicated exons? - points.within will count reads only once anyhow\n  exons <- exons[!duplicated(paste(exons[,1],exons[,2],exons[,3])),]\n  \n  # eliminate multiple variants - keep the largest ones\n  genes <- genes[order(genes$size,decreasing=T),]\n  genes <- genes[!duplicated(genes$name),]\n  cat("done\\n")\n  \n  # read in expression data\n  #  - ultimately we\'ll get three kinds of matrices here:\n  #      1. exonic reads\n  #      2. intronic-only reads\n  #      3. spanning reads (overlapping an intron and an exon)\n  #  - we will also use bam files to count the number of expressed exons,\n  #    to adjust the structural parameters of each gene. Though this is not\n  #    as important for the intron-only models.\n  \n  # read in all bam files\n  cat("reading in",length(bam.files),"bam files ... ")\n  # annotate individual reads\n  cdl <- parallel::mclapply(bam.files,t.annotate.bam.reads,genes=genes,exons=exons,margin=1,exon.margin=1,mc.cores=n.cores)\n  cat("done\\n")\n  # get count estimates per gene\n  cat("estimating gene counts ... ")\n  edl <- parallel::mclapply(cdl,t.get.estimates2,genes=genes,mc.cores=n.cores)\n  cat("done\\n")\n\n  cat("adjusting gene annotation based on expressed regions ... ")\n  # count total number of reads per exon to get the ones that are expressed ...\n  tl <- Matrix::colSums(do.call(rbind,parallel::mclapply(cdl,function(x) { ect <- table(c(x$exonstart,x$exonend)); fect <- rep(0,nrow(exons)); fect[as.integer(names(ect))] <- ect; fect; },mc.cores=n.cores,mc.preschedule=T)))\n  \n  # calculate gene length statistics, based on the expressed exons\n  expr.exons <- exons[tl>min.exon.count,]; # expressed exons - those showing >100 reads dataset-wide\n  expr.lstat <- lengthstats2(1e8,genes=genes,exons=expr.exons); # intron/exon length statistics for genes, considering only expressed exons\n  \n  # compile joint table\n  df <- data.frame(il=log10(expr.lstat[,2]+1),el=log10(expr.lstat[,3]+1)); rownames(df) <- rownames(expr.lstat)\n  df$nex <- as.integer(table(expr.exons$gene)[rownames(df)]);\n  df$nex[is.na(df$nex)] <- 0\n  cat("done\\n")\n  \n  # construct sparse input matrices\n  # exonic counts\n  emat <- do.call(cbind,lapply(edl,function(d) { (d[,\'exon\']) }));  emat[!is.finite(emat)] <- 0;  emat <- as(emat,\'dgCMatrix\')\n  # spanning counts\n  smat <- do.call(cbind,lapply(edl,function(d) { (d[,\'span\']) }));  smat[!is.finite(smat)] <- 0;  smat <- as(smat,\'dgCMatrix\')\n  # intron-only counts\n  iomat <- do.call(cbind,lapply(edl,function(d) { (d[,\'introno\']) }));  iomat[!is.finite(iomat)] <- 0;  iomat <- as(iomat,\'dgCMatrix\')\n  \n  \n  return(list(emat=emat,iomat=iomat,smat=smat,base.df=df,exons=exons,genes=genes,expr.lstat=expr.lstat))\n}\n\n##\' Read in cell-specific bam files for STRT/C1\n##\'\n##\' @param bam.files list of bam files (one per cell)\n##\' @param annotation.file gene annotation refFlat file\n##\' @param min.exon.count minimum number of molecules (across all cells) for the exon to be considered expressed\n##\' @param n.cores number of cores to use\n##\' @param min.umi.reads minimum number of read required per UMI/gene combination to be counted (defaults to 1)\n##\' @return a list structure analogous to the return of read.smartseq2.bams(), counting molecules instead of reads.\nread.strtc1.bams <- function(bam.files,annotation.file,min.exon.count=100,n.cores=defaultNCores(),min.umi.reads=1) {\n  # read in annotation \n  # TODO: enable direct gtf read\n  cat("reading gene annotation ... ")\n  x <- read.delim(annotation.file,header=F,sep="\\t",stringsAsFactors=F)\n  genes <- data.frame(name=x[,1],chr=x[,3],strand=x[,4],start=x[,5],end=x[,6],stringsAsFactors=F)\n  genes$p5 <- genes$start; genes$p5[genes$strand=="-"] <- genes$end[genes$strand=="-"];\n  genes$p3 <- genes$end; genes$p3[genes$strand=="-"] <- genes$start[genes$strand=="-"];\n  genes$size <- genes$end - genes$start;\n  cat("done (",nrow(genes),"genes)\\n")\n  \n  \n  # parse out exon information\n  cat("parsing exon information ... ")\n  exons <- do.call(rbind,lapply(1:nrow(x),function(i) {\n    df <- do.call(cbind,strsplit(as.character(x[i,c(10,11)]),\',\'))\n    cbind(df,rep(as.character(x[i,1]),nrow(df)))\n  }))\n  exons <- data.frame(gene=as.character(exons[,3]),start=as.numeric(exons[,1]),end=as.numeric(exons[,2]),stringsAsFactors=F)\n  exons$chr <- genes$chr[match(exons$gene,genes$name)]\n  \n  # eliminate duplicated exons? - points.within will count reads only once anyhow\n  exons <- exons[!duplicated(paste(exons[,1],exons[,2],exons[,3])),]\n  \n  # eliminate multiple variants - keep the largest ones\n  genes <- genes[order(genes$size,decreasing=T),]\n  genes <- genes[!duplicated(genes$name),]\n  cat("done\\n")\n  \n  # read in expression data\n  #  - ultimately we\'ll get three kinds of matrices here:\n  #      1. exonic reads\n  #      2. intronic-only reads\n  #      3. spanning reads (overlapping an intron and an exon)\n  #  - we will also use bam files to count the number of expressed exons,\n  #    to adjust the structural parameters of each gene. Though this is not\n  #    as important for the intron-only models.\n  \n  # read in all bam files\n  cat("reading in",length(bam.files),"bam files ... ")\n  # annotate individual reads\n\n  #t.reduce.umi <- function(z) { z[!duplicated(paste(gsub(".*?_","",z$name),z$gene)),] }\n  t.reduce.umi <- function(z) {\n    z$umig <- paste(gsub(".*?_","",z$name),z$gene)\n    z[match(names(which(table(z$umig)>=min.umi.reads)),z$umig),]\n  }\n  \n  cdl <- parallel::mclapply(bam.files,function(x) t.reduce.umi(t.annotate.bam.reads(x,genes=genes,exons=exons,margin=1,exon.margin=1,use.names=T)),mc.cores=n.cores)\n  \n  \n  cat("done\\n")\n  # get count estimates per gene\n  cat("estimating gene counts ... ")\n  edl <- parallel::mclapply(cdl,t.get.estimates2,genes=genes,mc.cores=n.cores)\n  cat("done\\n")\n\n  cat("adjusting gene annotation based on expressed regions ... ")\n  # count total number of reads per exon to get the ones that are expressed ...\n  tl <- Matrix::colSums(do.call(rbind,parallel::mclapply(cdl,function(x) { ect <- table(c(x$exonstart,x$exonend)); fect <- rep(0,nrow(exons)); fect[as.integer(names(ect))] <- ect; fect; },mc.cores=n.cores,mc.preschedule=T)))\n  \n  # calculate gene length statistics, based on the expressed exons\n  expr.exons <- exons[tl>min.exon.count,]; # expressed exons - those showing >100 reads dataset-wide\n  expr.lstat <- lengthstats2(1e8,genes=genes,exons=expr.exons); # intron/exon length statistics for genes, considering only expressed exons\n  \n  # compile joint table\n  df <- data.frame(il=log10(expr.lstat[,2]+1),el=log10(expr.lstat[,3]+1)); rownames(df) <- rownames(expr.lstat)\n  df$nex <- as.integer(table(expr.exons$gene)[rownames(df)]);\n  df$nex[is.na(df$nex)] <- 0\n  cat("done\\n")\n  \n  # construct sparse input matrices\n  # exonic counts\n  emat <- do.call(cbind,lapply(edl,function(d) { (d[,\'exon\']) }));  emat[!is.finite(emat)] <- 0;  emat <- as(emat,\'dgCMatrix\')\n  # spanning counts\n  smat <- do.call(cbind,lapply(edl,function(d) { (d[,\'span\']) }));  smat[!is.finite(smat)] <- 0;  smat <- as(smat,\'dgCMatrix\')\n  # intron-only counts\n  iomat <- do.call(cbind,lapply(edl,function(d) { (d[,\'introno\']) }));  iomat[!is.finite(iomat)] <- 0;  iomat <- as(iomat,\'dgCMatrix\')\n  \n  \n  return(list(emat=emat,iomat=iomat,smat=smat,base.df=df,exons=exons,genes=genes,expr.lstat=expr.lstat))\n}\n\n\n\n# parse bam file and annotate the reads relative to genes/exons\nt.annotate.bam.reads <- function(fname, genes, exons, chrl=unique(genes$chr), test.strand=F, margin=3e3, tags=NULL,use.names=FALSE,exon.margin=0) {\n\n  if (!requireNamespace("GenomicRanges", quietly = TRUE)) {\n    stop("Package \\"GenomicRanges\\" needed for this function to work. Please install it.", call. = FALSE)\n  }\n\n  if (!requireNamespace("Rsamtools", quietly = TRUE)) {\n    stop("Package \\"Rsamtools\\" needed for this function to work. Please install it.",call. = FALSE)\n  }\n\n  if (!requireNamespace("GenomeInfoDb", quietly = TRUE)) {\n    stop("Package \\"GenomeInfoDb\\" needed for this function to work. Please install it.",call. = FALSE)\n  }\n\n  \n  if(!is.null(tags)) {\n    param <- Rsamtools::ScanBamParam(flag=Rsamtools::scanBamFlag(isDuplicate=FALSE,isSecondaryAlignment=FALSE),tag=unlist(tags))\n  } else {\n    param <- Rsamtools::ScanBamParam(flag=Rsamtools::scanBamFlag(isDuplicate=FALSE,isSecondaryAlignment=FALSE))\n  }\n  z <- GenomicAlignments::readGAlignments(fname,param=param,use.names=use.names)\n  \n  bam.data <- data.frame(chr=as.vector(GenomeInfoDb::seqnames(z)),start=BiocGenerics::start(z),end=BiocGenerics::end(z),strand=as.vector(BiocGenerics::strand(z)),stringsAsFactors=F)\n  ## if(!is.null(tags)) {\n  ##   bam.data <- cbind(bam.data,as.data.frame(S4Vectors::elementMetadata((z))))\n  ## }\n  if(use.names) {\n    bam.data$name <- names(z)\n  }\n  bam.data <- bam.data[bam.data$chr %in% chrl,]\n  \n  chrl <- chrl[chrl %in% unique(bam.data$chr)]\n  \n  ## assign reads to genes\n  x <- do.call(rbind,lapply(chrl,function(chr) {\n    ri <- which(bam.data$chr==chr)\n    results <- data.frame(bam.data[ri,],gene=rep(NA,length(ri)), exonstart=0, exonend=0, readtype=\'NI\',stringsAsFactors=F) ## intronic read unless evidence to change it\n    gi <- which(genes$chr==chr)\n    ei <- which(exons$chr==chr)\n    \n    # check if the read is associated with any gene\n    pi1 <- points.within(bam.data$start[ri],genes$start[gi]-margin,genes$end[gi]+margin); # note that many gene fragments are supplied at once - no need to loop in R!\n    pi2 <- points.within(bam.data$end[ri],genes$start[gi]-margin,genes$end[gi]+margin);\n    vi <- pi1>0 | pi2>0;\n    if(any(vi)) { \n      results$gene[vi] <- genes$name[gi[pmax(pi1[vi],pi2[vi])]]; # take largest matched gene index .. should check for consistency\n    }\n    \n    # check exon mapping\n    pi1 <- points.within(bam.data$start[ri],exons$start[ei]-exon.margin,exons$end[ei]+exon.margin);\n    pi2 <- points.within(bam.data$end[ri],exons$start[ei]-exon.margin,exons$end[ei]+exon.margin);\n    \n    # see if both ends map to the same exon - that\'s NC\n    vi <- pi1>0  & pi1==pi2\n    if(any(vi)) { results$readtype[vi] <- \'NC\'; results$gene[vi] <- exons$gene[ei[pi1[vi]]]; }\n    \n    # different exons - NE\n    vi <- pi1>0 & pi2>0 & pi1!=pi2\n    if(any(vi)) { results$readtype[vi] <- \'NE\'; results$gene[vi] <- exons$gene[ei[pi1[vi]]]; }\n    \n    # one exon, and one something else - assume NS for now, will check for margins later\n    vi <- sign(pi1) != sign(pi2);\n    if(any(vi)) { results$readtype[vi] <- \'NS\'; results$gene[vi] <- exons$gene[ei[pmax(pi1,pi2)[vi]]]; }\n    \n    # note: I am not sure what these two values are needed for, but here\'s how to get them\n    pi1[pi1==-1] <- NA; pi2[pi2==-1] <- NA;\n    results$exonstart <- ei[pi1]; results$exonend <- ei[pi2];\n    \n    # assign margin classes (N5 - fully in the 5\' margin, N3 - fully in the 3\', N5b - spanning exon and 5\' margin, N3b - spanning exon and 3\' margin)\n    if(margin>0) {\n      # start is in the start margin\n      pi1 <- points.within(bam.data$start[ri],genes$start[gi]-margin,genes$start[gi]-1);\n      pi1check <- points.within(bam.data$start[ri],genes$start[gi],genes$end[gi]); # check for genes\n      ivi <- pi1>0 & pi1check>0 # both in margin and in some other gene - should be invalidated\n      if(any(ivi)) { results$gene[ivi] <- NA }\n      \n      # if it is in vi, then it must be partially margin (joining margin and the first/last exon), otherwise it\'s fully in margin\n      vi2 <- pi1>0 & vi;\n      if(any(vi2)) { results$readtype[vi2] <- ifelse(genes$strand[gi[pi1[vi2]]]==\'+\',\'N5b\',\'N3b\') }\n      vi2 <- pi1>0 & !vi;\n      if(any(vi2)) { results$readtype[vi2] <- ifelse(genes$strand[gi[pi1[vi2]]]==\'+\',\'N5\',\'N3\') }\n      \n      # and now check the other end of the read\n      # end is in the margin\n      pi2 <- points.within(bam.data$end[ri],genes$end[gi]+1,genes$end[gi]+margin);\n      pi2check <- points.within(bam.data$end[ri],genes$start[gi],genes$end[gi]); # check for genes\n      ivi <- pi2>0 & pi2check>0 # both in margin and in some other gene - should be invalidated\n      if(any(ivi)) { results$gene[ivi] <- NA }\n      \n      # if it is in vi, then it must be partially margin (joining margin and the first/last exon), otherwise it\'s fully in margin\n      vi2 <- pi2>0 & vi;\n      if(any(vi2)) { results$readtype[vi2] <- ifelse(genes$strand[gi[pi2[vi2]]]==\'+\',\'N3b\',\'N5b\') }\n      vi2 <- pi2>0 & !vi;\n      if(any(vi2)) { results$readtype[vi2] <- ifelse(genes$strand[gi[pi2[vi2]]]==\'+\',\'N3\',\'N5\') }\n    }\n    \n    # omit reads that didn\'t get assinged to genes (or were invalidated)\n    results <- results[!is.na(results$gene),]\n    return(results)\n  }))\n}\n\n# count different read types per gene\nt.get.estimates2 <- function(cd,genes) {\n  cd$gene <- match(cd$gene,genes$name);\n  # number of reads within the exons\n  vi <- cd$readtype %in% c("NC","NE");  exon.counts <- tapply(cd$gene[vi],cd$gene[vi],length)\n  \n  # intronic reads\n  vi <- cd$readtype %in% c("NI","NS"); intron.counts <- tapply(cd$gene[vi],cd$gene[vi],length)\n  # intron only counts\n  vi <- cd$readtype %in% c("NI"); introno.counts <- tapply(cd$gene[vi],cd$gene[vi],length)\n  # span only counts\n  vi <- cd$readtype %in% c("NS"); span.counts <- tapply(cd$gene[vi],cd$gene[vi],length)\n  \n  # number of reads in the upstream region (don\'t include those briding exon, since they\'re supposed to be a different strand)\n  vi <- cd$readtype %in% c("N5"); upstream.counts <- tapply(cd$gene[vi],cd$gene[vi],length)\n  # downstream region (running from the exon is fine, since it can be the same strand)\n  vi <- cd$readtype %in% c("N3","N3b"); downstream.counts <- tapply(cd$gene[vi],cd$gene[vi],length)\n  \n  \n  # construct report matrix\n  df <- matrix(NA,nrow=nrow(genes),ncol=7)\n  df[as.integer(names(exon.counts)),1] <- exon.counts\n  df[,2] <- 0\n  # normalize by the expected fraction of reads from the last 1kb (assume that the last exon is ~1kb) TODO: transcript-coordinate estimation\n  #df[,2] <- df[,2]/lstat.e3$e*lstat$e;\n  \n  df[as.integer(names(intron.counts)),3] <- intron.counts\n  df[as.integer(names(upstream.counts)),4] <- upstream.counts\n  df[as.integer(names(downstream.counts)),5] <- downstream.counts\n  df[as.integer(names(introno.counts)),6] <- introno.counts\n  df[as.integer(names(span.counts)),7] <- span.counts\n  rownames(df) <- genes$name; colnames(df) <- c("exon","mg5","intron","upstream","downstream","introno","span")\n  df\n}\n\n\nlengthstats2 <- function(maxlen,genes,exons,n.cores=defaultNCores(),p3=FALSE) {\n  lf <- do.call(rbind,parallel::mclapply(1:nrow(genes),function(gi) {\n    ei <- which(exons$gene==genes$name[gi])\n    if((genes$strand[gi]=="+" & !p3) | (genes$strand[gi]=="-" & p3)) {\n      nt <- seq(genes$start[gi],min(genes$start[gi]+maxlen-1,genes$end[gi]),by=1);\n    } else {\n      nt <- seq(max(genes$end[gi]-maxlen-1,genes$start[gi]),genes$end[gi],by=1)\n    }\n    c(t=length(nt),i=sum(points.within(nt,exons$start[ei],exons$end[ei],sorted=T)==-1))\n  },mc.cores=n.cores,mc.preschedule=T))\n  lf <- cbind(lf,e=lf[,"t"]-lf[,"i"])\n  rownames(lf) <- genes$name\n  data.frame(lf)\n}\n\n# quick utility function to calculate library sizes using edgeR\nedgeR.libsize <- function(mat, ...) {\n  f <- edgeR::calcNormFactors(mat)\n  f <- f/exp(mean(log(f)))\n  Matrix::colSums(mat)*f\n}\n\n# quick self-naming vector routine\nsn <- function(x) { names(x) <- x; x}\n\n#\' adjust colors, while keeping the vector names\n#\' \n#\' @param x color vector\n#\' @param alpha transparenscy value (passed to adjustcolors as alpha.f)\n#\' @param ... parameters passsed to adjustcolor\n#\' @export\nac <- function(x, alpha=1, ...) { y <- adjustcolor(x, alpha.f=alpha, ...); names(y) <- names(x); return(y)}\n\n# quick function to map value vector to colors\nval2col <- function(x,gradientPalette=NULL,zlim=NULL,gradient.range.quantile=0.95) {\n  if(all(sign(x)>=0)) {\n    if(is.null(gradientPalette)) {\n      gradientPalette <- colorRampPalette(c(\'gray90\',\'red\'), space = "Lab")(1024)\n    }\n    if(is.null(zlim)) {\n      zlim <- as.numeric(quantile(na.omit(x),p=c(1-gradient.range.quantile,gradient.range.quantile)))\n      if(diff(zlim)==0) {\n        zlim <- as.numeric(range(na.omit(x)))\n      }\n    }\n    x[x<zlim[1]] <- zlim[1]; x[x>zlim[2]] <- zlim[2];\n    x <- (x-zlim[1])/(zlim[2]-zlim[1])\n    \n  } else {\n    if(is.null(gradientPalette)) {\n      gradientPalette <- colorRampPalette(c("blue", "grey90", "red"), space = "Lab")(1024)\n    }\n    if(is.null(zlim)) {\n      zlim <- c(-1,1)*as.numeric(quantile(na.omit(abs(x)),p=gradient.range.quantile))\n      if(diff(zlim)==0) {\n        zlim <- c(-1,1)*as.numeric(na.omit(max(abs(x))))\n      }\n    }\n    x[x<zlim[1]] <- zlim[1]; x[x>zlim[2]] <- zlim[2];\n    x <- (x-zlim[1])/(zlim[2]-zlim[1])\n    \n  }\n  \n  gp <- gradientPalette[x*(length(gradientPalette)-1)+1]\n  if(!is.null(names(x))) { names(gp) <- names(x) }\n  gp\n}\n\n# estimate expression of a projected cell given the original expression matrix and deltaE matrix\n# delta is the amount of time onto which the projection should be done\nt.get.projected.cell <- function(em,deltae,delta=1,target.mult=1e3,model.mult=1e3,size.normalize=FALSE) {\n  rz <- matrix(0,nrow=nrow(em),ncol=ncol(em)); colnames(rz) <- colnames(em); rownames(rz) <- rownames(em)\n  gn <- intersect(rownames(deltae),rownames(rz))\n  rz[match(gn,rownames(rz)),colnames(deltae)] <- deltae[gn,]*target.mult/model.mult; # correcting for different mult\n  emn <- em+delta*rz;  emn[emn<0] <- 0\n  # rescale ... note: none of these seem appropriate\n  #i <- 11;smoothScatter(sqrt(em[,i]),sqrt(emn[,i])); abline(a=0,b=1)\n  #emn.scale <- unlist(lapply(1:ncol(em),function(i) { n <- calcNormFactors(cbind(em[,1],emn[,1])); n[2]/n[1]}))\n  #emn <- t(t(emn)/emn.scale)\n  if(size.normalize) {\n    emn <- t(t(emn)/Matrix::colSums(emn)*Matrix::colSums(em))\n  }\n  emn\n}\n\n# calculates the difference in the number of counts based on the library size, renormalizes\n# note: also introduces centripetal velocity\nt.get.projected.cell2 <- function(em,cellSize,deltae,mult=1e3,delta=1) {\n  rz <- matrix(0,nrow=nrow(em),ncol=ncol(em)); colnames(rz) <- colnames(em); rownames(rz) <- rownames(em)\n  gn <- intersect(rownames(deltae),rownames(rz))\n  rz[match(gn,rownames(rz)),colnames(deltae)] <- deltae[gn,]; \n  # translate fpm delta into the number of molecules based on the current cell size\n  rz <- t(t(rz)*cellSize)\n  emm <- t(t(em)*cellSize)\n  emn <- emm + rz*delta; \n  emn[emn<0] <- 0;\n  newCellSize <- (cellSize+Matrix::colSums(emn-emm)/mult)\n  emn <- t(t(emn)/newCellSize)\n  \n  #emn <- t(t(emn)/Matrix::colSums(emn)*Matrix::colSums(em))\n  emn\n}\n\n# reads layers/spliced/unspliced/ambiguous from a loom file\n##\' Read in loom matrices of spliced/unpsliced reads as\n##\' prepared by velocyto.py CLI\n##\'\n##\' @param file loom file name\n##\' @param engine Use hdf5r or h5 to import loom file\n##\' @return a list containing spliced, unspliced, ambiguous and spanning matrices\n##\' @export\nread.loom.matrices <- function(file, engine=\'hdf5r\') {\n  if (engine == \'h5\'){\n    cat(\'reading loom file via h5...\\n\')\n    f <- h5::h5file(file,mode=\'r\');\n    cells <- f["col_attrs/CellID"][];\n    genes <- f["row_attrs/Gene"][];\n    dl <- c(spliced="/layers/spliced",unspliced="/layers/unspliced",ambiguous="/layers/ambiguous");\n    if("/layers/spanning" %in% h5::list.datasets(f)) {\n      dl <- c(dl,c(spanning="/layers/spanning"))\n    }\n    dlist <- lapply(dl,function(path) {\n      m <- as(f[path][],\'dgCMatrix\'); rownames(m) <- genes; colnames(m) <- cells; return(m)\n    })\n    h5::h5close(f)\n    return(dlist)\n  } else if (engine == \'hdf5r\') {\n    cat(\'reading loom file via hdf5r...\\n\')\n    f <- hdf5r::H5File$new(file, mode=\'r\')\n    cells <- f[["col_attrs/CellID"]][]\n    genes <- f[["row_attrs/Gene"]][]\n    dl <- c(spliced="layers/spliced",\n            unspliced="layers/unspliced",\n            ambiguous="layers/ambiguous")\n    if("layers/spanning" %in% hdf5r::list.datasets(f)) {\n      dl <- c(dl, c(spanning="layers/spanning"))\n    }\n    dlist <- lapply(dl, function(path) {\n      m <- as(t(f[[path]][,]),\'dgCMatrix\')\n      rownames(m) <- genes; colnames(m) <- cells;\n      return(m)\n    })\n    f$close_all()\n    return(dlist)\n  }\n  else {\n    warning(\'Unknown engine. Use hdf5r or h5 to import loom file.\')\n    return(list())\n  }\n}\n\n\n##\' identify positions of likely internal priming sites by looking for polyA/polyT stretches within\n##\' annotated intronic regions\n##\'\n##\' @param gtf.file location of a gtf file with gene annotations\n##\' @param genome bioC genome structure (i.e. Mmusculus)\n##\' @param genome.name name of the genome assembly (i.e. mm10)\n##\' @param w A/T weight in the PWM\n##\' @param n length of the motif\n##\' @param min.score minimal required match score\n##\' @param add.chr whether to add \'chr\' prefix to the chromosome names in the gtf annotation (to match bioC)\n##\' @return a data frame containing list of likely internal priming sites, listing chromosome ($chr), positions ($start/$end), name, PWM matching score, PWM match strand ($strand), gene, gene strand ($gs), and whether the motif is in concordant direction of gene transcription ($conc)\n##\' @examples\n##\' \\dontrun{\n##\' library(BSgenome.Mmusculus.UCSC.mm10)\n##\' ip.mm10 <- find.ip.sites(\'refdata-cellranger-mm10-1.2.0/genes/genes.gtf\',Mmusculus,\'mm10\')\n##\' }\n##\' @export\nfind.ip.sites <- function(gtf.file,genome,genome.name,w=0.9,n=15,min.score=\'80%\',add.chr=TRUE) {\n  \n  if (!requireNamespace("GenomicRanges", quietly = TRUE)) {\n    stop("Package \\"GenomicRanges\\" needed for this function to work. Please install it.",\n         call. = FALSE)\n  }\n\n  if (!requireNamespace("Biostrings", quietly = TRUE)) {\n    stop("Package \\"Biostrings\\" needed for this function to work. Please install it.",\n         call. = FALSE)\n  }\n\n  # helper functions to move out of bioC classes\n  grange2df <- function(x,name=\'hit\') {\n    data.frame(chr=as.character(GenomicRanges::seqnames(x)),\n               start=GenomicRanges::start(x),\n               end=GenomicRanges::end(x),\n               name=name,\n               score=GenomicRanges::score(x),\n               strand=GenomicRanges::strand(x))\n  }\n  \n  # read specified features from gtf file, recording specified attributes\n  t.read.gtf <- function(file,feature="gene",atts=c("gene_id"),n.cores=30) {\n    if (!requireNamespace("data.table", quietly = TRUE)) {\n      stop("Package \\"data.table\\" needed for this function to work. Please install it.",\n           call. = FALSE)\n    }\n    #x <- read.table(file,header=F,sep="\\t",stringsAsFactors=F)\n    x <- data.table::fread(file,header=F,sep="\\t",stringsAsFactors=F,data.table=FALSE)\n    vi <- which(x[,3]==feature)\n    names(atts) <- atts;\n    ad <- do.call(cbind,lapply(atts,function(att) {\n      gsub("\\\\\\"","",gsub(paste(\'.*\',att,\' ([^;]*);.*\',sep=""),"\\\\1",x[vi,9]))\n    }))\n    df <- x[vi,c(1,4,5,7)]\n    colnames(df) <- c("chr","start","end","strand");\n    cbind(df,ad,stringsAsFactors=F)\n  }\n\n  cat("reading genes ... ");\n  genes <- t.read.gtf(gtf.file,atts=c("gene_name"))\n  cat("done\\n");\n  cat("reading exons ... ");\n  exons <- t.read.gtf(gtf.file,feature=\'exon\',atts=c("gene_name","transcript_biotype"))\n  exons <- exons[exons$transcript_biotype==\'protein_coding\',] # want to mask everything else\n  if(add.chr) {\n    genes$chr <- paste0(\'chr\',genes$chr);\n    exons$chr <- paste0(\'chr\',exons$chr);\n  }\n  cat("done\\n");\n  cat("making ranges ... ")\n  gene.ranges <- GenomicRanges::GRanges(genes$chr,IRanges::IRanges(start=genes[,2],end=genes[,3]),strand=genes$strand)\n  names(gene.ranges) <- genes$gene_name\n  exon.ranges <- GenomicRanges::GRanges(exons$chr,IRanges::IRanges(start=exons[,2],end=exons[,3]),strand=exons$strand)\n  cat("done\\n");\n  cat("matching hits ... ")\n  N <- 1e3;\n  m <- as.matrix(rbind(A=rep(round(w*N),n),C=rep(round((1-w)/3*N),n),G=rep(round((1-w)/3*N),n),T=rep(round((1-w)/3*N),n))); storage.mode(m) <- \'integer\'\n  pwm <- Biostrings::PWM(m)\n  hits <- Biostrings::matchPWM(pwm,genome,min.score=min.score,with.score=T)\n  cat("done\\n");\n  cat("annotating hits .");\n  df <- grange2df(hits,name=paste0(\'(A)\',n)) # convert into a table\n  cat(".");\n  df <- do.call(rbind,tapply(1:nrow(df),df$chr,function(ii) df[ii[order(df$start[ii])],])) # sort\n  cat(".");\n  df <- rbind(groupMotifs(df[df$strand==\'+\',]),groupMotifs(df[df$strand==\'-\',])) # cluster\n  cat(".");\n  df <- do.call(rbind,tapply(1:nrow(df),df$chr,function(ii) df[ii[order(df$start[ii])],])) # sort again\n  cat(".");\n  # get gene strand information and classify concordance\n  sn <- function(x) { names(x) <- x; x}\n  gene.margin <- 0;\n  df <- do.call(rbind,lapply(sn(intersect(unique(genes$chr),unique(df$chr))),function(chr) {\n    vg <- which(genes$chr==chr);\n    vm <- which(df$chr==chr);\n    ei <- points.within((df$start[vm]+df$end[vm])/2,genes$start[vg]-gene.margin,genes$end[vg]+gene.margin)\n    vi <- which(ei>-1)\n    x <- cbind(df[vm[vi],],gene=genes$gene_name[vg[ei[vi]]],gs=genes$strand[vg[ei[vi]]])\n    x$conc <- x$strand==x$gs\n    x\n  }))\n  cat(". done\\n");\n  return(df)\n}\n\n##\' read in detailed molecular mapping info from hdf5 file as written out by "-d" option of velocyto.py\n##\'\n##\' @param fname name of the hdf5 detailed molecular mapping (debug) file, written out by velocyto.py\n##\' @param cell.clusters optional cell cluster factor\n##\' @param internal.priming.info optionall internal priming info, as produced by find.ip.sites() function\n##\' @param min.exon.count minimal total (dataset-wide) number of molecules for an exon to be considered expressed\n##\' @param n.cores number of cores to use\n##\' @param engine use either h5 or hdf5r to read the input hdf5 file\n##\' @return a list containing gene structural information data structure ($gene.df, with el,il, nex,nipconc,nipdisc columns corresponding to the log10 exonic length, intronic length, number of exons, numebr of internal concordant and discordant priming sites, respectively), and $info tables from the hdf5 file with an additional per-cluster entry $cluster.feature.counts table showing per-feature (rows) per-cluster (column) molecule counts (if cell.clusters are not supplied $info$cluster.feauture.counts will contain one column, \'all\' giving dataset-wide counts)\n##\' @export\nread.gene.mapping.info <- function(fname,cell.clusters=NULL,internal.priming.info=NULL,min.exon.count=10,n.cores=defaultNCores(),engine=\'hdf5r\') {\n  cat("reading in mapping info from",fname,\' via\', engine)\n  if (engine == \'h5\') {\n    f <- h5::h5file(fname,mode=\'r\')\n  } else if (engine == \'hdf5r\') {\n    f <- hdf5r::H5File$new(fname, mode=\'r\')\n  } else {\n    stop(\'Unknown engine. Use either hdf5r or h5.\\n\')\n  }\n  #list.datasets(f)\n  # read in info tables\n  if (engine == \'h5\') info <- lapply(sn(c("chrm","exino","features_gene","is_intron","is_last3prime","start_end","strandplus","tr_id")),function(n) { cat(\'.\'); f[paste(\'/info\',n,sep=\'/\')][] })\n  if (engine == \'hdf5r\') {\n  info <- lapply(\n    sn(c("chrm","exino","features_gene", "is_intron","is_last3prime",\n         "start_end","strandplus","tr_id")),\n    function(n) {\n      cat(\'.\'); x <- f[[paste(\'info\', n, sep=\'/\')]]$read();\n      if (is.matrix(x)) {t(x)} else {x} })\n  }\n  info$chrm <- gsub("^chr","",info$chrm)\n  cat(" done\\n")\n  # extract cell names\n  if (engine == \'h5\') cnames <- gsub(\'/pos\',\'\',gsub(\'/cells/\',\'\',grep(\'/pos\',grep("/cells/", h5::list.datasets(f),value=T),value=T)))\n  if (engine == \'hdf5r\') {\n    cnames <- gsub(\n      \'/pos\', \'\',\n      gsub(\'cells/\',\'\',\n           grep(\'/pos\',\n                grep("cells/", hdf5r::list.datasets(f), value=T),\n                value=T)))\n  }\n  if(!is.null(cell.clusters)) {\n    # count abundancies per element for each cell cluster\n    cell.clusters <- as.factor(cell.clusters)\n    if(!any(names(cell.clusters) %in% cnames)) {\n      warning(paste("could not match any of the specified cell names. hdf5 file contains names like [",paste(cnames[1:3],collapse=\' \'),"... ]"))\n      cat("parsing out feature counts across all cells ... ")\n      if (engine == \'h5\') {\n        info$cluster.feature.counts <- cbind(\'all\'=tabulate(unlist(lapply(\n          cnames,\n          function(n) f[paste(\'/cells\',n,\'ixs\',sep=\'/\')][] ))+1,nbins=length(info$chrm)))\n      } else if (engine == \'hdf5r\') {\n        info$cluster.feature.counts <- cbind(\'all\'=tabulate(unlist(lapply(\n          cnames,\n          function(n) f[[paste(\'cells\',n,\'ixs\',sep=\'/\')]]$read() ))+1,\n          nbins=length(info$chrm)))\n      } else {stop(\'Unknown engine. Use either hdf5r or h5.\\n\')}\n      cat("done\\n")\n    } else {\n      cat("parsing out info for",length(levels(cell.clusters)),"clusters: [");\n      if (engine == \'h5\'){\n        cluster.feature.counts <- do.call(cbind,tapply(names(cell.clusters),as.factor(cell.clusters),function(ii) {\n          cat(".")\n          tabulate(unlist(lapply(\n            ii,\n            function(n) f[paste(\'/cells\',n,\'ixs\',sep=\'/\')][] ))+1,\n            nbins=length(info$chrm))\n        }))\n      }\n      if (engine == \'hdf5r\') {\n        cluster.feature.counts <- do.call(cbind,tapply(names(cell.clusters),as.factor(cell.clusters),function(ii) {\n          cat(".")\n          tabulate(unlist(lapply(\n            ii,\n            function(n) f[[paste(\'cells\',n,\'ixs\',sep=\'/\')]]$read() ))+1,\n            nbins=length(info$chrm))\n        }))\n      }\n\n      cat(". ]. done\\n")\n      info$cluster.feature.counts <- cluster.feature.counts;\n    }\n  } else {\n    # combine counts on all cells\n    cat("parsing out feature counts across all cells ... ")\n    if (engine == \'h5\'){\n      info$cluster.feature.counts <- cbind(\'all\'=tabulate(unlist(lapply(\n        cnames,\n        function(n) f[paste(\'/cells\',n,\'ixs\',sep=\'/\')][] ))+1,\n        nbins=length(info$chrm)))\n    }\n    if (engine == \'hdf5r\'){\n      info$cluster.feature.counts <- cbind(\'all\'=tabulate(unlist(lapply(\n        cnames,\n        function(n) f[[paste(\'cells\',n,\'ixs\',sep=\'/\')]]$read() ))+1,\n        nbins=length(info$chrm)))\n    }\n    cat("done\\n")\n  }\n  if (engine == \'h5\' ) h5::h5close(f)\n  if (engine == \'hdf5r\') f$close_all()\n\n  # calculate dataset-wide effective gene length and other parameters\n  # attempt to get unique gene names\n  #gchr <- paste(info$features_gene,info$chrm,sep=\':\')\n  genes <- info$features_gene;\n\n  if(!is.null(internal.priming.info)) {\n    internal.priming.info$chr <- gsub("^chr","",as.character(internal.priming.info$chr))\n  }\n\n  t.get.lengthinfo <- function(fc,min.exon.count=10) {\n    #fc <- rowSums(cluster.feature.counts)\n    # find last expressed exon for each gene\n    gdf <- do.call(rbind,mclapply(sn(unique(genes)),function(gene) {\n      ii <- which(genes==gene);\n      exons <- info$is_intron[ii]==\'FALSE\'\n      # select exons with a valid minimal expression level\n      valid.exons <- fc[ii[exons]]>=min.exon.count\n      # is gene more on one chromosome, ignore the gene\n      if(length(unique(info$chrm[ii]))>1) {\n        valid.exons[valid.exons] <- FALSE;\n      }\n\n      if(any(valid.exons)) {\n        # number of exons, their lengths\n        valid.exons.sizes <- info$start_end[ii[exons][valid.exons],,drop=F]\n        el <- flatLength(valid.exons.sizes[order(valid.exons.sizes[,1,drop=F],decreasing=FALSE),,drop=F]);\n        # define effective range\n        gene.range <- range(valid.exons.sizes)\n        gene.size <- diff(gene.range)+1\n        rv <- c(el=log10(el+1),il=log10((gene.size-el)+1),nex=nrow(valid.exons.sizes))\n        if(!is.null(internal.priming.info)) {\n          vi <- which(internal.priming.info$chr==info$chrm[ii[1]] & internal.priming.info$start>=gene.range[1] & internal.priming.info$end<=gene.range[2])\n          if(length(vi)>0) {\n            nipconc <- sum(internal.priming.info$conc[vi]);\n            rv <- c(rv,c(nipconc=nipconc,nipdisc=length(vi)-nipconc))\n          } else {\n            rv <- c(rv,c(nipconc=0,nipdisc=0))\n          }\n        }\n        return(rv)\n      } else {\n        return(NULL)\n      }\n    },mc.cores=n.cores,mc.preschedule=TRUE))\n  }\n  cat("calculating gene stats ... ")\n  base.df <- t.get.lengthinfo(rowSums(info$cluster.feature.counts),min.exon.count = min.exon.count)\n  cat("done\\n")\n  return(list(gene.df=data.frame(base.df),info=info))\n}\n\n# estimate projected delta given x\'=(y-o) - gamma*x solution\n# em - normalized expression matrix\n# nm - normalized nascent matrix\n# gamma - inferred degradation coefficients\n# o - inferred offset (assumed to be zero by default)\n# delta - time to project forward\nt.get.projected.delta <- function(em,nm,gamma,offset=rep(0,length(gamma)),delta=0.5) {\n  # adjust rownames\n  gn <- intersect(names(gamma),rownames(em));\n  if(is.null(names(offset))) { names(offset) <- names(gamma); }\n  em <- em[gn,]; nm <- nm[gn,]; gamma <- gamma[gn]; offset <- offset[gn];\n  # time effect constant\n  egt <- exp(-gamma*delta);\n  y <- nm-offset; y[y<0] <- 0; # zero out entries with a negative n levels after offset adjustment\n  em*egt + (1-egt)*y/gamma  - em\n}\n\n# conservative estimate based on the sensitivity to addition/subtraction of a single n count\nt.get.projected.delta2 <- function(em,nm,nm.size,gamma,offset=rep(0,length(gamma)),delta=0.5,scount=1) {\n  # adjust rownames\n  gn <- intersect(names(gamma),rownames(em));\n  if(is.null(names(offset))) { names(offset) <- names(gamma); }\n  em <- em[gn,]; nm <- nm[gn,]; gamma <- gamma[gn]; offset <- offset[gn];\n  # time effect constant\n  egt <- exp(-gamma*delta);\n  y <- nm-(offset %o% nm.size);\n  y1 <- y-scount; y2 <- y+scount;\n  y[y<0] <- 0; # zero out entries with a negative n levels after offset adjustment\n  y1[y1<0] <- 0; y2[y2<0] <- 0;\n  d <- em*egt + (1-egt)*t(t(y)/nm.size)/gamma  - em;\n  d1 <- em*egt + (1-egt)*t(t(y1)/nm.size)/gamma  - em;\n  d2 <- em*egt + (1-egt)*t(t(y2)/nm.size)/gamma  - em;\n  \n  cd <- d;\n  zi <- abs(cd)>abs(d1); cd[zi] <- d1[zi]\n  zi <- abs(cd)>abs(d2); cd[zi] <- d2[zi]\n  cd[sign(d1)!=sign(d2)] <- 0;\n  cd\n}\n\n\n# estimate projected delta given log2 fold observed/expected nascent ratio \n# em - normalized expression matrix\n# nm - normalized nascent matrix\n# gamma - inferred degradation coefficients\n# r - log2 observed/expected nascent cont ratio\n# delta - time to project forward\nt.get.projected.delta.from.log2ratio <- function(em,gamma,r,delta=0.5,min.val=1e-4) {\n  # adjust rownames\n  gn <- intersect(intersect(names(gamma),rownames(em)),rownames(r));\n  em <- em[gn,]; gamma <- gamma[gn]; r <- 2^r[gn,];\n  # time effect constant\n  egt <- exp(-gamma*delta);\n  (em+min.val)*(egt*(1-r) +r) - em\n}\n\n\n# determine membership of points in fragments\npoints.within <- function(x,fs,fe,return.list=F,return.unique=F,sorted=F,return.point.counts=F) {\n  if(is.null(x) | length(x) < 1) { return(c()) };\n  if(!sorted) {\n    ox <- rank(x,ties.method="first");\n    x <- sort(x);\n  }\n  \n  se <- c(fs,fe);\n  fi <- seq(1:length(fs));\n  fi <- c(fi,-1*fi);\n  \n  fi <- fi[order(se)];\n  se <- sort(se);\n  \n  storage.mode(x) <- storage.mode(fi) <- storage.mode(se) <- "integer";\n  if(return.unique) { iu <- 1; } else { iu <- 0; }\n  if(return.list) { il <- 1; } else { il <- 0; }\n  if(return.point.counts) { rpc <- 1; } else { rpc <- 0; }\n  storage.mode(iu) <- storage.mode(il) <- storage.mode(rpc) <- "integer";\n  result <- points_within2(x,se,fi,il,iu,rpc)\n  #result <- .Call("points_within2",x,se,fi,il,iu,rpc);\n  if(!sorted & !return.point.counts) {\n    result <- result[ox];\n  }\n  return(result);\n}\n\n\n\nbalancedKNN <- function(val,k,maxl=k,return.distance.values=FALSE,n.threads=1,dist=\'cor\') {\n  if(class(dist)=="dist") { # actual distance was passed\n    if(!all(labels(dist)==colnames(val))) { stop("balancedKNN(): supplied distance doesn\'t match the columns of val") }\n    cd <- as.matrix(dist);\n  }  else {\n    if(dist==\'cor\') {\n      cd <- 1-cor(val);\n    } else if(dist==\'euclidean\') {\n      cd <- as.matrix(dist(t(val)))\n    } else {\n      stop(paste("unknown distance",dist,"specified"))\n    }\n  }\n  z <-  balanced_knn(cd,k,maxl,return.distance.values,n.threads);\n  rownames(z) <- colnames(z) <- colnames(val);\n  z\n}\n\n\n# fater matrix correlations wtih armadillo\n##\' A slightly faster way of calculating column correlation matrix\n##\' @param mat matrix whose columns will be correlated\n##\' @param nthreads number of threads to use \n##\' @return correlation matrix \n##\' @export\narmaCor <- function(mat,nthreads=1) {\n  cd <- arma_mat_cor(mat);\n  rownames(cd) <- colnames(cd) <- colnames(mat);\n  return(cd)\n}\n\n\n\ndefaultNCores <- function() { parallel::detectCores(logical=F) }\n'